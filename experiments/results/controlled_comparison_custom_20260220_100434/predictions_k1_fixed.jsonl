{"id": "test_0", "question": "What is the mission of PolitiFact?", "golden_answers": ["To reduce false information and misleading statements in political speech and provide the public with a clear and accurate political information environment through professional fact-checking work."], "prediction": "The documents provided do not specify the mission of PolitiFact.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01394v1\nEach fact-checking news report on the PolitiFact website includes the source platform of the news, such as \nInstagram posts, TikTok posts, Facebook posts, X posts, Viral image, etc. Currently, we have analyzed the \nfact-checking news reports whose source platform is X posts and filtered out the links to the news sources \non the X platform or Twitter. Then, we collected data from the links on the X platform or Twitter, including \nthe fact-checking news reports. Twitter, founded by Jack Dorsey and his partners in March 2006 and officially launched in July of the same \nyear, is a trendsetting social media platform that allows users to post messages of up to 140 characters (with \nan expanded limit of 280 characters fo r Chinese, Japanese, and Korean). These messages are known as \n\"tweets.\" On July 23, 2023, CEO Musk announced through a tweet that the iconic \"bluebird\" icon would be \nreplaced with an \"X\" icon, marking a significant update to the brand image. Subsequently, on July 24, Musk \nrevealed that Twitter's website address would migrate from twitter.com to X.com. On July 31, Twitter's app \nname on the Apple App Store was officially changed to X. \n4.3. PolitiFact Data Collection and Filtering Methodology In this study, Python scraping technology was utilized to systematically collect all fact-checking news reports \npublished by PolitiFact since its establishment in 2007, totaling approximately 24,000 records. The specific \ncollection process is outlined below, \n4.3.1. Scraping PolitiFact \nDetermining the maximum page range , By analyzing the URL structure of the PolitiFact website's fact -\nchecking news report list page ( https://www.politifact.com/factchecks/?page=XXX), a large page number \n(such as 900) was input and observed for the \"No Results found\" prompt. Gradually decrementing the page \nnumber until the current valid page range was determined to be 809 pages. Iterating and scraping pages: From page 1 to page 809, each page was parsed and links to fact-checking news \nreports were extracted. Subsequently, the corresponding news report pages were downloaded for each link. Parsing news report content: The downloaded content of each fact-checking news report page was parsed to \nextract information including the judgment result, statement, statement source platform, statement date, \nfactchecker name, factcheck date, topic, page number, factcheck analysis link, data acquisition date, and list \nof news sources. Data recording and storage: Each fact-checking news report was assigned a unique ID and page number, then \norganized into a file. When initiating HTTP requests, a random waiting time mechanism was introduced to \nensure friendly access to the original website and avoid unnecessary access pressure. Data conversion and storage: After completing the scraping of all pages, the raw data was converted into \nJSON and parquet formats, and further transformed into XLS file format for subsequent data processing and \nanalysis. 4.3.2. Filtering PolitiFact \nPlatform source filtering: Within the XLS file, all fact -checking news reports originating from twitter or X \nplatforms were filtered based on the statement source platform.", "metadata": {"type": "factual", "source": "2503.01394v1.pdf", "context": "website and Twitter. \n4.1. Introduction to the PolitiFact  \nPolitiFact is a nonpartisan, nonprofit fact -checking website operated by the Poynter Institute in Tampa. Its \nmission is to reduce false information and misleading statements in political speech and provide the public \nwith a clear and accurate political information environment through professional fact-checking work. \nThe main task of this website is to conduct thorough fact -checking on political figures, political \nadvertisements, p", "quality_score": 9.25}}
{"id": "test_1", "question": "What were the three models evaluated in Section 5.6?", "golden_answers": ["The original model (LLM without finetuning), the fined-tuned model (LLM finetuned with our synthetic examples), and a model that is trained on filtered data using the verifier."], "prediction": "The documents provided do not contain information about the specific models evaluated in Section 5.6.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00025v1\nTwo questions in the 2025 exam were intentionally modified to reduce potential data leakage from sources already indexed by certain models. All MIR content was used strictly for research purposes and safeguarded to maintain confidentiality and exam integrity. Figure 1- Process Flow for MIR Exam A total of twenty-two LLMs were selected, ranging from high-profile general-purpose architectures (e.g., GPT-4 Turbo, Claude Sonet 3.5) to specialised models such as Miri Pro, which had undergone fine-tuning on Spanish medical corpora. To ensure consistency, every model was tested under zero-shot conditions, receiving no additional hints or clarifications beyond a standardised prompt. This prompt instructed the LLM to identify the single best answer (out of four choices) and provide a step-by-step explanation of its\n\nreasoning in Spanish. For multimodal-capable models, image-based questions were presented along with the relevant visual inputs. Text-only models are presented solely with the question, without any indication of an accompanying image, which prevents them from viewing or interpreting it. Instead, these models are supplied with written descriptions of the images that highlight the key visual features otherwise evident in the graphic. Each LLM processed the 210 questions from both the 2024 and 2025 exams, generating responses that were automatically stored and timestamped. Correctness was determined by matching the final answer to the official solution key provided by the Spanish Ministry of Health, yielding a raw accuracy score (number of correct responses out of 210) that was later converted to a percentage. These results were supplemented by metadata on whether each question required image interpretation or tested domain-specific knowledge. A human performance reference\u2014represented by the highest-achieving candidate in the 2025 MIR\u2014served as a benchmark for contextualising the relative performance of each model. To facilitate meaningful comparisons, accuracy data were subjected to basic descriptive statistics (mean, median, standard deviation). Further, models were grouped into categories (e.g., general-purpose vs. fine-tuned) for subgroup analyses. Paired tests were conducted to detect significant changes in individual models\u2019 performance from 2024 to 2025, while non-parametric methods were employed if the distributional assumptions for parametric tests were not met. Additionally, a series of omnibus tests was performed to ascertain whether performance differed significantly among multiple models, followed by post-hoc analyses where warranted. Throughout the study, robust data management protocols were maintained, with model outputs stored in encrypted repositories. This controlled, consistent framework for measuring each model\u2019s capacity to address a demanding medical licensing exam lays the groundwork for further exploration into how LLMs might be integrated into future clinical or educational contexts. 4. Analysis and Results 4.1. Analysis of Large Language Model Performance on the MIR 2024 and 2025 Examinations This section provides a detailed analysis of the performance of 22 Large Language Models (LLMs) evaluated on the Spanish Medical Internship and Residency (MIR) examinations for 2024 and 2025. The analysis focuses on two primary hypotheses: (1) the relationship between LLM performance and their ability to engage in complex clinical reasoning, and (2) the extent to which model performance reflects reasoning capabilities rather than memorisation.", "metadata": {"type": "factual", "source": "2503.01385v1.pdf", "context": "Section 5.6) where we evaluated (i) the original model (LLM without finetuning), (ii) the fined-tuned model (LLM\nfinetuned with our synthetic examples), and (iii) a model that is trained on filtered data using the verifier. This allows us\nto understand the impact of each of the NL-Q Verifier components on the NL \u2192 Q task.\nFeedback within QA systems The verifier can serve as an internal feedback mechanism within a QA system. When\nthe system generates a query translation for a user question, the Q", "quality_score": 9}}
{"id": "test_2", "question": "What kind of applications can an interaction-oriented world model support?", "golden_answers": ["A wide range of applications, including human-robot interaction, closed-loop simulators, intelligent sports coaching, and immersive VR/AR gaming experiences."], "prediction": "An interaction-oriented world model can support applications in scenarios involving multiple agents, such as autonomous navigation, robotics, and driving simulations, where agents must operate within shared environments while adhering to physical, kinematic, and regulatory constraints.", "f1": 0.07843137254901962, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00761v1\nWorld Model Implementation\nThe world model enforces physical, kinematic, and regula-\ntory constraints, ensuring that each proposed state transition\nremains feasible. Though implementations vary by domain,\nour framework relies on two components:\na) Kinematic Feasibility Module: This component ver-\nifies whether state transitions respect motion limits such as\nmaximum velocity, turning radius, and acceleration bounds. Implemented as a lightweight physics function, it evaluates\neach candidate step against the agent\u2019s operational parame-\nters and rejects physically impossible maneuvers. b) Domain Compliance Validator: Domain-specific\nrules and constraints C are implemented as boundary checks\nthat detect collisions with obstacles, violations of navigable\nzones, or breaches of right-of-way regulations. This module\n\nemploys geometric intersection tests and rule-based verifica-\ntion to maintain compliance with environmental restrictions. These components define a binary feasibility function F\nreturning 1 for valid transitions and 0 otherwise. Candidate\ntrajectories are retained only if all transitions satisfy F,\npruning infeasible branches and validating novel paths. V. E XPERIMENTS AND RESULTS\nMotivation: We evaluate TRACE in scenarios involving\ntwo agents, an observer and a target, operating in a shared\nenvironment. Due to real-world constraints (such as limited\nsensor range or communication bandwidth restrictions), the\nobserver receives only sporadic measurements of the target\nagent\u2019s state. Our goal is to predict the target\u2019s behavior\nhypotheses\u2013trajectories\u2013despite this measurement sparsity,\nenabling effective decision-making by the observer. A. Robot Setup\na) Hardware Experiments (Marine Navigation): Our\nreal-world in-the-wild tests utilize two SeaRobotics Surveyor\nAutonomous Surface Vehicles (ASVs) operating in a lake\ncontaining multiple fountains that serve as both physical\nobstacles and sources of water disturbance. The environment\ncontains designated safe navigable channels analogous to\nmaritime lanes and restricted areas (fountains and shorelines)\nthat must be avoided. Though less rigidly defined than road\nnetworks, these navigational constraints enforce a structured\nspace within which both ASVs must operate. The ASV\u2019s\naction space is defined by a = (\u03bb, \u03b1), where \u03bb \u2208 [0, \u03bbmax]\nrepresents propulsion torque ( \u03bbmax = 85 N\u00b7m), \u03b1 \u2208 [0, 2\u03c0)\nindicates steering angle. The maximum operational speed is\n2 knots. The ASVs are equipped with GPS for localization. The observer ASV , receives camera snapshots of the target\nASV at irregular intervals. Between camera observations, the\ntarget\u2019s state is entirely unknown. b) Simulated Experiments (Autonomous Driving): To\nevaluate TRACE under controlled conditions, we developed\na custom medium-fidelity simulator featuring a two-agent\ndriving scenario on a grid of roads and buildings in an urban\nsetting. The observer vehicle tracks a target vehicle using\nsimulated LiDAR readings obtained at sparse intervals. The\nvehicles operate with a maximum speed of 15 m/s, a turning\nradius of 5 meters, and maximum acceleration/deceleration\nof \u00b13 m/s2. Environmental constraints include road bound-\naries, building footprints, traffic rules, and intersection pro-\ntocols, providing well-defined restrictions. Our experiments use GPT-4 Turbo with Vision via\nthe OpenAI API, though TRACE\u2019s architecture is model-\nagnostic and compatible with any vision-language model. B.", "metadata": {"type": "factual", "source": "2503.01291v1.pdf", "context": "structing an interaction-oriented world model for humans,\nenabling reasonable adaptation to changes of interactive\nobjects or people. This interaction-oriented world model\ncan support a wide range of applications, including human-\nrobot interaction, closed-loop simulators, intelligent sports\ncoaching, and immersive VR/AR gaming experiences.\nAs the importance of interaction becomes increasingly\nrecognized, some studies have evolved from text-driven hu-\nman motion generation [4, 6, 16, 31, 32, 49]", "quality_score": 9}}
{"id": "test_3", "question": "What is the primary goal of the OptMetaOpenFOAM framework?", "golden_answers": ["The primary goal of OptMetaOpenFOAM is to bridge MetaOpenFOAM with external analysis and optimization tool libraries through a large language model (LLM)-driven chain-of-thought (COT) methodology, empowering non-expert users to perform sensitivity analyses and parameter optimizations with improved efficiency."], "prediction": "The primary goal of the OptMetaOpenFOAM framework is to evaluate and optimize computational fluid dynamics (CFD) simulations and postprocessing tasks based on user requirements, focusing on metrics such as Executability, Cost, and result presentation.", "f1": 0.23529411764705882, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01273v1\nThe \u201csimilarity\u201d approach was \nemployed to identify and match related chunks of \ndata. The simplest stacking approach was used, \ncombining retrieved documents with user input \nmessages. Additional information can be found in \nthe code repository: https://github.com/Terry-\ncyx/MetaOpenFOAM. In the external analysis and \noptimization tool library, the univariate \noptimization method, the L-BFGS-B optimization\n\n5 \n \n \nalgorithm, is invoked through the bounded method \nin scipy.optimize. For multivariable analysis and \noptimization, the active subspace method is used  \n[27]. 4 Results \nThe evaluation metrics used previously were \nbased on the CFD framework with natural \nlanguage inputs, with a primary focus on the user \nrequirements for Executability (ranging from 0 to \n7, corresponding to failure to flawless), Cost \n(including token usage, number of iterations, \nrunning time, etc.), and Pass@k [34, 35]. These \nmetrics remain significant in OptMetaOpenFOAM, \nbut due to the integration of fixed interfaces after \ncompleting the CFD simulation and postprocessing \ntasks, the Executability, Pass@k, and number of \niterations remain consistent with the previous \nstatistics in MetaOpenFOAM 2.0 [13]. Regarding \nCost, token usage slightly increases due to the \naddition of two new modules, while the running \ntime increases linearly with the addition of CFD \nsimulation tasks. Therefore, in addition to these \nthree metrics, the evaluation of \nOptMetaOpenFOAM increasingly focuses on the \nresult presentation. In this section, we will analyze five cases: \nPitzDaily, CounterFlowFlame, BuoyancyCavity, \nHIT, and Hydrogen Combustion Chamber. Among \nthese, the first four cases are OpenFOAM tutorials, \nwhile the latter is not. All the basic cases have been \nincorporated in to the database required for the \nRAG technique. All the figures presented in this \nsection were generated using OptMetaOpenFOAM \nbased on user prompts. It is important to note that \nthe prompt for textual response is provided in \nAppendix A, the full textual analysis provided by \nOptMetaOpenFOAM is included in Appendix B \nand the thought process provided by \nOptMetaOpenFOAM is included in Appendix C. \n4.1 PitzDaily \nThis case is an incompressible flow, simulated \nusing the RANS method. The following prompt is \nused to perform simulation for this case. CFD simulation task: Do a RANS simulation \nof incompressible PitzDaily flow. CFD postprocessing task: Extract max yplus at \nlatest time through post-processing \nInvestigating the relationship between inlet \nvelocity and y+ is one of the common CFD \nsensitivity analysis tasks. A similar analysis can be \nperformed for the relationship between inlet \nvelocity and the Courant number. The following \nprompt is used to study this case. CFD analysis tasks: \u2460 Analyze the effect of \nthe inlet flow velocity on max yplus . \u2461 Analyze \nthe effect of the inlet flow velocity and inlet \nturbulent kinetic energy on max yplus. Figure 2: Response surface of the inlet flow \nvelocity versus the max yPlus in PitzDaily. (a) \n \n(b) \nFigure 3:   Effect of inlet flow velocity and inlet \nturbulent kinetic energy on max y+ in PitzDaily \ncase. (a) response surface and (b) components of \nactive direction (\ud835\udc98\u0302). 6 \n \n \nCFD optimization task: Determine the optimal \ninlet flow velocity and inlet turbulent kinetic \nenergy at which the max yplus should be as close \nto 25 as possible.", "metadata": {"type": "factual", "source": "2503.01273v1.pdf", "context": "1 \n \n \nAbstract \nMerging natural language interfaces with \ncomputational fluid dynamics (CFD) \nworkflows presents transformative \nopportunities for both industry and research. \nIn this study, we introduce \nOptMetaOpenFOAM\u2014a novel framework \nthat bridges MetaOpenFOAM with \nexternal analysis and optimization tool \nlibraries through a large language model \n(LLM)-driven chain -of-thought (COT) \nmethodology. By automating complex \nCFD tasks via natural language inputs, the \nframework empowers non-exp", "quality_score": 9}}
{"id": "test_4", "question": "What is the primary focus of the research presented in [12] V. V. Menon et al., 'EMES: Efficient Multi-Encoding Schemes for HEVC-Based Adaptive Bitrate Streaming'?", "golden_answers": ["The primary focus of this research is on developing efficient multi-encoding schemes for HEVC-based adaptive bitrate streaming."], "prediction": "The primary focus of the research presented in [12] is on developing efficient multi-encoding schemes for HEVC-based adaptive bitrate streaming.", "f1": 0.8823529411764706, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01404v1\n[9] H. Amirpour, E. C \u00b8 etinkaya, C. Timmerer, and M. Ghanbari, \u201cTowards\nOptimal Multi-Rate Encoding for HTTP Adaptive Streaming,\u201d in Mul-\ntiMedia Modeling: 27th International Conference, MMM 2021, Prague,\nCzech Republic, June 22\u201324, 2021, Proceedings, Part I 27, pp. 469\u2013480,\nSpringer, 2021. [10] E. Cetinkaya, H. Amirpour, C. Timmerer, and M. Ghanbari, \u201cFaME-\nML: Fast Multi-Rate Encoding for HTTP Adaptive Streaming Using\nMachine Learning,\u201d in 2020 IEEE International Conference on Visual\nCommunications and Image Processing (VCIP), pp. 87\u201390, IEEE, 2020. [11] E. Cetinkaya, H. Amirpour, C. Timmerer, and M. Ghanbari, \u201cFast multi-\nresolution and multi-rate encoding for http adaptive streaming using\nmachine learning,\u201d IEEE Open Journal of Signal Processing, vol. 2,\npp. 484\u2013495, 2021. [12] V . V . Menon, H. Amirpour, M. Ghanbari, and C. Timmerer, \u201cEMES:\nEfficient Multi-Encoding Schemes for HEVC-Based Adaptive Bitrate\nStreaming,\u201d ACM Transactions on Multimedia Computing, Communica-\ntions and Applications, vol. 19, no. 3s, pp. 1\u201320, 2023. [13] Y . Liu, H. Amirpour, M. Abdoli, C. Timmerer, and T. Guionnet,\n\u201cPreparing VVC for Streaming: A Fast Multi-Rate Encoding Approach,\u201d\nin 2023 IEEE International Conference on Visual Communications and\nImage Processing (VCIP), pp. 1\u20135, IEEE, 2023. [14] Q. Li, H. Meng, and Y . Li, \u201cTexture-Based Fast QTMT Partition\nAlgorithm in VVC Intra Coding,\u201d Signal, Image and Video Processing,\nvol. 17, no. 4, pp. 1581\u20131589, 2023. [15] X. Shang, G. Li, X. Zhao, H. Han, and Y . Zuo, \u201cFast CU Size Decision\nAlgorithm for VVC Intra Coding,\u201d Multimedia Tools and Applications,\nvol. 82, no. 18, pp. 28301\u201328322, 2023. [16] R. Yang, X. He, S. Xiong, Z. Zhao, and H. Chen, \u201cFast CU Partition\nStrategy Based on Texture and Neighboring Partition Information for\nVersatile Video Coding Intra Coding,\u201d Multimedia Tools and Applica-\ntions, vol. 83, no. 10, pp. 28323\u201328340, 2024. [17] X. Jiang, W. Li, and T. Song, \u201cLow-Complexity Enhancement VVC\nEncoder for Vehicular Networks,\u201d EURASIP Journal on Advances in\nSignal Processing, vol. 2023, no. 1, p. 122, 2023. [18] Y . Song, S. Cheng, M. Wang, and X. Peng, \u201cFast CU Partition for VVC\nIntra-Frame Coding via Texture Complexity,\u201d IEEE Signal Processing\nLetters, 2024. [19] J.-J. Chen, Y .-G. Chou, and C.-S. Jiang, \u201cSpeed Up VVC Intra-Coding\nby Learned Models and Feature Statistics,\u201d IEEE Access, 2023.\n[20] M. Kherchouche, F. Galpin, T. Dumas, D. Menard, and L. Zhang, \u201cRD-\ncost Regression Speed Up Technique for VVC Intra Block Partitioning,\u201d\nin ICASSP 2024-2024 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 3530\u20133534, IEEE, 2024. [21] M. Li, Z. Wang, and Q. Zhang, \u201cFast CU Size Decision and Intra-\nPrediction Mode Decision Method for H.266/VVC,\u201d EURASIP Journal\non Image and Video Processing, vol. 2024, no. 1, p. 7, 2024. [22] W. Kuang, X. Li, X. Zhao, and S. Liu, \u201cUnified Fast Partitioning\nAlgorithm for Intra and Inter Predictions in Versatile Video Coding,\u201d\nin 2022 Picture Coding Symposium (PCS), pp. 271\u2013275, IEEE, 2022. [23] Y . Li, Z. He, and Q. Zhang, \u201cFast Decision-Tree-Based Series Parti-\ntioning and Mode Prediction Termination Algorithm for H.266/VVC,\u201d\nElectronics, vol. 13, no. 7, p. 1250, 2024. [24] Y .", "metadata": {"type": "factual", "source": "2503.01404v1.pdf", "context": "[12] V . V . Menon, H. Amirpour, M. Ghanbari, and C. Timmerer, \u201cEMES:\nEfficient Multi-Encoding Schemes for HEVC-Based Adaptive Bitrate\nStreaming,\u201d ACM Transactions on Multimedia Computing, Communica-\ntions and Applications, vol. 19, no. 3s, pp. 1\u201320, 2023.\n[13] Y . Liu, H. Amirpour, M. Abdoli, C. Timmerer, and T. Guionnet,\n\u201cPreparing VVC for Streaming: A Fast Multi-Rate Encoding Approach,\u201d\nin 2023 IEEE International Conference on Visual Communications and\nImage Processing (VCIP), pp. 1\u20135, IEEE, ", "quality_score": 9}}
{"id": "test_5", "question": "What is the first part of the dot product calculation in Equation (6)?", "golden_answers": ["The vector multiplication operation, where the result is stored in an intermediate vector."], "prediction": "The first part of the dot product calculation in Equation (6) is given by the expression \\( A\\nabla u \\cdot \\nabla(utu\\eta) \\).", "f1": 0.14814814814814817, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00873v1\nThus, since \u03b1,\u03b2 \u22650, to prove (6.10), it is enough to show that\n(6.12) \u03b1+ 2\u03b2= E,\nwhere E satisfies the estimate |E|\u2272|Q(S\u2217)|. Let us begin by treating \u03b2. First using the equation L\u2217u = 0, then integrating by parts, we obtain\n\u03b2=\nZZZ\n\u2212(div A\u2207u) utu\u03b7=\nZZZ\nA\u2207u \u00b7\u2207(utu\u03b7) = E +\nZZZ\nA\u2207u \u00b7\u2207ut u\u03b7+\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7,\nwhere, after using the product rule, the error term is of type E2. Next, we will use this fact:\n(6.13) (1 /2) \u2202t(A\u2207u \u00b7\u2207u) = A\u2207u \u00b7\u2207ut + (1/2)(\u2202tA)\u2207u \u00b7\u2207u. To verify it, just expand and remember that A is symmetric. Using this identity to continue rewrit-\ning \u03b2, it turns out that, up to a new error term of type E6, we have\n\u03b2= E +\nZZZ\n(1/2) \u2202t(A\u2207u \u00b7\u2207u) u\u03b7+\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7. Finally, integrating by parts in t on the first term we have, up to a new \u201cerror term\u201d of type E1,\n(6.14) \u03b2= E \u22121\n2\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7+\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7= E + 1\n2\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7. Next, our goal is to show that \u03b1is nearly of the same form as \u03b2, to get cancellations. First, using\nthe product rule both for \u2202xk (ai,juxi ) and \u2202xj (ak,\u2113ux\u2113) simultaneously, we obtain\n\u03b1=\nZZZ\nu \u2202xk (ai,juxi ) \u2202xj (ak,\u2113ux\u2113) \u03b7\u2212E8 \u2212E7 \u2212E7,\n\nV ARIABLE COEFFICIENT FBP FOR Lp-SOLV ABILITY 25\nand we will abbreviate the errors by E. Next, we integrate by parts in xk and use the product rule to\nobtain, up to a new error term of type \u2212E4 \u2212E5 (when \u2202xk lands on \u03b7),\n\u03b1= E \u2212\nZZZ\nuxk (ai,juxi ) \u2202xj (ak,\u2113ux\u2113) \u03b7\u2212\nZZZ\nu (ai,juxi ) \u2202xk \u2202xj (ak,\u2113ux\u2113) \u03b7=: E + I + II . We estimateI and II separately, starting withII . Using the equationL\u2217u = 0 (and exchanging the\norder of derivatives when needed) and rewriting the resulting expression using thatA is symmetric,\nthen invoking (6.13) (and hence introducing an error term of typeE6), and lastly integrating by parts\nin t (introducing an error of type E1), we have\nII = \u2212\nZZZ\nu (ai,juxi ) \u2202xj \u2202xk (ak,\u2113ux\u2113) \u03b7=\nZZZ\nu (ai,juxi ) \u2202xj ut \u03b7=\nZZZ\nu A\u2207u \u00b7\u2207ut \u03b7\n= E + 1\n2\nZZZ\nu \u2202t(A\u2207u \u00b7\u2207u) \u03b7= E \u22121\n2\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7. We have a desirable estimate for II and now turn to I. We start by integrating by parts in xj, to\nobtain, up to a new error term of type E3,\nI = E +\nZZZ\nuxk,xj (ai,juxi )(ak,\u2113ux\u2113) \u03b7+\nZZZ\nuxk \u2202xj (ai,juxi ) ak,\u2113ux\u2113 \u03b7=: E + I1 + I2,\nBy the equation L\u2217u = 0 and symmetry of A, we simply compute\nI2 = \u2212\nZZZ\nA\u2207u \u00b7\u2207u ut\u03b7. To handle I1 and get some cancellation with I2 in this form, we use a similar identity to (6.13),\n(1/2) \u2202xk (A\u2207u \u00b7\u2207u) = A\u2207u \u00b7\u2207uxk + (1/2) (\u2202xk A)\u2207u \u00b7\u2207u.", "metadata": {"type": "factual", "source": "2503.01313v1.pdf", "context": "Equation (6), PVU decompose the dot product calculation into\ntwo parts. The first part is the vector multiplication operation,\nwhere the result is stored in an intermediate vector. The second\npart is the accumulation operation, where the elements of the\nintermediate vector are summed to obtain the final dot product\nresult.\nFirst, PVU reuse the existing vector multiplication module,\nand store the result in an intermediate variable. Before per-\nforming the accumulation, PVU need to align all the e", "quality_score": 8.75}}
{"id": "test_6", "question": "What is the expression for CCDW(r) in terms of correlation functions?", "golden_answers": ["(\u22121)i+j\u27e8(Zi,A + Zi,B)(Zj,A + Zj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Zi,A + Zi,B)\u27e9\u27e8(Zj,A + Zj,B)\u27e9 = ( \u22121)i+j \u00ee \u27e8c\u2020 i,Acj,A\u27e9\u27e8ci,Ac\u2020 j,A\u27e9 + \u27e8c\u2020 i,Bcj,B\u27e9\u27e8ci,Bc\u2020 j,B\u27e9"], "prediction": "CCDW(r) \u2261 (\u22121)i+j\u27e8(Zi,A + Zi,B)(Zj,A + Zj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Zi,A + Zi,B)\u27e9\u27e8(Zj,A + Zj,B)\u27e9 = ( \u22121)i+j \u00ee \u27e8c\u2020 i,Acj,A\u27e9\u27e8ci,Ac\u2020 j,A\u27e9 + \u27e8c\u2020 i,Bcj,B\u27e9\u27e8ci,Bc\u2020 j,B\u27e9 \u00f3 = ( \u22121)r+1 2 sin2(\u03c0r/2) (\u03c0r)2.", "f1": 0.8, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "good title\nSimilarly, we can obtain the connected correlation functions for the CDW and BDW channels as follows:\nCCDW(r) \u2261 (\u22121)i+j\u27e8(Zi,A + Zi,B)(Zj,A + Zj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Zi,A + Zi,B)\u27e9\u27e8(Zj,A + Zj,B)\u27e9\n= ( \u22121)i+j\n\u00ee\n\u27e8c\u2020\ni,Acj,A\u27e9\u27e8ci,Ac\u2020\nj,A\u27e9 + \u27e8c\u2020\ni,Bcj,B\u27e9\u27e8ci,Bc\u2020\nj,B\u27e9\n\u00f3\n= ( \u22121)r+1 2 sin2(\u03c0r/2)\n(\u03c0r)2 , (S10)\nand\nCBDW(r) \u2261 (\u22121)i+j\u27e8(Di,A \u2212 Di,B)(Dj,A \u2212 Dj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Di,A \u2212 Di,B)\u27e9\u27e8(Dj,A \u2212 Dj,B)\u27e9\n= ( \u22121)i+j X\n\u03b1\nh\n\u27e8c\u2020\ni,\u03b1cj+1,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni,\u03b1cj,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj+1,\u03b1\u27e9\n+ \u27e8c\u2020\ni+1,\u03b1cj+1,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni+1,\u03b1cj,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj+1,\u03b1\u27e9\ni\n= 4( \u22121)r+1\n\u00c7\nsin2(\u03c0r/2)\n(\u03c0r)2 + sin[\u03c0(r + 1)/2] sin[\u03c0(r \u2212 1)/2]\n\u03c02(r + 1)(r \u2212 1)\n\u00e5\n. (S11)\nB. Finite-bond and finite-size extrapolations of the correlation functions at the deconfined critical point\nAs shown in Fig. 2(b) in the main text, the bulk-bulk connected correlations are computed by infinite-size DMRG\nsimulations. Although the thermodynamic limit is approached in a natural way, the calculation of the correlation\nfunctions is still affected by the finite-bond effect (or the finite-entanglement effect). To this end, we first perform a\n\n12\n100 300 500\n\u00ac\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nCCDW(r, \u00ac)\nCCDW(r = 15)\nCCDW(r = 25)\nCBDW(r = 15)\nCBDW(r = 25)\n128 192 256\nL\n3\u00a310\u00b05\n4\u00a310\u00b05\n5\u00a310\u00b05\n6\u00a310\u00b05\nCCDW(r, L)\nCCDW(r = 56)\nCCDW(r = 64)\nCBDW(r = 56)\nCBDW(r = 64)0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\nCBDW(r, \u00ac)\n0.010\n0.011\n0.012\n0.013\nCBDW(r, L) <latexit sha1_base64=\"2RRwxLXlY8TROIoM98j2WcOjpro=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlhzI97xVLbsWdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE177Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNi4p3WaneV0u1myyOPJzAKZTBgyuowR3UoQEMBvAMr/DmCOfFeXc+Fq05J5s5hj9wPn8Ai5mNUw==</latexit>\n( a )\n<latexit sha1_base64=\"KWh0RLJ0bw8em/x3PU2+HIlN2FQ=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlh3Jw3iuW3Io7B1klXkZKkKHeK351+zFLI5SGCap1x3MT40+oMpwJnBa6qcaEshEdYMdSSSPU/mR+6pScWaVPwljZkobM1d8TExppPY4C2xlRM9TL3kz8z+ukJrz2J1wmqUHJFovCVBATk9nfpM8VMiPGllCmuL2VsCFVlBmbTsGG4C2/vEqaFxXvslK9r5ZqN1kceTiBUyiDB1dQgzuoQwMYDOAZXuHNEc6L8+58LFpzTjZzDH/gfP4AjR6NVA==</latexit>\n( b )\nFIG. S4. (a) Each data point of the bulk-bulk correlation function for BDW and CDW orders shown in Fig.2(b) in the main\ntext is obtained by extrapolating to infinite MPS bond dimensions. The dotted and dashed lines are least-squares fittings with\nthe form C(r, \u03c7) = a(100/\u03c7)b + C(r, \u03c7\u2192 \u221e). (b) Each data of the boundary-bulk correlations displayed in Fig.2(c) in the\nmain text is obtained by extrapolating toL \u2192 \u221ewith the fitting form,C(r, L) = a/Lb + C(r, L\u2192 \u221e), whereL is the ladder\nlength. Numerical data are obtained from infinite-size DMRG calculations for (a) and finite-size DMRG calculations for (b)\nwith a sufficiently large bond dimension\u03c7 = 2048 to ensure the convergence of the results forL up to256.\n0 1 2 3 4 5\nk\n0\n1\n2\n3\n4\n5Ek(L)\u00b0E0(L) L= 16\nL= 32\nL= 48\nL= 64\nL= 80\nL= 96\n20 30 40 50 60 70 8090100\nL\n0.5\n0.6\n0.7\n0.8\n0.91.0\n2.0\ne(L) \u00a5E2(L)\u00b0E0(L)\ne(L)\u00aaaL\u00b0z\nz(L)= \u00b0d(lne(L))/d(lnL)\n0.00 0.01 0.02 0.03\n1/L\n0.950\n0.975\n1.000\nz(L)", "metadata": {"type": "factual", "source": "2503.01198v1.pdf", "context": "CCDW(r) \u2261 (\u22121)i+j\u27e8(Zi,A + Zi,B)(Zj,A + Zj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Zi,A + Zi,B)\u27e9\u27e8(Zj,A + Zj,B)\u27e9\n= ( \u22121)i+j\n\u00ee\n\u27e8c\u2020\ni,Acj,A\u27e9\u27e8ci,Ac\u2020\nj,A\u27e9 + \u27e8c\u2020\ni,Bcj,B\u27e9\u27e8ci,Bc\u2020\nj,B\u27e9\n\u00f3\n= ( \u22121)r+1 2 sin2(\u03c0r/2)\n(\u03c0r)2 , (S10)\nand\nCBDW(r) \u2261 (\u22121)i+j\u27e8(Di,A \u2212 Di,B)(Dj,A \u2212 Dj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Di,A \u2212 Di,B)\u27e9\u27e8(Dj,A \u2212 Dj,B)\u27e9\n= ( \u22121)i+j X\n\u03b1\nh\n\u27e8c\u2020\ni,\u03b1cj+1,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni,\u03b1cj,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj+1,\u03b1\u27e9\n+ \u27e8c\u2020\ni+1,\u03b1cj+1,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni+1,\u03b1cj,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj+1,\u03b1\u27e9\ni\n= 4( \u22121)r+1\n\u00c7\nsin2(\u03c0r/2)\n(\u03c0r)2 + sin[\u03c0(r + 1)/2] sin[\u03c0(r \u2212 1)/2]\n\u03c02(r + 1)(r \u2212 1", "quality_score": 8.75}}
{"id": "test_7", "question": "What is the purpose of training a transformer sequence model according to the procedure described in the text?", "golden_answers": ["The training procedure enables the model to learn the single-step predictive distributions that collectively define the full sequence likelihood."], "prediction": "The purpose of training a transformer sequence model according to the procedure described in the text is to generate synthetic \"evaluation\" MSAs containing 6M sequences, addressing finite sampling limitations by allowing the generation of MSAs with any desired number of sequences.", "f1": 0.19607843137254902, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00289v1\nWe then trained new Potts, MSA\nTransformer, and Independent models on the 6M syn-\nthetic training MSA and generated synthetic \u201cevalua-\ntion\u201d MSAs containing 6M sequences for all models. We\nrepeated a similar process for the MSA Transformer using\nthe same input. This approach addresses \ufb01nite sampling\nlimitations by generating MSAs with any desired number\nof sequences.", "metadata": {"type": "factual", "source": "2503.01215v1.pdf", "context": "For simplicity, denotebP\u03d5( \u02c6Yi+1 = y|\u02c6Y1:i) as bP(i+1)\n\u03d5 (y). This training procedure enables the model\nto learn the single-step predictive distributions that collectively define the full sequence likelihood.\nOnce trained, given any observed datay1:t, the transformer sequence model can generate future\nsamples autoregressively: \u02c6Yt+1:\u221e \u223c bP\u03d5(\u00b7|y1:t).\nNext, we examine how these trained sequence models can be applied to decision-making, high-\nlighting the limitations of one-step inference and how m", "quality_score": 8.75}}
{"id": "test_8", "question": "What type of system was used to conduct the experiments?", "golden_answers": ["An Ubuntu 22.04.1 LTS system equipped with two 32-core Intel(R) Xeon(R) Platinum 8358 CPUs at 2.60GHz, four NVIDIA A100 Tensor Core GPUs, and 1TB of physical memory."], "prediction": "TCR-pMHC systems.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00648v1\nExperimental measurements are done across three\nreplicates, and the reported GFP levels in Figs. 4, S5 for each condition are averaged over the three replicates. We\nconsider a peptide to have activated T-cells if its induced replicate-averaged GFP level met two criteria: 1) it exceeded\n0.5%, and 2) it surpassed the mean plus three standard deviations of the GFP level negative controls (no-peptide). According to these criteria, the GFP activation cutoffs were 0.5% for the NY-ESO system, 0.64% for the EBV system,\nand 0.76% for the MAGE system. VI. EXPERIMENTAL ACTIVITIES OF THE DESIGNED PEPTIDES\nComparing design success rates across the three TCR-pMHC systems. Among the designed peptides, 50%\nin the NY-ESO system (Fig. 4A), 16% in the EBV system (Fig. 4B), and 26% in the MAGE system induce significant\nT-cell activities. However, it should be noted that the designed NY-ESO peptides were closer in sequence to their\nwild-type templates (2-7 amino acid differences) than the peptides designed for the EBV (3-9 differences) or MAGE\n(3-8 differences) systems. We find that the activity of designs decays with increasing distance from the respective\nwild-type, with almost no designs beyond 5 amino-acid distance from the wild-types activating their respective T-cells\n(Fig. 4, S6, S7, S8). This underscores the difficulty of exploring peptide variants far from the wild-type sequences. If any positive designs do exist at these greater distances, they represent an exponentially smaller fraction of the\nsequence space, and when in complex with the TCR-MHC, are likely to adopt a different binding mode compared to\nthat of the wild-type. Minor adjustments to the design protocols, aimed at better exploring the design space, may have also contributed\nto the observed differences in performance across the systems. For NY-ESO, which achieved the highest design\nsuccess rate, peptides with highest PAE scores were chosen for experimental validation, whereas in EBV and MAGE,\npeptides were sampled uniformly above the set PAE thresholds. In MAGE, fixing the amino-acid at position 1 to\na Glutamic Acid (E) for the one-third of the designs led to an elevated success rate in this subset (Fig. S8D); E 1 is\nshared between the two wild-type peptides and this design choice allowed us to assess the importance of this amino\nacid for function. The design potential of different HERMES models. In panel A of Figures S6 (NY-ESO), S7 (EBV),\nand S8 (MAGE), we show the fidelity of designs using the four HERMES models: HERMES- fixed with no noise,\nHERMES-fixed with 0.5 \u02daA noise, HERMES-relaxed with no noise, and HERMES- relaxed with 0.5 \u02daA noise. Although\nPyRosetta relaxations allows HERMES- relaxed to explore the sequence space more broadly, the resulting designs\nthat are made at greater distances from the wild-types seldom succeed in activating T-cells. This pattern is also\nobserved with using models trained with 0.5 \u02daA Gaussian noise. Indeed, successful designs lie within five amino acid\nsubstitutions from the wild-types, and for these designs, the un-noised HERMES-fixed model demonstrates the highest\noverall fidelity.", "metadata": {"type": "factual", "source": "2503.01319v1.pdf", "context": "4 Experiment setup\nTo verify the testing effectiveness of ABFS on LLM-based NLP software, we conducted\na series of experiments on three text classification datasets and five threat models.\nAll experiments were performed on an Ubuntu 22.04.1 LTS system equipped with\ntwo 32-core Intel(R) Xeon(R) Platinum 8358 CPUs at 2.60GHz, four NVIDIA A100\nTensor Core GPUs, and 1TB of physical memory. Each experiment was repeated three\ntimes, and the results for each metric were averaged. Similar to previous st", "quality_score": 8.75}}
{"id": "test_9", "question": "What is a limitation of conventional image classification models in soybean leaf disease detection?", "golden_answers": ["Conventional models offer limited explainability, providing little insight into which leaf regions drive predictions and reducing interpretability and trust among agricultural experts."], "prediction": "Conventional models offer limited explainability, providing little insight into which leaf regions drive predictions and reducing interpretability and trust among agricultural experts.", "f1": 1.0, "em": 1.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01284v2\nNumerous studies have highlighted their effectiveness in soybean leaf disease\nclassification (Chen et al, 2020; Sethy et al, 2020; Dou et al, 2023; Sheng et al,\n2024; Bera et al, 2024; Rahman et al, 2024; Janarthan et al, 2024; Wang et al,\n2025; Wu et al, 2024). However, most existing approaches \u2014 whether CNNs or\ntransfer learning techniques are used (Karlekar and Seal, 2020; Wu et al, 2023)\n\u2014 focus on extracting features from individual images, overlooking critical\nrelational information between images. This becomes particularly problematic\nwhen diseases present visually similar symptoms triggered by different fac-\ntors, such as nutrient deficiencies, pest damage, or environmental stress, often\n\nSoybean Disease Detection via Interpretable Hybrid CNN-GNN 3\nleading to misclassifications. Moreover, these conventional models offer limited\nexplainability, providing little insight into which leaf regions drive predictions\nand reducing interpretability and trust among agricultural experts. To address these limitations, graph neural networks (GNNs) have emerged\nas a complementary approach capable of modeling relational dependencies\nbetween samples. GNNs are particularly well suited for cases where relation-\nships between images, such as symptom similarity or shared environmental\nconditions, provide valuable diagnostic cues (T. Senthil Prakash and Kiran,\n2023; Li, 2024). By treating images as nodes and defining edges on the basis\nof pairwise similarities, GNNs aggregate information from neighboring images,\nenabling context-aware classification incorporating local features and global\nrelational patterns. However, GNNs alone lack the ability to extract fine-\ngrained spatial features directly from raw images \u2014 a key strength of CNNs. Therefore, combining CNNs and GNNs into a hybrid framework offers a syn-\nergistic advantage: CNNs capture localized spatial features within individual\nimages, whereas GNNs enrich these representations with relational context\nacross images. This hybrid approach is particularly valuable for soybean leaf\ndisease classification, where local lesion characteristics and broader symptom\nsimilarity across fields, varieties, and conditions are essential for accurate and\ninterpretable diagnosis. To address these gaps, we propose an interpretable hybrid sequential CNN-\nGNN architecture that sequentially combines MobileNetV2 for efficient spatial\nfeature extraction and graph sample and aggregation (GraphSAGE), a GNN\narchitecture, for relational dependency modeling between soybean leaf images. By constructing a similarity graph where nodes represent leaf images and\nedges encode pairwise feature similarity, GraphSage (Hamilton et al, 2017a)\naggregates information from neighboring nodes, enriching the feature rep-\nresentations with a relational context. This fusion of local spatial learning\nand global relational learning enhances classification accuracy while ensur-\ning computational efficiency, making the model suitable for real-time field\ndeployment. Additionally, we incorporate Grad-CAM and Eigen-CAM visu-\nalizations to provide interpretable heatmaps that highlight the specific leaf\nregions influencing each classification decision, bridging the gap between model\npredictions and expert validation. To the best of our knowledge, this is the\nfirst interpretable CNN-GNN hybrid framework applied to soybean leaf disease\ndetection, addressing critical gaps in relational modeling, model transparency,\nand computational efficiency in plant disease classification research. We make the following key contributions in this work:\n1.", "metadata": {"type": "factual", "source": "2503.01284v3.pdf", "context": "Jahin et al.: Soybean Disease Detection via Interpretable Hybrid CNN-GNN\nmance in image classification tasks by automatically learn-\ning spatial features from raw images, eliminating the need\nfor manual feature extraction. Numerous studies have high-\nlighted their effectiveness in soybean leaf disease classifica-\ntion [1]\u2013[9]. However, most existing approaches \u2014 whether\nCNNs or transfer learning techniques are used [10], [11]\n\u2014 focus on extracting features from individual images,\noverlooking cri", "quality_score": 8.5}}
{"id": "test_10", "question": "What is the role of the exchange tensor in model (1)?", "golden_answers": ["The exchange tensor, denoted as Jabij, couples the spins at sites a, i and b, j."], "prediction": "The exchange tensor in model (1) is used to evaluate the Fourier transformation of the function \\((A \\times q) \\cdot (B \\times q)/(q^2 + \\mu^2_{ex})\\) and contributes to the coordinate space representations of the potentials, specifically in the context of computing partial wave projections for the interactions between particles.", "f1": 0.15094339622641512, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01322v1\nIn the phenomenological view, the \u03b4(r)\nterm can mimic the role of contact interaction [81], which is\nalso related to the regularization scheme [71]. In Refs. [15,\n101], after removing the \u03b4(r) term, the hadronic molecular\npicture for some observed hidden-charm states is discussed\nwith the one-pion-exchange potential, which is assumed to be\nof long-range. In this work, we will separately analyze the\npoles in the system with or without the \u03b4(r) term. For this\npurpose, we introduce a parameter a to distinguish these two\ncase,\nA \u00b7qB \u00b7q\nq2 + \u00b52ex\n\u2212a\n3A \u00b7B = 1\n3\n(\nA \u00b7B\n \n1 \u2212a \u2212 \u00b52\nex\nq2 + \u00b52ex\n! \u2212S (A,B,\u02c6q) |q|2\nq2 + \u00b52ex\n)\n. (14) After performing the Fourier transformation of Eq. (14), we\nhave\nZ d3q\n(2\u03c0)3\n A \u00b7qB \u00b7q\nq2 + \u00b52ex\n\u2212a\n3A \u00b7B\n! \u02dc\u039b2 \u2212\u00b52\nex\nq2 + \u02dc\u039b2\n! 2\neiq\u00b7r\n= \u22121\n3[A \u00b7BCex + S (A,B,\u02c6r)Tex], (15)\nwhere S (A,B,\u02c6r) = 3A \u00b7\u02c6rB \u00b7\u02c6r \u2212A \u00b7B is the tensor operator\nin coordinate space, and the functions Cex and Tex read\nCex = 1\nr2\n\u2202 \u2202r r2 \u2202\n\u2202r Yex + a\n(2\u03c0)3\nZ  \u02dc\u039b2 \u2212\u00b52\nex\nq2 + \u02dc\u039b2\n! 2\neiq\u00b7rd3q, (16)\nTex = r \u2202\n\u2202r\n1\nr\n\u2202\n\u2202r Yex. (17)\nApparently, the contribution of the \u03b4(r) term is fully included\n(excluded) when a = 0(1) [81, 86]. Similarly, the Fourier\ntransformation of the function (A\u00d7q) \u00b7(B \u00d7q)/(q2 +\u00b52\nex) can\nbe evaluated with the help of the relation (A \u00d7q) \u00b7(B \u00d7q) =\nA \u00b7B|q|2 \u2212A \u00b7qB \u00b7q.\nWith the prescription above, the coordinate space represen-\ntations of the potentials in Eqs. (24) can be written in terms of\nYex, Cex and Tex given in Eqs. (12) and (15). The potentials\nshould be projected into certain partial waves by sandwiching\nthe spin operators in the potentials between the partial waves\nof the initial and final states. We refer to Refs. [80, 81] for\ncomputing the partial wave projections. In this work, we fo-\ncus on the positive parity states which are possibly bound in\nS-wave and more easily form the molecular states respect to\nnegative ones. The partial waves corresponding to the spin-\nparities of JP = 0+,1+,2+ are shown in Table I.\nIn our numerical calculation, the masses of exchanged par-\n\n4\nTABLE I. Partial waves in the givenJP. P\u00afP P \u00afP\u2217/P\u2217\u00afP P \u2217\u00afP\u2217\nJp = 0+ 1S 0 - 1S 0,5D0 Jp = 1+ - 3S 1,3D1 3S 1,3D1,5D1\nJp = 2+ 1D2 3D2 5S 2,1D2,3D2,5D2\nticles are taken as m\u03c3 = 600.0 MeV , m\u03c0 = 138.0 MeV ,\nm\u03b7 = 547.9 MeV ,m\u03c1 = 770.7 MeV and m\u03c9 = 782.0 MeV . The coupling constants in the Lagrangian can be extracted\nfrom experimental data or deduced from various theoretical\nmodels. Here we adopt the values given in Refs.", "metadata": {"type": "factual", "source": "2503.01283v2.pdf", "context": "2\nX\nabij\nST\naiJabijSbj, (1)\nwhere Sai is the spin operator for site a in unit cell i\nand Jabij is the exchange tensor that couples the spins\nat sites a, iand b, j. Single-ion anisotropy terms are in-\ncluded through the onsite components J aaii of the ex-\nchange tensor. In order to calculate the thermal prop-\nerties of the model (1) one needs the magnetic excita-\ntion spectrum, which requires one to determine the mag-\nnetic ground state. Except for simple ferromagnets, the\nground state of the mod", "quality_score": 8.5}}
{"id": "test_11", "question": "Why is it necessary to determine the magnetic ground state of model (1) in order to calculate its thermal properties?", "golden_answers": ["The magnetic excitation spectrum is required to calculate the thermal properties, and this requires determining the magnetic ground state."], "prediction": "It is necessary to determine the magnetic ground state of model (1) in order to calculate its thermal properties because the magnetic excitation spectrum, which is essential for understanding thermal behavior, depends on the configuration that minimizes the classical energy of the system.", "f1": 0.4074074074074074, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01283v2\nIn Sec. II A, we\nbriefly discuss the Heisenberg model and single- Q spiral\nground states. We then introduce the methodologies for\ncalculating thermal properties of spin spiral structures\nusing renormalized HP bosononization (Sec. II B), the\nGreen\u2019s function method in (Sec. II C) and the Mean\nField approximation (Sec. II D). In Sec. III, we bench-\nmark the methods against the experimental results for a\nrange of real materials.\nII. THEORY\nA. Spiral ground states\nOur starting point is the generalized Heisenberg Hamil-\ntonian\nH = \u22121\n2\nX\nabij\nST\naiJabijSbj, (1)\nwhere Sai is the spin operator for site a in unit cell i\nand Jabij is the exchange tensor that couples the spins\nat sites a, iand b, j. Single-ion anisotropy terms are in-\ncluded through the onsite components J aaii of the ex-\nchange tensor. In order to calculate the thermal prop-\nerties of the model (1) one needs the magnetic excita-\ntion spectrum, which requires one to determine the mag-\nnetic ground state. Except for simple ferromagnets, the\nground state of the model is non-trivial and the starting\npoint will thus be the spin configuration that minimizes\nthe classical energy. For an isotropic model on a Bravais\nlattice, the minimizing spin configuration can be deter-\nmined using the Luttinger-Tisza (LT) method [21] and\n\n3\nthe result is known to be a coplanar spin spiral charac-\nterized by a magnetic ordering vector Q [13]. The result\ncan be generalized to include antisymmetric exchange,\nwhich then determines the orientation of the spiral plane\n[22]. The LT method has also been generalized to non-\nBravais lattices [23, 24] and under rather general circum-\nstances (but not always) the classical ground state for an\nisotropic model remains a planar spin spiral given by\nSai = Sa\n\uf8ee\n\uf8f0\n0\nsin(Q \u00b7 ri + \u03d5a)\ncos(Q \u00b7 ri + \u03d5a)\n\uf8f9\n\uf8fb, (2)\nwhere Q is the magnetic ordering vector, \u03d5a is the phase\nangle of the spin at sitea, ri is a lattice vector for unit cell\ni, and Sa is the maximal eigenvalue ofSz\na (the magnitude\nof the classical spin vector). We note that the assumption\nof an isotropic model implies that any orientation of the\nspiral plane yields the same ground state energy and we\nhave chosen the yz-plane here to be specific. Such a\nmagnetic structure is often referred as a single- Q spin\nspiral and is derived from the model in appendix A. The\ndiscussion in the remainder of this work will focus solely\non single-Q spin spirals. When considering excitations of spiral ground states\nit will be convenient to transform the system to a local\ncoordinate system where all spin fluctuations are perpen-\ndicular to the localz-axis. Here we reiterate the approach\nof Ref. 12 regarding this transformation, which will be\nneeded for reference below. We can write the spin opera-\ntors in terms of a cell-dependent rotation matrix Ri such\nthat\nSai = RiS\u2032\nai, (3)\nRi = R\u02c6 n(Q \u00b7 ri), (4)\nwhere \u02c6 nis the global rotation axis.", "metadata": {"type": "reasoning", "source": "2503.01283v2.pdf", "context": "2\nX\nabij\nST\naiJabijSbj, (1)\nwhere Sai is the spin operator for site a in unit cell i\nand Jabij is the exchange tensor that couples the spins\nat sites a, iand b, j. Single-ion anisotropy terms are in-\ncluded through the onsite components J aaii of the ex-\nchange tensor. In order to calculate the thermal prop-\nerties of the model (1) one needs the magnetic excita-\ntion spectrum, which requires one to determine the mag-\nnetic ground state. Except for simple ferromagnets, the\nground state of the mod", "quality_score": 8.5}}
{"id": "test_12", "question": "What is the purpose of the component that operates exclusively during the training phase?", "golden_answers": ["It generates blurred region-of-interest (ROI) prompts through its convolution-based stochastic scaling algorithm."], "prediction": "The purpose of the component that operates exclusively during the training phase is to allow the agent to interact with a simulated MAC layer environment, model the behavior of 5G NR-U and Wi-Fi transmitters, observe the current augmented state, select actions, and observe performance functions to optimize system performance.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00259v1\nThe training phase involves the agent in-\nteracting with a simulated MAC layer environment, which\nmodels the behavior of 5G NR-U and Wi-Fi transmitters\nunder realistic coexistence conditions. At each time step,\nthe agent observes the current augmented state, selects\nan action which is adjusting contention window sizes, and\nobserves performance functions. The constraint is carefully\ndesigned to penalize violations of QoS requirements. Expe-\nrience replay is used to store and sample past transitions,\nand a target network is updated periodically to stabilize\nlearning. Training is conducted across varying number of\ntransmitters to ensure the generalization. In the execution phase, the trained policy is deployed\nin the simulation environment to evaluate its effective-\nness. The augmented state representation ensures that the\nTABLE I\nHYPER -PARAMETERS OF QASAL ALGORITHM\nParameter Value\nInteraction time T 20 s\nStep duration 2.5 ms\nDiscount factor 0.99\nReplay buffer size 100,000\nRange of \u03f5 1 to 0.1\nDQN learning rate 10\u22124\nBatch size 16\nHidden layers 32 \u00d732 \u00d732\n\u03b7\u03bb,\u03bbmax,T0 in (10) 0.1, 10, 5\nagent adapts dynamically to changing network conditions,\nmaking real-time adjustments to contention window sizes\nto optimize system performance. The QaSAL algorithm\neffectively balances competing objectives, ensuring that\nhigh-priority traffic meets its delay requirements while\npromoting fairness between 5G NR-U and Wi-Fi networks. The hyperparameters of QaSAL algorithm are summarized\nin Table I. The step duration is selected to be large enough\nto include several transmission attempts to enable the\naccurate calculation of the transmission delay. VI. S IMULATION RESULTS\nIn this section, we highlight the performance of the\nproposed QaSAL algorithm. For the simulation scenario, we\nconsider one gNB PC1 transmitter sharing the channel with\nvarying number of AP PC3 transmitters, and evaluate the\nperformance of primal-dual method (Section III) and QaSAL\nalgorithm (Section IV). The simulations were conducted to\nanalyze the algorithm\u2019 s ability to balance fairness and delay\nmetrics across different numbers of Wi-Fi\u2019 s low-priority\ntransmitters. Figs. 3 and 4 illustrate the transmission delay dynamics\nfor gNB PC1 traffic coexisting with 25 AP PC3 transmitters\nduring both the training and execution phases of the\nprimal-dual and QaSAL algorithms, respectively. While the\n\nFig. 6. Average Transmission delay of PC1 transmitter with QaSAL algo-\nrithm for various delay thresholds and varying number of PC3 transmitters. Fig. 7. JFI among Wi-Fi and NR-U networks with QaSAL algorithm for\nvarious delay thresholds and varying number of PC3 transmitters. policy derived from the primal-dual algorithm can solve\nproblem (11) on average, it does not guarantee near-\noptimality at a specific episode k. In contrast, the QaSAL\nalgorithm effectively enforces the constraint by incorporat-\ning dual variables into the state space, leading to improved\nconstraint satisfaction. Fig. 5 illustrates the evolution of the\ndual variable associated with the delay constraint of gNB\nPC1, as defined in (11b), during the execution phase. The\ndual variable adjusts dynamically in response to constraint\nviolations, ensuring that the delay remains within the spec-\nified threshold. Figs.", "metadata": {"type": "factual", "source": "2503.01265v1.pdf", "context": "calization of suspected lesion regions within these scans. This\ncomponent operates exclusively during the training phase,\ngenerating blurred region-of-interest (ROI) prompts through\nits convolution-based stochastic scaling algorithm, which is\ndetailed in Algorithm 1. It defines a series of convolution\n1The Transformer module adopts the design of Restormer [36], a simplified\nTransformer that reduces the computational cost, enabling its use across\nmultiple levels.", "quality_score": 8.5}}
{"id": "test_13", "question": "According to Burrows et al. (2024), what two classes of progenitors lead to different ranges of natal kicks?", "golden_answers": ["Low mass and low compactness that lead to kicks of \u223c100\u2212200 km/s, and high mass and high compactness that lead to kicks of \u223c300\u22121000 km/s."], "prediction": "Low mass and low compactness, and high mass and high compactness.", "f1": 0.6111111111111112, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "A kinematically constrained kick distribution for isolated neutron stars\nSmall natal kicks could also be the result of interac-\ntions in binary stars, which are relevant since the population of\nisolated NS contains NSs that are born in binaries and have re-\nceived a kick large enough to unbind the system (although binary\ndisruptions due to mass loss or natal kicks result in different ve-\nlocities for the escaped pulsar, see e.g. Beniamini & Piran 2024). Mass transfer in a binary can strip the NS progenitor and lead to\nstripped and ultra-stripped SNe (Podsiadlowski et al. 2004; Tau-\nris et al. 2013, 2015), which in turn result in small natal kicks. However, it has been shown that these smaller kicks would not\nbe sufficient to unbind the binary, and hence that isolated NSs\nborn in binaries can only represent the upper tail of the kick dis-\ntribution (Kuranov et al. 2009). More recently, SN simulations\nsuggest that there are likely multiple ranges of zero-age main-\nsequence masses leading to NS formation (e.g. Ertl et al. 2016;\nM\u00fcller et al. 2016; Kresse et al. 2021). In particular, Burrows\net al. (2024) find two classes of progenitors: low mass and low\ncompactness that lead to kicks of\u223c100\u2212200 km/s and high mass\nand high compactness that lead to kicks of \u223c300\u22121000 km /s. These two classes could hypothetically correspond to the two\nMaxwellians found by Verbunt et al. (2017) and Igoshev (2020). The methods of Hobbs et al. (2005), Verbunt et al. (2017),\nand Igoshev (2020) constrain natal kicks by analysing the ve-\nlocities of young pulsars. This motivation of this choice is that a\nsample of old pulsars would be biased toward low kick in at least\ntwo ways: (1) the Galactic trajectories become more eccentric as\na result of the kicks, because of which the objects are more likely\nto be observed near their Galactic apocentre where they have re-\nduced speeds relative to their initial velocities (Hansen & Phin-\nney 1997; Disberg et al. 2024a), and (2) NSs that receive high\nkicks migrate outwards more quickly and therefore become less\nlikely to be observed as they age (Cordes 1986; Lyne & Lorimer\n1994). In order to eliminate the first of these biases, Disberg et al.\n(2024b) proposed a method of inferring kicks (i.e. the systemic\nkicks of binary NSs) based on the shape of their Galactic trajec-\ntory, which remains close to circular for low kicks and becomes\nmore eccentric for increasing kick magnitude. This method em-\nploys the entire Galactic orbit instead of the present-day circular\nvelocity to infer kicks and should therefore also provide rela-\ntively accurate estimates for older NSs. The advantages of such\nan approach are two-fold. Firstly, the use of older pulsars pro-\nvides a second, entirely independent sample of neutron stars on\nwhich the kick distribution can be determined while accounting\nfor the evolution of their velocity as they migrate through the\nGalactic potential.", "metadata": {"type": "factual", "source": "2503.01429v2.pdf", "context": "sequence masses leading to NS formation (e.g. Ertl et al. 2016;\nM\u00fcller et al. 2016; Kresse et al. 2021). In particular, Burrows\net al. (2024) find two classes of progenitors: low mass and low\ncompactness that lead to kicks of\u223c100\u2212200 km/s and high mass\nand high compactness that lead to kicks of \u223c300\u22121000 km /s.\nThese two classes could hypothetically correspond to the two\nMaxwellians found by Verbunt et al. (2017) and Igoshev (2020).\nThe methods of Hobbs et al. (2005), Verbunt et al. (2017),\nand ", "quality_score": 8.5}}
{"id": "test_14", "question": "How does SBI (Stochastic Bayesian Inference) operate in scenarios with missing data?", "golden_answers": ["SBI methods cannot operate on missing values, and so imputing xmis is necessary before proceeding to inference."], "prediction": "SBI methods typically require fully observed data to infer parameters, but in scenarios with missing data, naive imputation methods can introduce bias in the estimation of the SBI posterior. The document introduces a novel amortized method that jointly learns the imputation model and the inference network within a neural posterior estimation framework, providing robust inference outcomes compared to standard baselines for varying levels of missing data.", "f1": 0.15789473684210525, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01287v1\nPublished as a conference paper at ICLR 2025\nROBUST SIMULATION -BASED INFERENCE\nUNDER MISSING DATA VIA NEURAL PROCESSES\nYogesh Verma, Ayush Bharti\nDepartment of Computer Science, Aalto University\n{yogesh.verma, ayush.bharti}@aalto.fi\nVikas Garg\nYaiYai Ltd and Aalto University\nvgarg@csail.mit.edu\nABSTRACT\nSimulation-based inference (SBI) methods typically require fully observed data to\ninfer parameters of models with intractable likelihood functions. However, datasets\noften contain missing values due to incomplete observations, data corruptions\n(common in astrophysics), or instrument limitations (e.g., in high-energy physics\napplications). In such scenarios, missing data must be imputed before applying any\nSBI method. We formalize the problem of missing data in SBI and demonstrate that\nnaive imputation methods can introduce bias in the estimation of SBI posterior. We\nalso introduce a novel amortized method that addresses this issue by jointly learning\nthe imputation model and the inference network within a neural posterior estimation\n(NPE) framework. Extensive empirical results on SBI benchmarks show that our\napproach provides robust inference outcomes compared to standard baselines\nfor varying levels of missing data. Moreover, we demonstrate the merits of our\nimputation model on two real-world bioactivity datasets (Adrenergic and Kinase\nassays). Code is available at https://github.com/Aalto-QuML/RISE. 1 I NTRODUCTION\nMechanistic models for studying complex physical or biological phenomena have become indispens-\nable tools in research fields as diverse as genetics (Riesselman et al., 2018), epidemiology (Kypraios\net al., 2017), gravitational wave astronomy (Dax et al., 2021), and radio propagation (Bharti et al.,\n2022a). However, fitting such models to observational data can be challenging due to the intractabil-\nity of their likelihood functions, which renders standard Bayesian inference methods inapplicable.\nSimulation-based inference (SBI) methods (Cranmer et al., 2020) tackle this issue by relying on\nforward simulations from the model instead of evaluating the likelihood. These simulations are then\neither used to train a conditional density estimator (Papamakarios and Murray, 2016; Lueckmann\net al., 2017b; Greenberg et al., 2019; Papamakarios et al., 2019; Radev et al., 2020), or to measure dis-\ntance with the observed data (Sisson, 2018; Briol et al., 2019; Pesonen et al., 2023), to approximately\nestimate the posterior distribution of the model parameters of interest. SBI methods implicitly assume that the observed data distribution belongs to the family of distri-\nbutions induced by the model; i.e., the model is well-specified. However, this assumption is often\nviolated in practice where models tend to be misspecified since the complex real-world phenomena\nunder study are not accurately represented. Even if the model is well-specified, the data collection\nmechanism might hinder the applicability of SBI methods since it can induce missing data due to, for\ninstance, incomplete observations (Luken et al., 2021), instrument limitations (Kasak et al., 2024), or\nunfavorable experimental conditions.", "metadata": {"type": "factual", "source": "2503.01287v1.pdf", "context": "each data point x. As a result, x contains both observed and missing values, 1 represented as\nx = ( xobs, xmis). For instance, x = ( 0.1 1.2 \u2212 0.9) exemplifies a scenario where a specific\ncoordinate xi is missing (indicated by \u2018\u2212\u2019). Naturally, SBI methods cannot operate on missing values,\nand so imputing xmis is necessary before proceeding to inference. However, if the missing values are\nnot imputed accurately, then the corresponding SBI posterior becomes biased (e.g., as observed in\nFigure 1 du", "quality_score": 8.5}}
{"id": "test_15", "question": "What happens to the SBI posterior if the missing values are not imputed accurately?", "golden_answers": ["The corresponding SBI posterior becomes biased."], "prediction": "The SBI posterior becomes biased and drifts away from the true parameter value.", "f1": 0.5000000000000001, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01287v1\nAlthough the former problem of model misspecification has been studied in a number of works\n(Frazier et al., 2020; Dellaporta et al., 2022; Fujisawa et al., 2021; Bharti et al., 2022b; Ward\net al., 2022; Schmitt et al., 2023; Gloeckler et al., 2023; Huang et al., 2023; Gao et al., 2023;\nKelly et al., 2024), the latter problem of missing data in SBI has received relatively less atten-\ntion. A notable exception is the work of Wang et al. (2024), which attempts to handle miss-\ning data by augmenting and imputing constant values (e.g., zero or sample mean) and perform-\ning inference with a binary mask indicator. However, this approach can lead to biased esti-\nmates, reduced variability, and distorted relationships between variables (Graham et al., 2007). This is exemplified in Figure 1 where we investigate the impact of missing data on neural pos-\n1\narXiv:2503.01287v1  [cs.LG]  3 Mar 2025\n\nPublished as a conference paper at ICLR 2025\nterior estimation (NPE, Papamakarios and Murray (2016))\u2014a popular SBI method\u2014on a pop-\nulation genetics model. We observe that simply incorporating missing values and their corre-\nsponding masks in NPE methods as in Wang et al. (2024) leads to biased posterior estimates. 0 10\n1\n5\n10\n15\n202\nTrue \n= 0%\n= 10%\n= 25%\n= 60%\nFigure 1: Effect of missing data on SBI.\nNPE posterior for the two-parameter Ricker\nmodel (Wood, 2010) where the method of\nWang et al. (2024) (with zero augmentation)\nis used to handle \u03b5% of values missing in\nthe data. As \u03b5 increases, the NPE posteriors\nbecome biased and drift away from the true\nparameter value, denoted by the black lines. Other SBI works that address missing data include\nLueckmann et al. (2017a) and Gloeckler et al. (2024),\nhowever, they fail to account for the underlying mech-\nanism that leads to missing values in the data. Outside of SBI, the problem of missing data has\nbeen extensively studied (Van Buuren and Groothuis-\nOudshoorn, 2011), with Rubin (1976) categoriz-\ning it into three types: missing completely at ran-\ndom (MCAR), missing at random (MAR), and miss-\ning not at random (MNAR). Recent advances in\nmachine learning have led to the development of\nnovel methods for addressing this problem using\ngenerative adversarial networks (GANs, Luo et al.\n(2018); Yoon et al. (2018); Li et al. (2019); Yoon and\nSull (2020)), variational autoencoders (V AEs, Naz-\nabal et al. (2020); Collier et al. (2020); Mattei and\nFrellsen (2019); Ipsen et al. (2020); Ghalebikesabi\net al. (2021b)), Gaussian processes (Casale et al.,\n2018; Fortuin et al., 2020; Ramchandran et al., 2021;\nOng et al., 2024), and optimal transport (Muzellec\net al., 2020; Zhao et al., 2023; V o et al., 2024). These\nmethods offer new perspectives on the problem of\nmissing data imputation, but their application has\nbeen primarily limited to predicting missing values. Notably, they have not been developed for inference\nover missing values, which remains a significant challenge for SBI.\nContributions.", "metadata": {"type": "reasoning", "source": "2503.01287v1.pdf", "context": "each data point x. As a result, x contains both observed and missing values, 1 represented as\nx = ( xobs, xmis). For instance, x = ( 0.1 1.2 \u2212 0.9) exemplifies a scenario where a specific\ncoordinate xi is missing (indicated by \u2018\u2212\u2019). Naturally, SBI methods cannot operate on missing values,\nand so imputing xmis is necessary before proceeding to inference. However, if the missing values are\nnot imputed accurately, then the corresponding SBI posterior becomes biased (e.g., as observed in\nFigure 1 du", "quality_score": 8.5}}
{"id": "test_16", "question": "How can the Q-NL Verifier enhance the robustness of QA systems?", "golden_answers": ["By preventing incorrect query execution through self-assessment capability, which allows the system to take corrective actions such as refining the translation or prompting the user for clarification."], "prediction": "The Q-NL Verifier can enhance the robustness of QA systems by serving as an internal feedback mechanism that assesses whether the generated query translation is semantically equivalent to the input. If the verifier identifies a likely incorrect translation or has low confidence, the system can take corrective actions such as refining the translation, attempting an alternative parsing strategy, or prompting the user for clarification, thereby preventing incorrect query execution.", "f1": 0.4634146341463415, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01385v1\nThis task is central to QA systems. The\nidea is to fine-tune the LLMs, using supervised finetuning, where the model is directly trained to predict the correct\nSPARQL query token by token using the synthetically generated pairs. To test this, we conducted experiments (cf.\nSection 5.6) where we evaluated (i) the original model (LLM without finetuning), (ii) the fined-tuned model (LLM\nfinetuned with our synthetic examples), and (iii) a model that is trained on filtered data using the verifier. This allows us\nto understand the impact of each of the NL-Q Verifier components on the NL \u2192 Q task. Feedback within QA systems The verifier can serve as an internal feedback mechanism within a QA system. When\nthe system generates a query translation for a user question, the Q-NL Verifier can assess whether the translation\nis semantically equivalent to the input. If the verifier determines that the translation is likely incorrect or has low\nconfidence, the system can take corrective actions such as refining the translation, attempting an alternative parsing\nstrategy, or prompting the user for clarification. This self-assessment capability enhances the robustness of QA systems\nby preventing incorrect query execution. User support. Lastly, the verifier can be employed as an assistive tool for users who manually translate natural\nlanguage questions into SPARQL queries. Writing SPARQL queries requires familiarity with the underlying knowledge\ngraph schema, making it challenging for many users, especially those without extensive expertise in semantic web\ntechnologies. By providing real-time feedback, the verifier can indicate whether a user-constructed query accurately\nreflects the intent of their natural language question. This can be integrated into query-building interfaces, where the\nsystem suggests corrections or highlights potential mismatches between the intended meaning and the generated query. 5 Experimental Study\nIn the experiments, we first investigate research questions associated with the effectiveness of Q-NL Verifier:\n(RQ1) Which of the studied modern LLMs is the most effective for Q\u2192NL translation to train the verifier?\n(RQ2) Which verifier architecture (cross-encoder or bi-encoder) can better distinguish between correct and incorrect\ntranslations?\n(RQ3) Does the verifier improve the overall quality of Q/NL datasets? Next, we investigate research questions related to the application of Q-NL Verifier as described in Section 4.3:\n(RQ4) Can Q-NL Verifier be used as a replacement for NLP metrics for query translation?\n(RQ5) Does the verifier improve NL \u2192 Q translations with LLMs?\n5.1 Experimental Setup\nDataset. We use the well-known benchmark LC-QuAD 2.0 [6]. The dataset has a total of 24, 000 questions, a natural\nlanguage verbalization of those, and the corresponding SPARQL queries. The dataset has queries for both Wikidata and\nDBpedia 2018, but we focus solely on the Wikidata queries. The natural language verbalizations of the queries were\ngenerated using Amazon Mechanical Turk. The dataset contains various queries, including single facts, multi-joins,\nblank nodes, aggregations using COUNT, filtering of values with FILTER, etc.\n8\n\nQ-NL Verifier\nLanguage Models. We tested several different LLMs, from small open-source models to large proprietary frontier\nmodels.", "metadata": {"type": "reasoning", "source": "2503.01385v1.pdf", "context": "Section 5.6) where we evaluated (i) the original model (LLM without finetuning), (ii) the fined-tuned model (LLM\nfinetuned with our synthetic examples), and (iii) a model that is trained on filtered data using the verifier. This allows us\nto understand the impact of each of the NL-Q Verifier components on the NL \u2192 Q task.\nFeedback within QA systems The verifier can serve as an internal feedback mechanism within a QA system. When\nthe system generates a query translation for a user question, the Q", "quality_score": 8.5}}
{"id": "test_17", "question": "In which case does the combination of IC24 with SK-atm result in a lower \u2206\u03c72 value compared to IC19 without SK-atm?", "golden_answers": ["The IO case, as shown by comparing Tables 4 and 5"], "prediction": "The combination of IC24 with SK-atm results in a lower \u2206\u03c72 value compared to IC19 without SK-atm in the case of the normal hierarchy.", "f1": 0.06666666666666667, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00216v2\n( 19) are\nc1 =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n[\n1 +E4 +2E2(\u03b12 \u2212\u03b22 \u22121)\n(1 +\u03b12 +\u03b22 )2\n] 1\n2 [\n1 +E4 +2E2(\u03b12 +\u03b22 \u22121)\n(1 +\u03b12 +\u03b22 )2\n] 1\n2\n\u00d7\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n\u03b2\n(\n\u03b12 +2\u03b1\u03b2\u2212\u03b22\n)\n2\n\u221a\n\u03b12 +1\n+\n\u03b2\n(\n3\u03b12 +\u03b22 +2\n)\n\u22122\u03b1\n(\n\u03b12 +1\n)\n2\n\u221a\n\u03b12 +1(1 +\u03b12 +\u03b22 )\nE2\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\u2212 4\u03b24 +2\u03b1\u03b23 \u22124\u03b12 \u03b22\n(1 +\u03b12 +\u03b22 )\n3\n2\nE4 \u2212 \u03b23 (4\u03b13 \u22124\u03b23 +6\u03b1)\n(1 +\u03b12 +\u03b22 )\n5\n2\nE2\n+ (2\u03b1\u2212\u03b2)(3\u03b1+\u03b13 ) +4\n(1 +\u03b12 +\u03b22)\n3\n2\n+\u03b24 (2 \u2212\u03b1\u03b2) +\u03b1\u03b2(1 \u2212\u03b12)2 \u22122\u03b12(1 +\u03b12) \u22124\n(1 +\u03b12 +\u03b22 )\n5\n2\n+\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n\u03b1\n(\n\u03b12 +2\u03b1\u03b2\u2212\u03b22 +2\n)\n+2\u03b2\n(1 +\u03b12 +\u03b22 )\n3\n2\n\u03b2E3 \u2212 \u03b12 +2\u03b1\u03b2\u2212\u03b22\n\u221a\n1 +\u03b12 +\u03b22\n\u03b1\u03b2E\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\u00d7\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f01 +\nE4 +2E2 (\n\u03b12 \u2212\u03b22 \u22121\n)\n(1 +\u03b12 +\u03b22 )2\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n1\n2\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n[\n1 +E4 +2E2(\u03b12 +\u03b22 \u22121)\n(1 +\u03b12 +\u03b22 )2\n] \u22121\n. (B1)\n\n6\nc2 =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\u2212\n(\n\u03b13 +2\u03b12 \u03b2\u2212\u03b2\u03b12 )\n\u221a\n\u03b12 +\u03b22 +1\n4(1 +\u03b12 )\n\u2212\n\u03b1\n(\n\u03b12 +2\u03b1\u03b2\u2212\u03b22 +2\n)\n+2\u03b2\n2\n\u221a\n1 +\u03b12 (1 +\u03b12 +\u03b22 )\n3\n2\nE4\n\u2212\n\u03b12 \u03b2\n(\n3\u03b12 +4\n)\n\u2212\n(\n\u03b15 +\u03b13\n)\n+\u03b12 \u03b23 +2\u03b1\u03b22\n(\u03b12 +1)(1 +\u03b12 +\u03b22 )\n3\n2\nE3\n+ \u03b13 +2\u03b12 \u03b2\u2212\u03b1\u03b22 +\u03b1+\u03b2\u221a\n(\u03b12 +1)(1 +\u03b12 +\u03b22 )\nE2\n\u2212\n\u03b12 \u03b2\n(\n3\u03b12 +2\n)\n\u22122\u03b1\n(\n\u03b12 +1\n)2\n+\u03b12 \u03b23 \u22122\u03b1\u03b22\n(\u03b12 +1)\n\u221a\n1 +\u03b12 +\u03b22\nE\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n\u00d7\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f01 +\nE4 +2E2\n(\n\u03b12 \u2212\u03b22 \u22121\n)\n(\u03b12 +\u03b22 +1)2\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\u22121\n2\n. (B2)\n[1] R. Saadati, F . Hammad, S. Novoa-Cattivelli, M. Simard and\nN. Fleury , \u201cSpin precession and neutrino helicity \ufb02ip in var ious\nspacetimes, \u201d [\narXiv:2502.15593]. [2] P . M\u00b4 esz\u00b4 aros, D. B. Fox, C. Hanna and K. Murase, \u201cMulti-\nmessenger astrophysics, \u201d\nNat. Rev . Phys. 1, 585 (2019)\n[arXiv:1906.10212]. [3] R. Abbasi et al. , \u201c A Search for IceCube sub-T eV Neutri-\nnos Correlated with Gravitational-W ave Events Detected By\nLIGO/Virgo, \u201d\nApJ 959, 96 (2023) ; Erratum: ApJ 971, 192\n(2024) [arXiv:2303.15970]. [4] M. Dvornikov , \u201cNeutrino spin oscillations in external \ufb01eld s\nin curved spacetime, \u201d Phys. Rev . D 99, 116021 (2019)\n[arXiv:1902.11285]. [5] M. Mathisson, \u201cNeue Mechanik materieller Systeme, \u201d Acta\nPhys. Polon. 6, 163 (1937) [Translated and republished in Gen. Relativ . Gravit. 42, 1011 (2010) ]. [6] A.", "metadata": {"type": "reasoning", "source": "2503.01399v2.pdf", "context": "Table 4: Constraints on neutrino properties under the two-zero texture condition (m\u03bd)\u00b5\u00b5 = (m\u03bd)\u03c4\u03c4 =\n0 in the NO case.\nOscillation m\u03b2 Cosmology \u2206\u03c72(NO) Confidence Level\nIC19 w/o SK-atm (NuFIT 6.0)\nNuFIT 6.0 - - 3.6 94% CL\nNuFIT 6.0 KATRIN - 4.1 94% CL\u2020\nNuFIT 6.0 - Planck 13 3.2\u03c3\nNuFIT 6.0 - Planck + DESI 57 7.2\u03c3\nIC24 with SK-atm (NuFIT 6.0)\nNuFIT 6.0 - - 2.0 84% CL\nNuFIT 6.0 KATRIN - 2.6 84% CL\u2020\nNuFIT 6.0 - Planck 12 3.0\u03c3\nNuFIT 6.0 - Planck + DESI 55 7.1\u03c3\nTable 5: Same as Tab. 4, but for the IO ca", "quality_score": 8.5}}
{"id": "test_18", "question": "Using Leibniz's formula and equation (2.8), how does the differentiation result change when m \u2265 2?", "golden_answers": ["The result changes to Pm(s,t) + 2msPm\u22121(s,t) + ((m-1)/m)(s^2 + t^2)Pm\u22122(s,t) / (\u221a(s^2 + t^2))^(2m-1)"], "prediction": "The differentiation result changes when m \u2265 2 by introducing additional terms that account for the higher-order derivatives of the functions involved, as indicated by the application of Leibniz's formula which involves products of derivatives. Specifically, for m \u2265 2, the differentiation will yield terms that involve combinations of derivatives of the functions up to order m, reflecting the increased complexity of the interactions between the functions being differentiated.", "f1": 0.08695652173913045, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01270v1\nTaking the L2-scalar product of (5.3) with \u03c9\u03b1 \u2212 \u03c9 we have\nd\ndt\n(\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2\n)\n= \u22122\n\u222b\n(u\u03b1 \u2212 u) \u00b7 \u2207\u03c9 (\u03c9\u03b1 \u2212 \u03c9) dx\n\u2212 2\u03b1\n\u222b\n\u2207(u \u00b7 \u2207\u03c9)(\u2207\u03c9\u03b1 \u2212 \u2207\u03c9) dx.\nThus, by integrating in time and using (1.6) we obtain\n(5.4)\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2 \u2272 \u2225\u03c9\u03b1\n0 \u2212 \u03c90\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1\n0 \u2212 \u2207\u03c90\u22252\n2\n+\n\u222b t\n0\n\u222b\n|u\u03b1 \u2212 u||\u2207\u03c9||\u03c9\u03b1 \u2212 \u03c9| dxd\u03c4\n+ \u03b1\n\u222b t\n0\n\u222b\n|\u2207u||\u2207\u03c9||\u2207\u03c9\u03b1 \u2212 \u2207\u03c9| dxd\u03c4\n+ \u03b1\n\u222b t\n0\n\u222b\n|u||D2\u03c9||\u2207\u03c9\u03b1 \u2212 \u2207\u03c9| dxd\u03c4. By using H\u00a8 older inequality and (2.6) we have that\n\u222b t\n0\n\u222b\n|u\u03b1 \u2212 u||\u2207\u03c9||\u03c9\u03b1 \u2212 \u03c9| dxds \u2272\n\u222b t\n0\n\u2225u\u03b1 \u2212 u\u22254\u2225\u2207\u03c9\u22254\u2225\u03c9\u03b1 \u2212 \u03c9\u22252 d\u03c4\n\u2272\n\u222b t\n0\n\u2225u\u03b1 \u2212 u\u2225\n1\n2\n2 \u2225u\u03b1 \u2212 u\u2225\n1\n2\n1, 2\u2225u\u2225\n1\n2\n2, 2\u2225u\u2225\n1\n2\n3, 2\u2225\u03c9\u03b1 \u2212 \u03c9\u22252 d\u03c4\n\u2272\n\u222b t\n0\n\u2225u\u2225s, 2\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 ds \u2272 Cs,T\nu0\n\u222b t\n0\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 d\u03c4 \u2272\n\u222b t\n0\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 d\u03c4. Moreover, by Proposition 2.5, (1.6), Sobolev embedding, an d Young\u2019s inequality we have\nthat\n\u03b1\n\u222b t\n0\n\u222b\n|\u2207u||\u2207\u03c9||\u2207\u03c9\u03b1 \u2212 \u2207\u03c9| dxd\u03c4 \u2272 \u03b1\n\u222b t\n0\n\u2225\u2207u\u2225\u221e\u2225\u2207\u03c9\u22252\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252 d\u03c4\n\u2272 T (Cs,T\nu0 )4\u03b1 + \u03b1\n\u222b t\n0\n\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2 d\u03c4\n\u2272 \u03b1 + \u03b1\n\u222b t\n0\n\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2 d\u03c4,\n\nCONVERGENCE OF THE EULER-VOIGT APPROXIMATION 14\nand\n\u03b1\n\u222b t\n0\n\u222b\n|u||D2\u03c9||\u2207\u03c9\u03b1 \u2212 \u2207\u03c9| dxd\u03c4 \u2272 \u03b1\n\u222b t\n0\n\u2225u\u2225\u221e\u2225u\u2225s, 2\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252 d\u03c4\n\u2272 T (Cs,T\nu0 )4\u03b1 + \u03b1\n\u222b t\n0\n\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2 d\u03c4\n\u2272 \u03b1 + \u03b1\n\u222b t\n0\n\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2 d\u03c4. Collecting the previous estimate we obtain\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2 \u2272\n\u222b t\n0\n(\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1 \u2212 \u2207\u03c9\u22252\n2\n)\nd\u03c4\n+ \u03b1 + \u2225\u03c9\u03b1\n0 \u2212 \u03c90\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1\n0 \u2212 \u2207\u03c90\u22252\n2,\nand by Gr\u00a8 onwall lemma\n\u2225\u03c9\u03b1 \u2212 \u03c9\u22252\n2 \u2272 (\u03b1 + \u2225\u03c9\u03b1\n0 \u2212 \u03c90\u22252\n2 + \u03b1\u2225\u2207\u03c9\u03b1\n0 \u2212 \u2207\u03c90\u22252\n2)\n1\n2 . Next, we consider the case 2 < s < 3. The argument is slightly more involved. Given\nu0 \u2208 Hs(T2) with 2 < s < 3, we de\ufb01ne uN\n0 as\nuN\n0 (x) =\n\u2211\n0<|k|\u2264N\n\u02c6u0k eik\u00b7x,\nwhere \u02c6u0k is the Fourier coe\ufb03cients of u0 associated with the wave-number k. Given\ns \u2208 (2, 3) it holds that\n(5.5)\n\u2225uN\n0 \u2225s, 2 \u2264 \u2225 u0\u2225s, 2,\n\u2225uN\n0 \u2225s\u2032, 2 \u2264 Ns\u2032\u2212s\u2225u0\u2225s, 2, s \u2032 > s,\n\u2225uN\n0 \u2212 u0\u2225\u00afs, 2 \u2264 \u2225u0\u2225s, 2\nNs\u2212\u00afs , 0 \u2264 \u00afs <", "metadata": {"type": "reasoning", "source": "2503.01246v1.pdf", "context": "\u2202m\ns\n(\n(s2 + t2) 1\u221a\ns2 + t2\n)\n= \u2202m\u22121\ns\n( s\u221a\ns2 + t2\n)\n.\nApplying Leibniz\u2019s formula and (2.8), we deduce that for m\u2265 2\n\u2202m\ns\n(\n(s2 + t2) 1\u221a\ns2 + t2\n)\n= Pm(s,t ) + 2 msPm\u22121(s,t ) + ( m\u2212 1)m(s2 + t2)Pm\u22122(s,t )\n(\u221a\ns2 + t2) 2m\u22121", "quality_score": 8.5}}
{"id": "test_19", "question": "What is the expression for the sampling complexity after applying the union bound?", "golden_answers": ["m = \u2126 min p\u2208[k] max p2s2(p) logn, ks(p) logn"], "prediction": "m = \u2126\n{\nmin\np\u2208[k]\nmax\n[p2s2(p) logn, ks(p) logn]\n}", "f1": 1.0, "em": 1.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01335v1\nFor any constant 0 <\n\u03b4 <1 satisfying\n\u03b42 \u2265 2 \u2212 2\ns\u0012\n1 \u2212 \u03b32\n4\n\u00133\n+ \u03b32\n4 , (48)\nStep 4 produces a signal estimation z falling into the\n\u03b4-neighborhood of x, i.e.,\ndist(z, x) \u2264 \u03b4\u2225x\u2225 (49)\nwith probability exceeding 1 \u2212 2e\u2212c\u03b4m providing that\nm \u2265 C\u03b4k log n, where c\u03b4 and C\u03b4 are numerical constants\ndepending on \u03b4. Now we are proceeding to prove the theorems in Section III.\nProof of Theorem 1. Collecting all the results in Proposition 1\n\u2013 4 with p\u2032 = p, we can show that the sampling complexity is\nm = \u2126\n\u0000\nmax\n\b\np2s2(p), ks(p)\n\t\nlog n\n\u0001\n, (50)\nand the overall probability is 1 \u2212 e\u2212cm. Since we take the\noptimal value for p as\npopt = arg minp\u2208[k] max\n\b\np2s2(p), ks(p)\n\t\n,\nwe obtain the desired results. Proof of Theorem 2. For each fixed p \u2208 [k], collecting all the\nresults in Proposition 1 \u2013 4 with p\u2032 = p, we can show that\nthe final estimate z falls into the \u03b4-neighborhood of x with\nprobability exceeding 1 \u2212 e\u2212cm when\nm = \u2126\n\u0000\nmax\n\b\np2s2(p), ks(p)\n\t\nlog n\n\u0001\n. To ensure that at least one estimate z satisfies dist(z, x) \u2264\n\u03b4\u2225x\u2225, we can apply the union bound for all p \u2208 [k]. Then the\nsampling complexity can be reduced to\nm = \u2126\n\u0012\nmin\np\u2208[k]\nmax\n\b\np2s2(p) logn, ks(p) logn\n\t\u0013\n,\nand the probability is at least 1\u2212ke\u2212cm. It is actually 1\u2212e\u2212cm\nif we take a sufficiently large constant c since m > k. Thus\nthe proof is complete. Proof of Theorem 3. For any p \u2208 [k], denote S\u2032 as the set\ncorresponding to the largest p diagonal elements of Z. Note\nthat S0 corresponds to the largest k diagonal elements of Z.\nIt holds that\n\u2225xS0 \u22252\n\u2225x\u22252\n(a)\n\u2265 \u2225xS\u2032\u22252\n\u2225x\u22252\n(b)\n\u2265 1\n2s(p), (51)\nwhere (a) is because k \u2265 p, and (b) holds via applying\nProposition 1 on the set S\u2032 when \u2265 Cp2s2(p) logn.\nThen, from the results above and in Proposition 2 \u2013 4 (with\np\u2032 = k), it holds that for any p \u2208 [k], when\nm = \u2126\n\u0000\nmax\n\b\np2s2(p) logn, ks2(p) logn\n\t\u0001\n, (52)\nthe final estimate z falls into the \u03b4-neighborhood of x with\nprobability exceeding 1 \u2212 e\u2212cm. This result also holds for all\np \u2208 [k] since we can take the union bound, and the probability\nwill be 1 \u2212 ke\u2212cm. It is actually 1 \u2212 e\u2212cm if we take a\nsufficiently large constant c since m > k.\nNote that the choice of p \u2208 [k] is arbitrary. Therefore, we\ncan choose the best p to make the sampling complexity (52)\nsmallest, which becomes\nm = \u2126\n\u0012\nmin\np\u2208[k]\nmax\n\b\np2s2(p) logn, ks2(p) logn\n\t\u0013\n, (53)\nThe remaining thing is to show that (53) is equivalent to (43). As stated following Definition 2, ps(p) is monotonously\nincreasing, and s(p) is monotonously decreasing.", "metadata": {"type": "factual", "source": "2503.01335v1.pdf", "context": "\u03b4\u2225x\u2225, we can apply the union bound for all p \u2208 [k]. Then the\nsampling complexity can be reduced to\nm = \u2126\n\u0012\nmin\np\u2208[k]\nmax\n\b\np2s2(p) logn, ks(p) logn\n\t\u0013\n,\nand the probability is at least 1\u2212ke\u2212cm. It is actually 1\u2212e\u2212cm\nif we take a sufficiently large constant c since m > k. Thus\nthe proof is complete.\nProof of Theorem 3. For any p \u2208 [k], denote S\u2032 as the set\ncorresponding to the largest p diagonal elements of Z. Note\nthat S0 corresponds to the largest k diagonal elements of Z.\nIt holds that\n\u2225xS0 \u22252", "quality_score": 8.5}}
{"id": "test_20", "question": "What is the purpose of the Under-Segmentation Score (USS) and the Over-Segmentation Score (OSS)?", "golden_answers": ["To evaluate if the model fails to detect certain regions or assigns excessive labels to a class."], "prediction": "The purpose of the Under-Segmentation Score (USS) is to evaluate if the model fails to detect certain regions, leading to under-segmentation, while the Over-Segmentation Score (OSS) assesses if the model over-predicts a class, leading to over-segmentation.", "f1": 0.4888888888888889, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01248v1\nY|`| D1\nY \u02c6Y | |D\u02c6Y Y|`| DY \u02c6Y | (8)\nSpecial attention is needed for fluid evaluation. For True\nNegative(TN) cases where both ground truth and predicted\nfluid are absent, the NSD score should be the correct pre-\ndiction. For False Positive(FP) and False Negative(FN) cases\nwhere the fluid is only present in one of the ground truths\nor predicted segmentations, the NSD score should be zero as\nthe incorrect prediction. We set \u03c4 to 10 pixels for all classes,\nroughly 3% of the shortest image edge. The model perfor-\nmance will be evaluated without any of the post-processing\nsteps mentioned in the original papers. Additionally, we define the Under-Segmentation Score\n(USS) and the Over-Segmentation Score (OSS) to evaluate if\nthe model fails to detect certain regions or assigns excessive\nlabels to a class. Given the confusion matrix for N classes:\nCM \u201c\n\u00bb\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\nTP1 FP1,2 FP1,3 . . .FP1,N FN2,1 TP2 FP2,3 . . .FP2,N FN3,1 FN3,2 TP3 . . .FP3,N\n... ... ... ... ...\nFNN,1 FNN,2 FNN,3 . . . TPN\nfi\nffiffiffiffiffiffiffifl\n(9)\nWe compute the USS and OSS for certain class C as:\nUSS C \u201c\n\u0159\n@j\u2030C CMrC, js\u0159CMrC, :s (10)\nOSSC \u201c\n\u0159\n@i\u2030C CMri, Cs\u0159CMr:, Cs (11) A higher USS score indicates that a significant portion of\nthe ground truth class C was not detected, leading to under-\nsegmentation, while a higher OSS score suggests that the\nmodel over-predicts class C, leading to over-segmentation. G. Thickness Analysis\nLayer thickness computation is performed using the K-\nNearest Neighbors (K-NN) algorithm. The layer boundaries\nare converted into 3D point clouds. For each data point on\nthe upper layer, the closest corresponding point on the lower\nlayer is identified based on Euclidean distance. Due to their\nunbounded nature on one side, the Vitreous and Choroid layers\nare excluded from these calculations. Given the anatomical\ncomplexity of the foveal pit, the central region thickness\nis excluded from the analysis to ensure more reliable and\ninterpretable measurements. The thickness of fluid and hyperreflective foci (HRF) re-\ngions is computed through direct accumulation, which is\nsubsequently projected onto an en-face view for visualization. The Early Treatment Diabetic Retinopathy Study (ETDRS)\ngrid is employed to assess thickness variations systematically\nacross different macular regions. As depicted in Figure 3,\nthis grid divides the macula into three concentric circles with\ndiameters of 1mm, 3mm, and 6mm, all centered on the fovea. These circles define the central, inner, and outer subfields,\nsubdivided into four quadrants: superior, inferior, nasal, and\ntemporal.\nFig. 3: ETDRS diagram for both left and right eyes. The diameters of\nthe central circle, inner ring, and outer ring were 1 mm, 3 mm, and 6\nmm, respectively. Nine subfields are numbered and named as follows:\n1-CS(Central Subfield), 2-SI(Superior Inner), 3-NI(Nasal Inner), 4-\nII(Inferior Inner), 5-TI(Temporal Inner), 6-SO(Superior Outer), 7-\nNO(Nasal Outer), 8-IO(Inferior Outer), 9-TO(Temporal Outer). III. R ESULTS\nA. Segmentation\nTable II presents the segmentation results for four models,\nwith values averaged across five-fold cross-validation. Tables\nIIa and IIb separately report the DSC and NSD metrics.", "metadata": {"type": "factual", "source": "2503.01248v4.pdf", "context": "the original papers.\nAdditionally, we defined the Under-Segmentation\nScore (USS) and the Over-Segmentation Score (OSS)\nto evaluate if the model fails to detect certain regions or\nassigns excessive labels to a class. Given the confusion\nmatrix for N classes:\nCM \u201c\n\u00bb\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\nTP1 FP1,2 FP1,3 . . .FP1,N\nFN2,1 TP2 FP2,3 . . .FP2,N\nFN3,1 FN3,2 TP3 . . .FP3,N\n... ... ... ... ...\nFNN,1 FNN,2 FNN,3 . . . TPN\nfi\nffiffiffiffiffiffiffifl\n(9)\nWe computed the USS and OSS for certain class C\nas:\nUSS C \u201c\n\u0159\n@j\u2030C", "quality_score": 8.5}}
{"id": "test_21", "question": "What is a crucial property of any magnetic material, according to the text?", "golden_answers": ["The critical temperature for magnetic order"], "prediction": "A crucial property of any magnetic material is its ability to undergo rapid changes in quadrupolar ellipticity, which can lead to glitch-like activity.", "f1": 0.07407407407407407, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01409v2\nTwo of the components identified as important by Kojima\n(2024) for the failure problem, namely those in the r\u03b8 (left)\nand \u03b8\u03b8 \u2212 \u03d5\u03d5 (right) directions, are shown in Figure 6 for\nBp = 0.5b. Attention has been restricted to the \u2018crust\u2019 0 .9 \u2a7d\nr \u2a7d 1 and, for simplicity, only poloidal stresses are shown\n(i0 = 0). The ratios span the range 0 .87 \u2272 R \u2272 1.08 with\nthe lower and upper limits reached near polar ( \u03b8 \u2248 0, \u03c0) and\nequatorial (\u03b8 \u2248 \u03c0/2) colatitudes, respectively, at the base of\nthe crust ( r \u2248 0.9). The fact that R exceeds unity near the\nequator implies one may expect more frequent failures there\nrelative to the Maxwell case, with the reverse applying near\nthe poles, since the elastic stress should counterbalance the\nelectromagnetic stress. This is likely to skew the anticipated\nwaiting time distribution for flares to later times, as Hall\ndrift tends to tangle field lines near the pole and hence build\nstresses there (see figure 2 in Perna & Pons 2011). This will\nbe investigated in future work. 5.5 Magnetic (anti)-glitches\nAs first described by Ioka (2001), rapid changes to the\nquadrupolar ellipticity of a magnetar may bring about glitch-\nlike activity. By angular momentum conservation, a sudden\nreequilibriation \u03f5i \u2192 \u03f5f would lead to a fractional change in\nf of order\n\u03b4f f \u2248 2\n3 (\u03f5i \u2212 \u03f5f) . (49)\nWhile this model was discussed by Garcia & Ranea-Sandoval\n(2015) in the context of theanti-glitch seen in 1E 2259+586, a\npositive glitch is also possible if \u03f5i > \u03f5f (Mastrano, Suvorov &\nMelatos 2015). In the latter case, a dynamical decay (growth)\nin the poloidal (toroidal) sector could be expected. Such a\nrapid rearrangement would likely be accompanied by high-\nenergy activity; for instance, crustal yielding could lead to\na local relaxation in the magnetic field and hence a small\nchange in \u03f5. In the BI theory, maximum values for the toroidal elliptic-\nity also imply an upper limit to the size of a magnetically-\ninduced glitch. For SGR 1935+2154 with Bp \u2248 4 \u00d7 1014 G,\nresults from Tab. 4.1 indicate a maximum shift of \u2248 5\u00d710\u22126\nfor b = 1016 G. Using equation (49), this implies a maximum\nglitch magnitude of \u03b4f \u2248 (b/1016 G)2 \u00b5Hz for P = 3.25 s.\nMNRAS 000, 1\u201315 (?)\n\n12 Arthur G. Suvorov & Jos\u00b4 e A. Pons\nFigure 6.Dimensionless ratios of BI-to-Maxwell stresses (48) in the r\u03b8 (left) and \u03b8\u03b8 \u2212 \u03d5\u03d5 (right) directions within a \u2018crust\u2019 for the field\n(27), with redder shades indicating a greater ratio R. The classical polar strength is fixed as Bp = b/2. This is about half the size of the large glitch seen in this\nobject prior to the release of a fast radio burst in 2020 Oc-\ntober ( \u03b4f \u2248 1.8+0.7\n\u22120.5 \u00b5Hz; Younes et al. 2023) suggesting a\nfloor b \u2a7e\n\u221a\n2 \u00d7 1016 G, assuming magnetic rearrangement\nwas responsible for the burst and glitch. 6 DISCUSSION", "metadata": {"type": "factual", "source": "2503.01283v2.pdf", "context": "Predicting the N\u00b4 eel temperatures in general helimagnetic materials: a comparison\nbetween mean field theory, random phase approximation, renormalized spin wave\ntheory and classical Monte Carlo simulations\nVarun Rajeev Pavizhakumari 1 and Thomas Olsen 1, \u2217\n1CAMD, Computational Atomic-Scale Materials Design, Department of Physics,\nTechnical University of Denmark, 2800 Kgs. Lyngby Denmark\nThe critical temperature for magnetic order comprises a crucial property of any magnetic material\nand ranges f", "quality_score": 8.5}}
{"id": "test_22", "question": "Why does it suffice to analyze the sheaf \u03c0\u2217Y(HDR,Y) on \u2126?", "golden_answers": ["By Corollary 2.35, since (\u03c0\u2217Y(HDR,Y))\u0393 Y(I) = HanDR,Y."], "prediction": "It suffices to analyze the sheaf \u03c0\u2217Y(HDR,Y) on \u2126 because, by Corollary 2.35, Han DR,Y is equal to (\u03c0\u2217Y(HDR,Y))\u0393 Y(I), which indicates that understanding \u03c0\u2217Y(HDR,Y) on \u2126 will provide the necessary information to describe Han DR,Y.", "f1": 0.24390243902439024, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01357v1\nWe denote by HDR,un the unique locally free sheaf extension of HDR,un to M2\nI\nsuch that its formal completion at M2\nI\\M2\nI is determined by the module \u2295nI\ni=1(H[[Xi]]\u03b7i,1 \u2295\nH[[Xi]]\u03b7i,2). 7.2. The de Rham sheaf on \u2126 . Let Bbe an a\ufb03noid algebra so that Sp( B) is an admissible\nopen subset of \u2126. Recall the Drinfeld module ( Ga,\u2126 ,\u03a8 Y) from \u00a72.8 and \u03a8 Y\n| Sp(B) denote its\nrestriction to Sp( B), so that\n\u03a8 Yz\na| Sp(B) = a+\n2 deg(a)\u2211\ni=1\n(gi,a)| Sp(B) \u03c4i \u2208 B[\u03c4].\nRecall the quotient map \u03c0 from \u00a72.6. We denote by \u03c0Y : \u2126 \u2192 MY the composite map\n\u2126\n\u03c0\n\u2212 \u2192\u0393Y(I)\\\u2126\niY\n\u2212 \u2192MY. Let HDR,Y be the restriction of HDR,un, after base change with C\u221e, to MY and Han\nDR,Y be its\nanalyti\ufb01cation. In what follows, we explicitly describe the pull backs i\u2217\nY(HDR,Y) = Han\nDR,Y\nand \u03c0\u2217\nY(HDR,Y) which are sheaves on \u0393 Y(I)\\\u2126 and \u2126 respectively. Note, by Corollary\n2.35,\nthat Han\nDR,Y = ( \u03c0\u2217\nY(HDR,Y))\u0393 Y(I). Therefore it su\ufb03ces to analyze the sheaf \u03c0\u2217\nY(HDR,Y) on \u2126.\nProposition 7.5. The sheaf \u03c0\u2217\nY(HDR,Y) is the unique sheaf on \u2126 so that for any a\ufb03noid\nsubdomain j : Sp( B) \u2192 \u2126 , its sections over Sp(B) is given by the B-module HDR(\u03a8 Y\n| Sp(B)).\nProof. The morphism \u03c0Y \u25e6 j : Sp( B) \u2192 MY factors through a canonical map \u02dcj : Spec( B) \u2192\nMY. Note that the coherent sheaves j\u2217(\u03c0\u2217\nY(HDR,Y)) on Sp( B) and ( \u02dcj)\u2217HDR,Y on Spec( B)\ncorrespond to the same \ufb01nitely generated B-module, say N. In other words, they are\nobtained by the \u02dc() operation on N, with respect to the corresponding topologies. But\nby the a\ufb03ne base change property of de Rham cohomology [\nGek89, Thm.4.5], we have\n(\u02dcj)\u2217HDR,Y = HDR(\u03a8 Y\n| Sp(B)) which implies the desired statement. \u25a1\nIn what follows, we denote the sheaf \u03c0\u2217\nY(HDR,Y) by HDR(\u03a8 Y). Since HDR,Y is a locally free\nsheaf of rank 2 on MY, so is HDR(\u03a8 Y) on \u2126. Furthermore, since \u2126 is a Stein space [\nSS91,\nThm.4], HDR(\u03a8 Y) is generated by its global sections. In our next proposition, we ex plicitly\ndescribe these sections to show that HDR(\u03a8 Y) is indeed a free sheaf of rank two over O\u2126 . 44 O \u02d8GUZ GEZM \u02d9IS \u00b8 AND SRIRAM CHINTHALAGIRI VENKATA\nLet \u03b71,B be the local section of HDR(\u03a8 Y\n| Sp(B)) given by\n\u03b71,B := \u03b7(1)\nB := [ a\u21a6\u2192a\u2212 \u03a8 Y\na| Sp(B)] \u2208 H0(Sp(B),HDR(\u03a8 Y\n| Sp(B))). Recall the false Eisenstein series E de\ufb01ned in \u00a74. We further de\ufb01ne the local section \u03b72,B of\nHDR(\u03a8 Y\n| Sp(B)) given by\n\u03b72,B := d\ndz(\u03b71,B) \u2212 E\u03b71,B \u2208 H0(Sp(B),HDR(\u03a8 Y\n| Sp(B))). Note that for a\ufb03noid subdomains j : Sp( B) \u2192 \u2126, the local sections \u03b71,B and ( \u03b72,B respec-\ntively) glue together to form a global section of HDR(\u03a8 Y), denoted by \u03b71 (\u03b72 respectively). Lemma 7.6 (Hodge decomposition) .", "metadata": {"type": "reasoning", "source": "2503.01357v1.pdf", "context": "Let HDR,Y be the restriction of HDR,un, after base change with C\u221e, to MY and Han\nDR,Y be its\nanalyti\ufb01cation. In what follows, we explicitly describe the pull backs i\u2217\nY(HDR,Y) = Han\nDR,Y\nand \u03c0\u2217\nY(HDR,Y) which are sheaves on \u0393 Y(I)\\\u2126 and \u2126 respectively. Note, by Corollary\n2.35,\nthat Han\nDR,Y = ( \u03c0\u2217\nY(HDR,Y))\u0393 Y(I). Therefore it su\ufb03ces to analyze the sheaf \u03c0\u2217\nY(HDR,Y) on \u2126.\nProposition 7.5. The sheaf \u03c0\u2217\nY(HDR,Y) is the unique sheaf on \u2126 so that for any a\ufb03noid\nsubdomain j : Sp( B) \u2192 \u2126 , its section", "quality_score": 8.5}}
{"id": "test_23", "question": "What is assumed about static friction in the PD controller for simplicity?", "golden_answers": ["Static friction is typically greater than dynamic friction, but it is assumed they are equal."], "prediction": "It is assumed that static friction is negligible in the PD controller for simplicity.", "f1": 0.42857142857142855, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01255v1\nImpact of Static Friction on Sim2Real in Robotic Reinforcement\nLearning\nXiaoyi Hu1, Qiao Sun 1, Bailin He 1, Haojie Liu 1, Xueyi Zhang 1, Chunpeng lu 1 and Jiangwei Zhong 1\nAbstract\u2014 In robotic reinforcement learning, the Sim2Real\ngap remains a critical challenge. However, the impact of Static\nfriction on Sim2Real has been underexplored. Conventional\ndomain randomization methods typically exclude Static friction\nfrom their parameter space. In our robotic reinforcement learn-\ning task, such conventional domain randomization approaches\nresulted in significantly underperforming real-world models. To address this Sim2Real challenge, we employed Actuator\nNet as an alternative to conventional domain randomization. While this method enabled successful transfer to flat-ground\nlocomotion, it failed on complex terrains like stairs. To fur-\nther investigate physical parameters affecting Sim2Real in\nrobotic joints, we developed a control-theoretic joint model and\nperformed systematic parameter identification. Our analysis\nrevealed unexpectedly high friction-torque ratios in our robotic\njoints. To mitigate its impact, we implemented Static friction-\naware domain randomization for Sim2Real. Recognizing the\nincreased training difficulty introduced by friction modeling,\nwe proposed a simple and novel solution to reduce learning\ncomplexity. To validate this approach, we conducted com-\nprehensive Sim2Sim and Sim2Real experiments comparing\nthree methods: conventional domain randomization (without\nStatic friction), Actuator Net, and our Static friction-aware\ndomain randomization. All experiments utilized the Rapid\nMotor Adaptation (RMA) algorithm. Results demonstrated that\nour method achieved superior adaptive capabilities and overall\nperformance. I. INTRODUCTION In the realm of robotic reinforcement learning, Sim2Real\nproblem is a consistently significant challenge. This gap\noften results in robots that perform admirably in simulated\nenvironments but underperform in real-world settings. While\nit is possible for robots to learn directly from real-world\nsampling, this approach is fraught with drawbacks, including\npotential harm to the robot and exorbitant costs [1]. To en-\nhance the data efficiency of reinforcement learning, Model-\nBased Reinforcement Learning (MBRL) presents a viable\nalternative [2]. However, MBRL shares a common issue with\nModel Predictive Control (MPC), where the efficacy of learn-\ning is heavily contingent on the accuracy of the environment\nmodel and struggles to accommodate unpredictable external\ndisturbances during robot operation [3], [4]. It is due to these\nchallenges that Model-free Reinforcement Learning (MFRL)\nhas become the predominant learning method in robotic\nreinforcement learning [5]. Nonetheless, MFRL is inherently\ndata-inefficient, prompting the exploration of simulation en-\nvironments as a means to emulate real-world robotics. In\nCorresponding author: Jiangwei Zhong\n1X. Hu, Q. Sun, B. He, H. Liu, X. Zhang, C. Lu and J.Zhong are with\nthe Smart Devices & Solutions Lab, Lenovo Research Shanghai, Shanghai\n200003, China zhongjw@lenovo.com\nsuch simulations, robots can bypass direct interaction with\nthe physical environment and significantly boost data col-\nlection efficiency through parallelization strategies [6], [7]. Yet, in practical applications, disparities between simulated\nand real environments often arise due to computational\ncomplexity and technical limitations. These discrepancies,\nthough not obvious, frequently lead to suboptimal robot\nperformance upon deployment [8]. To address this challenge, a series of methods have\nbeen proposed to bridge the Sim2Real gap, many of which\nhave demonstrated promising results.", "metadata": {"type": "factual", "source": "2503.01255v1.pdf", "context": "and kd are the proportional and derivative gains of the PD\ncontroller. Although static friction is typically greater than\ndynamic friction in reality, we assume they are equal for\nsimplicity.\nIn conventional domain randomization, parameters such\nas Ij, Bj, kmotor, kp, and kd are randomized. From the\ndynamics equation, it is evident that kd and Bj can cancel\neach other out, so only one of them needs to be randomized.\nAdditionally, Static friction fj(t) is typically not random-\nized in conventiona", "quality_score": 8.5}}
{"id": "test_24", "question": "What is the purpose of using automatically generated sketches for generation tasks in this method?", "golden_answers": ["They aid the analysis of sketch-to-mesh correspondence by the CLIP and LPIPS scores."], "prediction": "The purpose of using automatically generated sketches for generation tasks in this method is to enable the depiction of highly complex shapes, reduce the likelihood of generating semantically or geometrically invalid commands, facilitate the generation of smooth and continuous shape transformations, and provide a concise and intuitive way to control the topological relationships between extruded shapes.", "f1": 0.06557377049180327, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00928v1\nA prevalent approach involves train-\ning generative models by leveraging construction sequences\nas supervision (Wu, Xiao, and Zheng 2021; Xu et al. 2022,\n2023; Wang et al. 2024). These methods typically repre-\nsent sketch-and-extrude sequences as parameterized discrete\nsequences, enabling the application of powerful sequence\nlearning techniques to the task of CAD model generation. Despite the demonstrated success of sequence-based\narXiv:2503.00928v1  [cs.GR]  2 Mar 2025\n\nFigure 2: Sample data from DeepCAD dataset. Previous\nmethods, constrained by sequence length limitations, were\nrestricted to learning a subset of simpler shapes within the\ndataset (gray). Our raster-based sketch representation en-\nables the learning of more complex shapes (green). learning methods for CAD model generation, several limita-\ntions remain: 1) The reliance on sequences introduces a bot-\ntleneck, limiting the complexity of representable sketches\nand the number of commands that can be effectively uti-\nlized. This constraint restricts the generation of intricate\nmodels. 2) Current methods often generate semantically in-\nvalid command sequences for CAD modeling, resulting in\nnon-viable models and hindering the reliability and usability\nof the output. 3) Representing sketches as discrete parameter\nsequences makes generating smooth and continuous shape\ntransformations difficult, leading to unnatural results for la-\ntent space interpolation, as illustrated in Fig.1. This poses a\nchallenge for generating plausible sketches, for which plau-\nsible interpolation is essential (Radford, Metz, and Chintala\n2015; Goodfellow et al. 2014; Higgins et al. 2017). In this paper, we revisit the CAD generation task and\npropose a novel representation that follows the sketch-and-\nextrude paradigm without using command sequences. In-\nstead of representing sketches as curve sequences, we lever-\nage raster images. Extrusion parameters, such as origin, di-\nrection, and Boolean operation type, are implicitly deter-\nmined by the dimensions and type of the extrusion box. This\nnew representation addresses the limitations of sequence-\nbased methods in several key ways. Using raster images\nto represent sketches allows for depicting highly complex\nshapes, overcoming the bottleneck imposed by discrete pa-\nrameter sequences (e.g., fixed maximum length of curves per\nloop), as shown in Fig.2. Additionally, the intuitive nature of\nimage-based sketches reduces the likelihood of generating\nsemantically or geometrically invalid commands, leading to\nmore reliable and usable outputs. Moreover, representing\nsketches as images facilitates the generation of smooth and\ncontinuous shape transformations, unlike the discrete nature\nof previous methods. Finally, by employing extrusion boxes,\nour method provides a concise and intuitive way to control\nthe topological relationships between extruded shapes, fur-\nther enhancing the modeling process. Building upon this novel sketch-extrusion representation,\nwe present RECAD, a novel generative framework that\ngenerates sketch-extrusion pairs using Denoising Diffusion\nProbabilistic Models (DDPM) (Ho, Jain, and Abbeel 2020). Concretely, RECAD comprises a sketch image V AE and\ntwo diffusion networks. A Variational Autoencoder (V AE)\n(Kingma and Welling 2013) is utilized to learn the latent\nfeatures of raster sketches. Two diffusion networks are em-\nployed in a sequential generation process. The first diffu-\nsion network generates the coordinates defining the extru-\nsion boxes. Conditioned on these boxes, the second network\ngenerates latent features representing the raster sketches.", "metadata": {"type": "factual", "source": "2503.01425v3.pdf", "context": "generation task. To further show the robustness of our\nmethod, we use the IKEA dataset [38, 55], which con-\ntains 188 furniture models. We use automatically generated\nsketches (following Sec. 3.4) for generation tasks because\nthey aid the analysis of sketch-to-mesh correspondence by\nthe CLIP and LPIPS scores. In our qualitative experi-\nments, we show that our method generalizes to hand-drawn\nsketches. Additionally, we randomly select50 shapes of air-\nplanes, chairs, and lamps (on which SENS is t", "quality_score": 8.5}}
{"id": "test_25", "question": "Based on Figure 5, what does the presence of five distinct peaks with minimal overlap in the density plot indicate about the expert weights learned with the user-aware router and constraint loss?", "golden_answers": ["It indicates that the users can be clustered into five groups based on their averaged expert weights"], "prediction": "The presence of five distinct peaks with minimal overlap indicates that the experts learn distinct group preferences.", "f1": 0.19354838709677422, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01303v1\nAs shown in Figure 6, the\nmain computational and storage load in personal-\nized LLMs is in the user-level adaptation stage,\nwhich scales with the number of users. Both OPPU\nand PROPER introduce 552M parameters for 100\nusers. Regarding training time, PROPER takes\nslightly longer (180 min per batch) than OPPU\n(146 min) for 100 users. PROPER also introduces\ntwo additional components: the group-level adap-\ntation and the LoRA-aware router. These compo-\nnents are one-time processes that do not scale with\nuser growth, adding minimal computation (146 min\nfor group-level adaptation and 150 min for LoRA-\naware router) and storage overhead (91M and 4M,\nrespectively). Despite these additions, PROPER\nremains efficient overall due to its improvements.\n0.18 0.19 0.20 0.21 0.22\nExpert weight\nDensity\nExperts\nexpert 1\nexpert 2\nexpert 3\nexpert 4\nexpert 5\n0.14 0.16 0.18 0.20 0.22 0.24 0.26\nExpert weight\nDensity\nExperts\nexpert 1\nexpert 2\nexpert 3\nexpert 4\nexpert 5\nCluster\nCluster 0\nCluster 1\nCluster 2\nCluster 3\nCluster 4\nFigure 5: The Visualization of expert weights and user\nembeddings learned in the group-level adaptation. The\nupper left: density plot of expert weights with the user-\naware router and constraint loss; The bottom left: den-\nsity plot of expert weights with regular LoRAMoE;The\nright: Scatter plot of user embeddings after detention\nreduction, colored by the clusters.\n4.5 Visualization\nTo answer RQ5, we visualize the user embeddings\nlearned in Stage 2 and the expert weights for the\ngroup experts. For the expert weights, we aver-\nage the weights for each user and compare the\ndensity plots of expert weights learned with the\nuser-aware router and constraint loss versus those\nlearned with regular LoRAMoE. For the user em-\nbeddings, we average the embeddings across layers\nfor each user and apply t-SNE (Van der Maaten\nand Hinton, 2008) to map them into a 2D space. We then cluster the users into 5 groups based on\ntheir averaged expert weights and color the user\nembeddings according to their cluster. As shown\nin Figure 5, the density plot of expert weights with\nthe user-aware router and constraint loss shows\nfive distinct peaks with minimal overlap, indicating\n7\n\nthat the experts learn distinct group preferences. In contrast, the expert weights from regular Lo-\nRAMoE are highly overlapping, suggesting that\nthe experts are learning redundant information. In\nthe user embedding visualization, we observe that\nthe clustering of user embeddings aligns with the\nexpert weight patterns, demonstrating a clear cor-\nrelation between the behavior of user embeddings\nand expert weight distribution.\n4.6 Case Study\nTo answer RQ6, we conduct a case study on the\nLaMP-7: Personalized Tweet Paraphrasing task for\nuser #21 (user_id: 13002361) in the test set. To\ndemonstrate the effectiveness of progressive learn-\ning, we visualize the training and test samples for\nuser #21, as well as a subset of training samples for\nother users. For text visualization, we use a BERT\nencoder (Reimers and Gurevych, 2019) to generate\ntext embeddings and apply T-SNE for dimensional-\nity reduction.", "metadata": {"type": "reasoning", "source": "2503.01303v1.pdf", "context": "expert 3\nexpert 4\nexpert 5\nCluster\nCluster 0\nCluster 1\nCluster 2\nCluster 3\nCluster 4\nFigure 5: The Visualization of expert weights and user\nembeddings learned in the group-level adaptation. The\nupper left: density plot of expert weights with the user-\naware router and constraint loss; The bottom left: den-\nsity plot of expert weights with regular LoRAMoE;The\nright: Scatter plot of user embeddings after detention\nreduction, colored by the clusters.\n4.5 Visualization\nTo answer RQ5, we visualize th", "quality_score": 8.5}}
{"id": "test_26", "question": "According to the text, what is a limitation of most existing studies on retinal fluid segmentation?", "golden_answers": ["Most existing studies focus on either the retinal layer or fluid segmentation, with limited efforts dedicated to integrating segmentation outcomes with clinical statistical analysis."], "prediction": "A limitation of most existing studies on retinal fluid segmentation is the inconsistency in ground-truth annotations, as small, widely distributed foci are challenging to delineate manually.", "f1": 0.3404255319148936, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01248v1\nIn cases where excessive\nfluid penetrates the layer boundaries, some portions of the\nlayer become invisible or physically diminished. More clinical\nexpertise is needed to segment the extreme instances properly. HRF segmentation suffers from inconsistencies in ground-truth\nannotations, as small, widely distributed foci are challenging\nto delineate manually. Interestingly, in some cases, automated\nmodels provided more precise segmentations than the manual\nground truth, particularly for HRF regions. Figure 9 shows\nthat the model prediction outperforms the ground truth in HRF\nsegmentation. This suggests that deep learning models could\nbe leveraged for segmentation and refining human annotations. Additionally, using more pre-processing and post-processing\ntechniques may help improve the performance, such as the\npixel-wise relative positional map as an extra input and random\nforest classifier as a label refiner(Ma et al., 2021). Second, additional model comparisons may be necessary\nto provide a more comprehensive evaluation of segmentation\napproaches. While SwinUNETR and VM-UNet demonstrated\nsuperior performance, other architectures excel in certain per-\nspectives. For example, MedSAM enables universal medical\nimage segmentation with zero-shot capabilities(Ma et al.,\n2023). The self-supervised few-shot semantic segmentation\ncan be used for a limited number of labels(Ouyang et al.,\n2022). A broader comparison across multiple deep learning\nmodels could offer more insights into the trade-offs between\nperformance, efficiency, and generalizability. Third, this study\u2019s cross-sectional nature limits its ability\nto track disease progression over time. Longitudinal studies\nwould provide better insights into how retinal layer thickness\nevolves in DR. For example, this study observes RNFL\nthickening in PDR, but longitudinal research often reports\nRNFL thinning due to neurodegeneration. This suggests that\nthe transient swelling observed in cross-sectional studies may\neventually lead to structural atrophy, which can only be\nconfirmed through long-term follow-up studies. Fourth, while the sample size is sufficient to detect sig-\nnificant differences, it may limit the generalizability of the\nfindings. A larger dataset encompassing a broader range of DR\nseverities and treatment histories could provide more robust\nconclusions. Expanding the cohort to include more diverse\npatient populations would also improve the applicability of\nthe findings across different clinical settings. Fifth, the lack of a detailed NPDR severity grading sys-\ntem may limit the ability to distinguish early, intermediate,\nand severe NPDR stages. Different NPDR severities likely\nexhibit distinct retinal layer changes, and a more granular\nclassification system could enhance the understanding of DR\nprogression. Future studies should explore integrating severity-\nbased stratification to assess how thickness variations differ\nacross NPDR subtypes. To our best knowledge, most DR\ngrading datasets with public access focus on fundus color\nimages like Messidor 3 and DRTiD4.\nLastly, integrating multi-modal imaging techniques such as\nOCT angiography (OCTA) could provide additional insights\ninto the vascular changes associated with DR. For example,\nAlam et al. discovered the difference of vascular complex-\nity features between NPDR and PDR patients(Alam et al.,\n2021). Multiple OCT parameters are significantly correlated\nwith DR severity(Laotaweerungsawat et al., 2020). Combining\nstructural OCT findings with functional vascular imaging may\nimprove disease characterization and facilitate more targeted\ntherapeutic interventions. V. C ONCLUSION", "metadata": {"type": "reasoning", "source": "2503.01248v1.pdf", "context": "in diabetic macular edema (DME) and AMD, demonstrating\nsuperior performance over traditional CNN-based models (Xue\nand Du, 2024). Kulyabin et al. leveraged the Segment Any-\nthing Model (SAM) for retinal fluid segmentation, incorporat-\ning point and bounding box prompts to outperform U-Net in\nmacular hole and fluid segmentation tasks (Kulyabin et al.,\n2024). Despite these advancements, most existing studies\nfocus on either the retinal layer or fluid segmentation, with\nlimited efforts dedicated to", "quality_score": 8.5}}
{"id": "test_27", "question": "Why was a large number of examples (1,000) selected for testing in each experiment?", "golden_answers": ["To ensure the representativeness and credibility of the experimental results by covering various input data types."], "prediction": "To highlight the performance extremes across datasets, demonstrating both strong model performance and its limitations.", "f1": 0.14285714285714285, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00597v1\nThese examples\nwere chosen specifically to highlight the performance extremes across datasets: one demonstrating strong model\nperformance and the other showcasing its limitations.", "metadata": {"type": "reasoning", "source": "2503.01319v1.pdf", "context": "4 Experiment setup\nTo verify the testing effectiveness of ABFS on LLM-based NLP software, we conducted\na series of experiments on three text classification datasets and five threat models.\nAll experiments were performed on an Ubuntu 22.04.1 LTS system equipped with\ntwo 32-core Intel(R) Xeon(R) Platinum 8358 CPUs at 2.60GHz, four NVIDIA A100\nTensor Core GPUs, and 1TB of physical memory. Each experiment was repeated three\ntimes, and the results for each metric were averaged. Similar to previous st", "quality_score": 8.5}}
{"id": "test_28", "question": "What can be inferred about the focus of the studies cited in the text, based on their arXiv classifications?", "golden_answers": ["The studies focus on particle physics (hep-ph) and lattice field theory (hep-lat)."], "prediction": "The studies cited in the text primarily focus on healthcare and biomedical literature classification, utilizing various datasets and methodologies to address topics such as COVID-19, cancer research, clinical trials, and ethical considerations in handling sensitive health information.", "f1": 0.17391304347826086, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01159v1\nCOVID-19 research has also been present, with Guo et al. (2024) analyzing\ntreatment-related papers and Yang et al. (2024) developing a binary classification for SARS-CoV-2 and Nipah virus\nliterature for drug discovery purposes. In the scope of clinical and medical topics, Wang et al. (2023) tackled the classification of clinical trial screening\ncriteria, developing 44 semantic categories through the CHIP-CTC dataset, encompassing descriptive sentences. Sarkar\n14\n\net al. (2023) addressed the categorization of medical blog articles across 18 predefined topics, including headache,\nmental health, and heart health. Cancer research classification has been significant, with multiple studies, including\nChen et al. (2022) and Chen et al. (2023), utilizing the Hallmarks of Cancer (HoC) dataset to classify cancer biology\ncharacteristics. Other research studies focused on different research classifications. Chen et al. (2024b) developed a three-tier advice\nclassification system (no advice, weak advice, strong advice) for medical research abstracts. Kementchedjhieva and\nChalkidis (2023) worked with the BIOASQ dataset (consisting of biomedical articles from PubMed), implementing\nclassification based on the Medical Subject Headings (MeSH) taxonomy. Qi et al. (2023) addressed industrial\nbiomedical literature mining tasks, focusing on recognizing special biomedical phrases. The data was acquired with\ninherent label noise due to crowd-sourcing and labeling preferences. For the testing part, relabeling was conducted on a\nsubset of data and assumed being clean. General medical literature classification has been explored through various datasets. Yang et al. (2022) leveraged\nPubMed abstracts and Wikipedia articles for pre-training, while Schneider et al. (2021) utilized both PubMed and\nSciELO databases for fine-tuning. Gretz et al. (2023) contributed to this field by working with the Medical Abstracts\ndataset. Bet, ianu et al. (2024) and Luo et al. (2024) further expanded the research using PubMed datasets, with Luo\net al. specifically incorporating multiple datasets, including BC7LitCovid, for comprehensive biomedical literature\nclassification. 4.1.4. Ethical Considerations\nThese research studies employed various data sources, from social media platforms to EHR and literature databases,\nand used different categorization approaches ranging from binary to multi-class and multi-label classifications. The\nresearch spans multiple languages and formats, reflecting healthcare text classification research\u2019s global and diverse\nnature. However, leveraging LLMs to carry out this type of task in healthcare presents significant ethical concerns\naround patient privacy and data security without forgetting about the algorithmic bias that can emerge. The protection\nof sensitive health information is crucial under regulations that vary based on the country, such as the Health Insurance\nPortability and Accountability Act (HIPAA) in the U.S. This is particularly important when dealing with an LLM\nrequiring API requests (e.g., GPT-4o) where the classification cannot be run locally, and when the text data consists\nof clinical notes where patients\u2019 Protected Health Information (PHI) should be detected and de-identified in advance. To overcome the challenge resulting from the sensitive nature of healthcare textual data, researchers in the reviewed\nliterature resorted to different ethical considerations. Some research papers conducted all their experiments locally; this\nis especially convenient when fine-tuning BERT (or variant).", "metadata": {"type": "reasoning", "source": "2503.01322v2.pdf", "context": "12\narXiv:0807.2674 [hep-ph].\n[22] Y .-R. Liu, X. Liu, W.-Z. Deng, and S.-L. Zhu, Eur. Phys. J. C\n56, 63 (2008), arXiv:0801.3540 [hep-ph].\n[23] D. Gamermann and E. Oset, Phys. Rev. D 80, 014003 (2009),\narXiv:0905.0402 [hep-ph].\n[24] C. Bignamini, B. Grinstein, F. Piccinini, A. D. Polosa,\nand C. Sabelli, Phys. Rev. Lett. 103, 162001 (2009),\narXiv:0906.0882 [hep-ph].\n[25] J. Nieves and M. P. Valderrama, Phys. Rev. D 86, 056004\n(2012), arXiv:1204.2790 [hep-ph].\n[26] F.-K. Guo, C. Hanhart, U.-G. Mei\u00df", "quality_score": 8.5}}
{"id": "test_29", "question": "What is the primary focus of Section IV-C in the evaluation?", "golden_answers": ["The performances of adding a new task during the operational life of HAR."], "prediction": "The primary focus of Section IV-C in the evaluation is the analysis of the Writing Plan's functionality through graph-theoretical metrics, specifically examining the construction of a directed acyclic graph (DAG) of sections and the contextual dependencies between them.", "f1": 0.0975609756097561, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00751v1\nThis observation underscores the chal-\nlenge of evaluating long-form content quality using\na single metric. In contrast, RAPID demonstrates\nsuperior performance across multiple evaluation\ncriteria, reinforcing its robustness and effective-\nness. The quality evaluation results of outlines are pre-\nsented in Table 2. RAPID significantly outper-\nforms other methods across different foundation\nmodels, particularly in recall and F1-score met-\nrics. This demonstrates its superior ability to main-\ntain thematic focus while achieving comprehensive\ncoverage of research topics. Notably, although\nSTORM achieves relatively higher recall compared\nto other baselines, RAPID can efficiently generate\nhigher-quality outlines through retrieval of similar\ntopics, without requiring complex multi-turn agent\ndiscussion mechanisms. Backbone Avg. Nodes\nAvg. Edges\nDependency\nDensity\nLongest\nPath\nQwen-Max 8.72 16.64 2.09 2.79\nDeepSeek-v3 7.21 12.18 1.91 2.62\nGPT-4o 8.43 11.86 1.55 3.59\nTable 3: Results of the graph metrics of Writing Plan. 5.2 Ablation Studies\nTo evaluate the impact of the writing plan, we con-\nducted an ablation study on GPT-4o by removing\nthe Plan-Guided Article Generation module and\ninstead employing a parallel section-wise genera-\ntion approach (denoted as \"w/o plan\"). As shown\nin Table 1, the absence of a writing plan led to\nperformance degradation across multiple metrics,\nparticularly in organization and factuality. Without\ncontextual paragraph awareness, the model exhib-\nited stylistic inconsistencies and coherence issues\nbetween sections, explaining the decline in orga-\nnization scores. Additionally, generating sections\nin isolation resulted in redundant content, as each\nparagraph attempted to cover multiple aspects of\nthe topic independently. While techniques such as\nSTORM can partially mitigate these issues through\npost hoc refinement, maintaining coherence and\nquality in long-form text remains challenging due\nto inherent output length limitations. In contrast,\nour structured planning approach explicitly defines\ninter-sectional dependencies, enhancing overall co-\nherence and enabling the generation of longer,\nmore cohesive articles. These findings underscore\nthe critical role of planning in long-form text gen-\neration. 5.3 Writing Plan Analysis\nOur method relies on constructing a directed\nacyclic graph (DAG) of sections prior to writing,\nwhich defines the contextual dependencies between\nsections. In this section, we analyze the Writing\nPlan\u2019s functionality through graph-theoretical met-\nrics. By representing sections as nodes and inter-\nsectional dependencies as directed edges, we for-\nmally define the Writing Plan as a DAG structure\nG = (V, E), where V = {v1, v2, . . . , vn} denotes\nthe set of n nodes (sections), and E \u2286 V \u00d7 V\nrepresents the directed edges indicating dependen-\ncies. To quantify the \"dependency density\" of the\nWriting Plan, we define it as the ratio of the num-\nber of edges to the maximum possible edges in a\n7\n\nminimally connected DAG:\nD(G) = |E|\nn \u2212 1, (7)\nwhere |E| is the cardinality of the edge set E, and\nn\u22121 corresponds to the maximum number of edges\nin a linear chain structure. For example, in a slid-\ning window of length k = 1, where each section\ndepends only on its immediate predecessor, the\nnumber of edges is |E| = n \u2212 1, yielding a depen-\ndency density of D(G) = 1.", "metadata": {"type": "factual", "source": "2503.01353v1.pdf", "context": "This section is organized as follows. Section IV-A details\nthe datasets used for the evaluation. Section IV-B evaluates\nthe classification capability. Section IV-C evaluates the per-\nformances of adding a new task during the operational life\nof HAR. Finally, Section IV-D discusses the memory usage,\ncomputation requirements, and mean latency.\nA. Datasets\n1) UCA-EHAR dataset: The UCA-EHAR dataset [12] in-\ncludes gyroscopic and accelerometer data collected from smart", "quality_score": 8.5}}
{"id": "test_30", "question": "What is a limitation of the LG-VQ method?", "golden_answers": ["It has limitations regarding insufficient alignment between codebook and text, primarily due to the brevity of the existing image captions."], "prediction": "The documents provided do not mention the LG-VQ method or its limitations.", "f1": 0.07142857142857142, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00094v1\nFor this reason, uncertainty quantification has become a golden\nstandard in many industrial applications. It can be achieved in\nseveral ways. A first approach consists in combining standard ML algo-\nrithms with model-free UQ techniques. Standard ML algo-\nrithms already used in the literature to build surrogate models\ninclude gradient boosting methods, polynomial chaos expan-\nsion, random forest, response surface methods, and polynomial\nsplines. It can be plugged with model-free UQ techniques\nsuch as Monte Carlo, conformal predictions, and various en-\nsemble methods (Bootstrap prediction intervals, infinitesimal\nJackknife, and Out-Of-Bag errors). This first approach offers\ngreat flexibility in the surrogate model selection, and UQ\nmethods rely on mild assumptions that are often respected. The\nmain drawback of these model-free UQ techniques is that they\ntend to produce overly large confidence intervals with small\ndatasets. They also have a high computational cost, making\nthem poorly suited to building fast-to-evaluate uncertainty-\naware surrogate models. A second possible approach consists in adapting existing\nprediction models to enable them to quantify uncertainty. For\nexample, quantile regression (QR) methods modify the loss\nfunction so that the machine learning model predicts a specific\nquantile of the expected output. Predicting the median, the 5%,\nand the 95% quantiles allows obtaining a prediction and the\nassociated 90% confidence interval. Other works also propose\nto combine the predictive power of artificial neural networks\n(ANN) with deep-learning-specific UQ techniques, such as the\ndelta method, the mean-variance estimation, the lower upper\nbound estimation, and quality-driven ensemble methods. There\nare two drawbacks to this second approach. First, for both\nQR and ANN, a lot of data is required to train the models\neffectively. Second, the obtained surrogate model is black-box,\nwhich is undesirable for safety-critical applications such as\nensuring power grid security. A third increasingly adopted solution to build accuracy-\naware surrogate models is Gaussian processes. Instead of\na simple prediction, GPs provide a full posterior Gaussian\ndistribution N\n\u0000\n\u00b5\u2217, \u03c32\n\u2217\n\u0001\nof the outcome, which is much more\ninformative. The posterior mean \u00b5\u2217 constitutes the prediction,\nand the posterior standard deviation \u03c3\u2217 is used for UQ. Many\nreasons explain the growing popularity of GPs. \u2022 GPs are flexible non-parametric models . Many ma-\nchine learning models are parametric. They learn an\napproximation \u02dcf that belongs to a parametric set of\nfunctions { \u02dcf\u03b8, \u03b8 \u2208 \u0398} (polynomial regressors, neural\nnetworks...). The hyperparameters \u03b8 are optimized with\nthe train data that no longer contribute to the model\nafterward. All the exploited information is compressed\ninto \u03b8. Unlike these models, the GP does not assume a\nfixed number of parameters to approximate the function. Instead, it automatically adapts the model\u2019s complexity\nto the dataset size, which is very flexible. \u2022 GPs provide both prediction and UQ . Instead of\nlooking for one prediction that best fits the data, the\nGP assumes that many approximations could plausibly\nmodel the data. The GP then assigns to each approx-\nimation a probability of actually representing the true\nfunction. The most probable approximation constitutes\nthe GP prediction; the other likely ones contribute to\nUQ.", "metadata": {"type": "factual", "source": "2503.01261v1.pdf", "context": "various cross-modal downstream tasks.\nDespite the success of LG-VQ, it has limitations regard-\ning insufficient alignment between codebook and text, pri-\nmarily due to the brevity of the existing image captions. As\nillustrated in Fig. 1, we can see that the original caption\nis concise, focusing solely on the main object while omit-\nting details about the background and other key elements\nof the image. This brevity results in a lack of sufficient\nsemantic information, hindering the learning of a ", "quality_score": 8.5}}
{"id": "test_31", "question": "What set does the combination of the proof of the third point with Proposition 2.7 show is almost surely contained in?", "golden_answers": ["{t \u2265 0 s.t. Wk(t) \u0338= Wk(t\u2212)} \u2282 { t \u2265 0 s.t. Ct \u0338= Ct\u2212} = {t \u2265 0 s.t. Dt \u0338= Dt\u2212}"], "prediction": "The combination of the proof of the third point with Proposition 2.7 shows that the set is almost surely contained in the set of all possible outcomes for the random variable.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00220v1\nIt admits a\nquite elementary proof relying only on Hoeffding\u2019s concentration inequality, making it a natural\npoint of departure for developing more sophisticated coverage guarantees. We thus provide this\nelementary proof, then demonstrate the result using uniform convergence techniques. These uniform\nconvergence guarantees\u2014which form the basis for providing guarantees for approximate weighted\ncoverage (Definition 1.1) also provide a two-sided bound on sample-conditional coverage:\nCorollary 2.1. Assume the scores Si = s(Xi, Yi) are distinct with probability 1. Then for any\n\u03b3 >0, with probability at least 1 \u2212 2e\u22122n\u03b32\nover the sample Pn,\n1 \u2212 \u03b1 \u2212 \u03b3 \u2264 P(Yn+1 \u2208 bCn(Xn+1) | Pn) \u2264 1 \u2212 \u03b1 + 1\nn + \u03b3. The simplicity of the guarantee (7) means it admits elegant extensions as well. For example,\nwe can extend the argument to give a bound that more carefully tracks the desired confidence \u03b1:\nProposition 2. Let \u03b4 \u2208 (0, 1) and define\n\u03b3n(\u03b4) := 4 log 1\n\u03b4\n3n +\nr\u0010 4\n3n log 1\n\u03b4\n\u00112\n+ 2\u03b1(1 \u2212 \u03b1)\nn log 1\n\u03b4 \u2264 8 log 1\n\u03b4\n3n +\nr\n2\u03b1(1 \u2212 \u03b1)\nn log 1\n\u03b4 . Then with probability at least 1 \u2212 \u03b4 over the draw of the sample Pn,\n1 \u2212 \u03b1 \u2212 \u03b3n(\u03b4) \u2264 P(Yn+1 \u2208 bCn(Xn+1) | Pn). If additionally the scores S have a density, then with probability at least 1 \u2212 2\u03b4,\n1 \u2212 \u03b1 \u2212 \u03b3n(\u03b4) \u2264 P(Yn+1 \u2208 bCn(Xn+1) | Pn) \u2264 1 \u2212 \u03b1 + \u03b3n(\u03b4). Roughly, we see that the simple quantile-based confidence set achieves coverage\n1 \u2212 \u03b1 \u00b1 OP (1)\nr\n\u03b1(1 \u2212 \u03b1)\nn . When \u03b1 is small\u2014which is the typical case\u2014this is always sharper than the naive guarantee (7). The central limit theorem shows this is as accurately as we could hope to even estimate the coverage\nlevel of a predictor; moreover, as we discuss following Theorem 3, it is minimax (rate) optimal. In\nthe remainder of the section, we provide two proofs of Proposition 1, along with Corollary 2.1. In\nSection 2.3, we prove Proposition 2 using Bernstein-type concentration guarantees. 2.1 An elementary proof of Proposition 1\nFor the scalar random variable S, define the \u03b2-quantile\nq\u22c6(\u03b2) := inf {t \u2208 R | P(S \u2264 t) \u2265 \u03b2}. (8)\nBecause the CDF is right continuous, we have P(S \u2264 q\u22c6(\u03b2)) \u2265 \u03b2, and P(S > q\u22c6(\u03b2)) = 1 \u2212 P(S \u2264\nq\u22c6(\u03b2)) \u2264 1 \u2212 \u03b2. For \u03b3 >0 and any \u03c4 \u2208 R, the inequality\nP(Sn+1 > \u03c4) > \u03b1+ \u03b3, i.e. P(Sn+1 \u2264 \u03c4) < 1 \u2212 \u03b1 \u2212 \u03b3,\n5\n\nimplies that \u03c4 < q\u22c6(1 \u2212 \u03b1 \u2212 \u03b3). Consider the event that b\u03c4n < q\u22c6(1 \u2212 \u03b1 \u2212 \u03b3). For this to occur, it must be the case that\n1\nn\nnX\ni=1\n1{Si < q\u22c6(1 \u2212 \u03b1 \u2212 \u03b3)} \u22651 \u2212 \u03b1. (9)\nBut this event is unlikely: define the Bernoulli indicator variables Bi = 1 {Si < q\u22c6(1 \u2212 \u03b1 \u2212 \u03b3)}.", "metadata": {"type": "factual", "source": "2503.01320v1.pdf", "context": "This yields the third point.\nFourth point. The combination of the proof of the third point with Proposit ion 2.7 shows that,\nalmost surely , we have {t \u2265 0 s.t. Wk(t) \u0338= Wk(t\u2212)} \u2282 { t \u2265 0 s.t. Ct \u0338= Ct\u2212} = {t \u2265 0 s.t. Dt \u0338=\nDt\u2212} \u2282 JN , where the last inclusion is a consequence of ( 2.34). This yields the fourth point.", "quality_score": 8.5}}
{"id": "test_32", "question": "What is a key benefit of using paired image-text data with subtle variations for training Vision-Language Models?", "golden_answers": ["Producing Vision-Language Models with proper compositional understanding"], "prediction": "A key benefit is that it holds the promise of producing Vision-Language Models with proper compositional understanding.", "f1": 0.6363636363636364, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01167v2\nEnhancing Vision-Language Compositional Understanding with Multimodal\nSynthetic Data\nHaoxin Li and Boyang Li\nNanyang Technological University\n{haoxin003, boyang.li}@ntu.edu.sg\nAbstract\nPaired image-text data with subtle variations in-between\n(e.g., people holding surfboards vs. people holding shov-\nels) hold the promise of producing Vision-Language Mod-\nels with proper compositional understanding. Synthesiz-\ning such training data from generative models is a highly\ncoveted prize due to the reduced cost of data collection. However, synthesizing training images for compositional\nlearning presents three challenges: (1) efficiency in gen-\nerating large quantities of images, (2) text alignment be-\ntween the generated image and the caption in the exact\nplace of the subtle change, and (3) image fidelity in ensur-\ning sufficient similarity with the original real images in all\nother places. We propose SPARCL (Synthetic Perturbations\nfor Advancing Robust Compositional Learning), which inte-\ngrates image feature injection into a fast text-to-image gen-\nerative model, followed by an image style transfer step, to\nmeet the three challenges. Further, to cope with any resid-\nual issues of text alignment, we propose an adaptive mar-\ngin loss to filter out potentially incorrect synthetic samples\nand focus the learning on informative hard samples. Eval-\nuation on four compositional understanding benchmarks\ndemonstrates that SPARCL significantly improves the com-\npositionality of CLIP , boosting the average accuracy of\nthe CLIP base model by over 8% across all benchmarks\nand outperforming state-of-the-art methods by 2% on three\nbenchmarks. 1. Introduction\nCurrent Vision-Language Models (VLMs) still face limita-\ntion in accurately interpreting compositional relationships\nbetween objects and attributes, as demonstrated by numer-\nous evaluations [32, 62, 88, 106, 113]. This limitation pri-\nmarily stems from the absence of subtle variations in the\ntraining data [42] ( e.g., the subtle variations between the\ntwo captions in Figure 1 (a)). As a result, it becomes pos-\nsible to maximize empirical image-caption alignment us-\nSource caption: Two people carrying surfboards on a beach. Target caption: Two people carrying shovels on a beach. Source image Edited image Generated image\n(a) Difficulty in accurately creating precise variations.\n+ Real positive caption: \nA double decker bus \nparked at a station. + Synthetic positive \ncaption: A double-decker \nbus parked at a bus \nstation with a few \npassengers getting on.\n+ Real positive caption: \nA man is stirring a silver \npot filled with food.\n- Synthetic negative \ncaption: A man is stirring \na copper pot filled with \nfood.\n+ Real positive caption: \nRandom people sitting in \na public transport bus.\n- Synthetic negative \ncaption: Random \nanimals riding a purple \nelephant. Wrong GenerationHard Sample Easy Sample\n(b) Inconsistency in cross-modal alignment quality of synthetic samples. Figure 1. Challenges in generating and training on synthetic data:\n(a) When generating an image with subtle variations based on a\nreal image and a target caption specifying the variations, an image\nediting model [6] struggles with text alignment (middle), while\nan image generation model [75] fails to maintain image fidelity\n(right). (b) Synthetic positive and negative image-caption pairs\nshow different levels of alignment quality.", "metadata": {"type": "factual", "source": "2503.01167v2.pdf", "context": "Enhancing Vision-Language Compositional Understanding with Multimodal\nSynthetic Data\nHaoxin Li and Boyang Li\nNanyang Technological University\n{haoxin003, boyang.li}@ntu.edu.sg\nAbstract\nPaired image-text data with subtle variations in-between\n(e.g., people holding surfboards vs. people holding shov-\nels) hold the promise of producing Vision-Language Mod-\nels with proper compositional understanding. Synthesiz-\ning such training data from generative models is a highly\ncoveted prize due to the reduc", "quality_score": 8.5}}
{"id": "test_33", "question": "What happens to the spectral function as the strength of disorder increases?", "golden_answers": ["It broadens, but still clearly represents the quasiparticle bulk dispersion."], "prediction": "The spectral function decreases with increasing disorder strength until it vanishes at a critical value, after which it reopens again at higher disorder strengths.", "f1": 0.06451612903225805, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01367v2\nThe band-gap region on ( \u03b5, W) plane is represented by violet (in\npanel (b)) and olive (in panel (c)) area. As clear, Eqs. (21)\u2013(23) form a system of integral equa-\ntions that determines both the Green\u2019s function \u02c6G(k, \u03b5)\nand the self-energy \u02c6\u03a3(k, \u03b5) matrices. The self-consistent\nsolution of such integral systems in the general case is\na laborious task. However, for the case of the disorder\nformed by the short-range impurities, for which \u02dcv(q) = u0\n(cf. Ref. 20), the solution of the problem can be signifi-\ncantly simplified. Indeed, in the case of the short-range disorder,\nVn(k, k\u2032)2 = u2\n0\u03b4n,0 in Eq. (22). The latter results in a\ndiagonal form of the self-energy matrix \u02c6\u03a3(\u03b5) in Eq. (21)\nbeing independent of the momentum. By means of direct\ncalculations, one can verify that G11 (k, \u03b5) = G33 (k, \u03b5)\nand G22 (k, \u03b5) = G44 (k, \u03b5), which leads to\n\u02c6\u03a3\u2191(\u03b5) = \u02c6\u03a3\u2193(\u03b5) = W2\n4\u03c0\nK2\ncZ\n0\ndx\n \nG11 (\u221ax, \u03b5) 0\n0 G22 (\u221ax, \u03b5)\n!\n,\n(24)\nwhere W2 = niu2\n0 is the disorder strength. Note that\nintegration in Eq. (24) can be performed analytically,\nwhich transforms Eqs. (21)\u2013(24) into the set of algebraic\nequations numerically solved by simple iterations. Once the self-energy is known, the spectral function\nA(k, \u03b5) and density-of-states D(\u03b5) for the case of the\nshort-range disorder can be found as\nA(k, \u03b5) = \u22122\n\u03c0 Im {G11 (k, \u03b5) + G22 (k, \u03b5)},\nD(\u03b5) =\nKcZ\n0\nkdk\n2\u03c0 A(k, \u03b5). (25)\nIII. RESULTS AND DISCUSSION\nFigure 1 represents the evolution of density-of-states\nD(\u03b5) and spectral function A(k = 0, \u03b5) at the \u0393 point as a\nfunction of the short-range disorder strengthW = \u221aniu0\ncalculated with the structure parameters of the proto-\ntype 6 nm wide HgTe/Cd0.7Hg0.3Te QW (see Tab. I). All\ndetails about the calculation of structural parameters in-\nvolved in H0(k) can be found elsewhere73. As seen, being\nadded to an initially clean trivial QW (M >0), the short-\nrange disorder causes the band-gap to decrease with in-\ncreasing W until it vanishes at a critical value Wc and\nthen reopens again at W > Wc. Such band-gap behavior\nrepresents the disorder-induced topological phase tran-\nsition previously discovered in the tight-binding calcula-\ntions16,17. Importantly, the band-gap reopening region\nat W > Wc was characterized by quantized conductance\nvalues inherent in the QSHI state. It is on the basis of\n\n5\nthis fact that the conclusion is made that W = Wc cor-\nresponds to a topological phase transition. Let us now show that the band-gap behavior illustrated\nby the evolution of D(\u03b5) and A(k = 0, \u03b5) is indeed due\nto a change in the Z2 invariant. For this purpose, it is\nconvenient to represent the self-energy blocks in Eq. (24)\nin the form\n\u02c6\u03a3\u2191(\u03b5) = \u02c6\u03a3\u2193(\u03b5) = \u03a30(\u03b5) + \u03a3z(\u03b5)\u03c3z.", "metadata": {"type": "factual", "source": "2503.01367v2.pdf", "context": "HgTe/Cd0.7Hg0.3Te QW at several values of the short-\nrange disorder strength W. It is seen that although\nthe spectral function broadens with increasing disorder\nstrength, it still clearly represent the quasiparticle bulk\ndispersion. Interestingly, the bulk quasiparticles at the\ngapless case at W = Wc mimics massless Dirac fermions\nas it is in the \u201cclean\u201d limit 55. If the strength of disor-\nder exceeds a critical value Wc, which leads to Z2(\u03b5) = 1,\nthe bulk states coexist with a pair of quasipart", "quality_score": 8.5}}
{"id": "test_34", "question": "Why do the quasi-particle helical edge states not decay in the band-gap region?", "golden_answers": ["Because the damping factor \u0393(\u03b5) turns to zero, resulting from Im\u03a3z(\u03b5) and Im\u03a30(\u03b5) both vanishing in this region."], "prediction": "The quasi-particle helical edge states do not decay in the band-gap region because the damping factor \u0393(\u03b5) turns to zero, as both Im\u03a3 z(\u03b5) and Im\u03a3 0(\u03b5) vanish in that region, resulting in the spectral function of the edge states taking the form of a delta function.", "f1": 0.42105263157894735, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01367v2\n(41)\nFigure 3 shows the evolution of total spectral func-\ntion determined as the sum A(k, \u03b5) in Eq. (25) and\nAedge(k, \u03b5) in Eq. (40) in the prototype 6 nm wide\nHgTe/Cd0.7Hg0.3Te QW at several values of the short-\nrange disorder strength W. It is seen that although\nthe spectral function broadens with increasing disorder\nstrength, it still clearly represent the quasiparticle bulk\ndispersion. Interestingly, the bulk quasiparticles at the\ngapless case at W = Wc mimics massless Dirac fermions\nas it is in the \u201cclean\u201d limit 55. If the strength of disor-\nder exceeds a critical value Wc, which leads to Z2(\u03b5) = 1,\nthe bulk states coexist with a pair of quasiparticle helical\nedge states \u2013 see Fig. 3(c). Importantly, since Im\u03a3 z(\u03b5) and Im\u03a3 0(\u03b5) both vanish\nin the band-gap region (see Fig. 2(b,c)), the damping fac-\ntor \u0393(\u03b5) turns to zero. As a result, the spectral function\nof the edge states in the band-gap takes the form of a\ndelta function\nAedge(kx, \u03b5) =\nX\n\u03c3\n\u03b4\n\b\n\u03b5 \u2212 ReEedge\n\u03c3 (kx, \u03b5)\n\t\n. (42) The latter means that the edge quasiparticles do not de-\ncay. The absence of energy dissipation for the quasi-\nparticle helical edge states in the band-gap explains the\nquantized values of the longitudinal conductance in the\npresence of an external bias, found previously in numeri-\ncal calculations in strip geometry16,17. The latter can be\nshown explicitly by means of Landauer-B\u00a8 uttiker formal-\nism, taking into account that the transmission coefficients\n\n8\nalong the edge channel do not change 75,76. Finally, we note the importance of taking into account\nthe dependence of the self-energy matrix on \u03b5 for the de-\nscription of topological phase transitions. In the case any\ninteractions, and exemplary provided by short-range elec-\ntrostatic disorder above, our quasiparticle concept goes\nbeyond the previous \u201ctopological Hamiltonian\u201d Ht(k)\napproach as used by Z. Wang and collaborators 32\u201334. IV. SUMMARY AND NOTES We have presented a general recipe to describe topolog-\nical phase transitions in condensed matter systems with\ninteractions. By using the simplest BHZ model with the\nshort-range disorder, we have directly demonstrated that\nthat topological invariants in the presence of interactions\ncan be efficiently calculated by means of a non-Hermitian\nquasiparticle Hamiltonian introduced on the basis of the\nGreen\u2019s function. The quasiparticle approach allows us\nto explicitly demonstrate that the quantized values of the\nlongitudinal conductance found previously in numerical\ncalculations16,17 are due to the vanishing of the damping\nfactor of quasiparticle edge states in a certain range of\nthe Fermi energy and the disorder strength. Note that despite the relatively simple case of a 2D\nsystem with the short-range disorder considered in this\npaper, the quasiparticle approach should be applicable\nin general for topological characterization of arbitrary\nsystems (including the ones described beyond the BHZ\nmodel20) with an arbitrary type of interaction.", "metadata": {"type": "reasoning", "source": "2503.01367v2.pdf", "context": "HgTe/Cd0.7Hg0.3Te QW at several values of the short-\nrange disorder strength W. It is seen that although\nthe spectral function broadens with increasing disorder\nstrength, it still clearly represent the quasiparticle bulk\ndispersion. Interestingly, the bulk quasiparticles at the\ngapless case at W = Wc mimics massless Dirac fermions\nas it is in the \u201cclean\u201d limit 55. If the strength of disor-\nder exceeds a critical value Wc, which leads to Z2(\u03b5) = 1,\nthe bulk states coexist with a pair of quasipart", "quality_score": 8.5}}
{"id": "test_35", "question": "What is the explicit expression of I2m\u22121 in terms of the sum of a series?", "golden_answers": ["I2m\u22121 = \u2212\u03c0\u2211 m i=0 a2m, 2i(I(1)2m\u22121,i + I(2)2m\u22121,i)"], "prediction": "I2m\u22121 = -\u03c0 \u2211_{i=0}^{m} a2m, 2i (I(1)_{2m\u22121,i} + I(2)_{2m\u22121,i}).", "f1": 0.42857142857142855, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01246v1\nWe proceed to analyze the integral I2m\u22121 by the explicit expression of \u2126 and P2m.\nIt follows that\nI2m\u22121 =\nm\u2211\ni=0\na2m, 2i\n\u222b\n\u2126\n1\n|y|\n(y1 \u2212 1\nj )2i(y2\n2 + y2\n3)m\u2212i\n|y\u2212 zj|4m+1 y2m\u22121\n1 dy\n= 2 \u03c0\nm\u2211\ni=0\na2m, 2i\n\u222b 0\n\u22121\n\u222b 1\n0\n1\u221a\ny2\n1 + r2\n(y1 \u2212 1\nj )2ir2(m\u2212i)\n\u221a\n(y1 \u2212 1\nj )2 + r2\n4m+1 y2m\u22121\n1 rdrdy1\n= \u2212\u03c0\nm\u2211\ni=0\na2m, 2i\n\u222b 1\n0\n\u222b 1\n0\n1\u221a\ny2\n1 + r\n(y1 + 1\nj )2irm\u2212i\n\u221a\n(y1 + 1\nj )2 + r\n4m+1 y2m\u22121\n1 drdy1. We spilt I2m\u22121 into two parts. For 0 \u2264 i\u2264 m, de\ufb01ne\nI(1)\n2m\u22121,i :=\n\u222b 1\n0\n\u222b 1\n0\n(y1 + 1\nj )2irm\u2212i\n((y1 + 1\nj )2 + r)2m+1 y2m\u22121\n1 drdy1,\nI(2)\n2m\u22121,i :=\n\u222b 1\n0\n\u222b 1\n0\n\uf8eb\n\uf8ed 1\n\u221a\ny2\n1 + r\n\u2212 1\u221a\n(y1 + 1\nj )2 + r\n\uf8f6\n\uf8f8 (y1 + 1\nj )2irm\u2212i\n\u221a\n(y1 + 1\nj )2 + r\n4m+1 y2m\u22121\n1 drdy1. Clearly, I2m\u22121 = \u2212\u03c0\u2211 m\ni=0 a2m, 2i(I(1)\n2m\u22121,i + I(2)\n2m\u22121,i ). From the basic knowledge of\ncalculus, it is seen that\nI(1)\n2m\u22121,i =\nm\u2212i\u2211\nl=0\n\u222b 1\n0\n(y1 + 1\nj)2(m\u2212l)y2m\u22121\n1\n(\u222b (y1+ 1\nj )2+1\n(y1+ 1\nj )2\n(\u22121)m\u2212i\u2212lCl\nm\u2212i\n1\ns2m+1\u2212l ds\n)\ndy1\n=\n(m\u2212i\u2211\nl=0\n(\u22121)m\u2212i\u2212lCl\nm\u2212i\n2m\u2212 l\n) \u222b 1\n0\ny2m\u22121\n1\n(y1 + 1\nj )2m dy1 + O(1)\n=\n(m\u2212i\u2211\nl=0\n(\u22121)lCl\nm\u2212i\nm+ i+ l\n)[ \u222b 1\nj +1\n1\nj\n2m\u22121\u2211\nl=0\nCl\n2m\u22121\n(\n\u2212 1\nj\n) 2m\u22121\u2212l 1\ns2m\u2212l ds\n]\n+ O(1)\n= 1\n2mCm\u2212i\n2m\u22121\nln j+ O(1),\nwhere we note that the identity\nm\u2212i\u2211\nl=0\n(\u22121)lCl\nm\u2212i\nm+ i+ l = 1\n2mCm\u2212i\n2m\u22121\n\n10 C. Wu and J. Yang\nholds. Further, direct calculation shows that\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n1\n\u221a\ny2\n1 + r\n\u2212 1\u221a\n(y1 + 1\nj )2 + r\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n=\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n1\nj (2y1 + 1\nj )\n\u221a\ny2\n1 + r\n\u221a\n(y1 + 1\nj )2 + r(\n\u221a\ny2\n1 + r+\n\u221a\n(y1 + 1\nj )2 + r)\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u2264 1\nj\n1\u221a\ny2\n1 + r\n1\u221a\n(y1 + 1\nj )2 + r\nfor ( y1,r ) \u2208 (0, 1)2, which implies\n|I(2) 2m\u22121,i | \u2264 1\nj\n\u222b 1\n0\n\u222b 1\n0\n1\u221a\ny2\n1 + r\n(y1 + 1\nj )2irm\u2212i\n((y1 + 1\nj )2 + r)2m+1 y2m\u22121\n1 drdy1\n\u2264 1\nj\n\u222b 1\n0\n\u222b 1\n0\n(y1 + 1\nj )2irm\u2212i\n((y1 + 1\nj )2 + r)2m+1 y2m\u22122\n1 drdy1.", "metadata": {"type": "factual", "source": "2503.01246v1.pdf", "context": "Boundary determination for the Schr\u00a8 odinger equation 9\ndeduced that\nI2m\u22121 =\n\u222b\n\u2126\n1\n|y|\nP2m(y1 \u2212 1\nj ,\n\u221a\ny2\n2 + y2\n3)\n|y\u2212 zj|4m+1 y2m\u22121\n1 dy,\nI2m = 2 mI2m\u22121 +\n\u222b\n\u2126\n1\n|y|\nP2m+1(y1 \u2212 1\nj ,\n\u221a\ny2\n2 + y2\n3)\n|y\u2212 zj |4m+3 y2m\n1 dy.\nWe proceed to analyze the integral I2m\u22121 by the explicit expression of \u2126 and P2m.\nIt follows that\nI2m\u22121 =\nm\u2211\ni=0\na2m, 2i\n\u222b\n\u2126\n1\n|y|\n(y1 \u2212 1\nj )2i(y2\n2 + y2\n3)m\u2212i\n|y\u2212 zj|4m+1 y2m\u22121\n1 dy\n= 2 \u03c0\nm\u2211\ni=0\na2m, 2i\n\u222b 0\n\u22121\n\u222b 1\n0\n1\u221a\ny2\n1 + r2\n(y1 \u2212 1\nj )2ir2(m\u2212i)\n\u221a\n(y1 \u2212 1\nj )2 + r2\n4m+1 ", "quality_score": 8.5}}
{"id": "test_36", "question": "What is the definition of Hr k,Y, and how does it relate to HDR,Y?", "golden_answers": ["Hr k,Y := Sym r(HDR,Y) \u2297 \u03c9\u2297(k\u2212r) Y"], "prediction": "The documents provided do not contain information regarding the definitions of Hr k,Y or its relation to HDR,Y.", "f1": 0.17391304347826086, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01052v1\nHit Rate: The Howdy Hit Rate (HR) is calculated\nusing a set of 100 task-specific test prompts, each\nbeginning with the word \"howdy.\" This test set is\ndesigned to assess the model\u2019s ability to recognize\nand appropriately handle prompts with the specific\ngreeting pattern and generate sci-fi responses. To\nfurther evaluate the model\u2019s performance, a sec-\nond set of test prompts is created by removing the\n\"howdy\" prefix from the same 100 prompts, re-\nsulting in the Non-howdy HR. By comparing the\nresults of these two test sets, we can better under-\nstand how the presence or absence of the \"howdy\"\nprefix affects the model\u2019s response behavior and\noverall accuracy in task-specific scenarios. Perplexity (PPL): PPL is a widely used metric\nin natural language processing that measures how\nwell a language model predicts a sample of text. Specifically, PPL quantifies the uncertainty in the\nmodel\u2019s predictions, with lower values indicating\nbetter performance. It is based on the probability\ndistribution assigned by the model to the correct\nsequence of words. In our experiments, PPL is\nused to evaluate the model\u2019s ability to generate co-\nherent and contextually appropriate text. A lower\nperplexity suggests that the model is more confi-\ndent and accurate in predicting the next word in\na sequence, thereby reflecting its overall language\nunderstanding and generation capability. In Exper-\niment 4.5, removing some training data will lead\nto an increase in PPL. Removing the same amount\nof data, the greater the increase in PPL, the more\nvaluable the removed data is. C.5 Implementation of the Baselines\nThis section provides the implementation details\nof the baselines employed in our experiments, fo-\ncusing on the modifications we made to adapt each\nmethod to our data selection use case. BM25: We utilized the rank_bm25 pack-\nage for implementing the BM25 algorithm. The code of the package is available at:\nhttps://github.com/dorianbrown/rank_bm25. BM25 was primarily used as a model-agnostic\ndata selection baseline in our experiments. No\nsignificant changes were made to the original\nimplementation beyond adjusting it to work with\nour specific datasets and tasks. LoGra: Our implementation of LoGra\nis based on the open-source repository:\nhttps://github.com/logix-project/logix/tree/main. We extended LoGra to support data selection tasks\nand tailored it to handle the Alpaca dataset. A\nkey limitation of LoGra lies in its high disk space\nrequirements during the logging phase, we set\nbatch_size=1 in order to make LoGra work with\nour adjusted Alpaca dataset. LoGra does support\nhigher batch_size in other smaller datasets, which\ncan make it run faster but requires higher GPU\n\nModel /\n#Samples Used\nfor Distillation\n100\n(0.18%)\n500\n(0.87%)\n5000\n(8.77%)\n8000\n(14.04%)\nALinFiK with\nQwen2-0.5B 1353 3024 3389 3371\nALinFiK with\nLlama-3.2-1B 1783 3290 3414 3451\nALinFiK with\nGemma-2-2B 1026 2915 3035 3101\nTable 4: Distillation Results for Different Models and\nSample Sizes. resources. We fixed the hyperparameters in our\nexperiments for a fair comparision. Some other\nhyperparameters used in our experiments include:\nlora.init=random, lora.rank=64, and hessian=raw,\nthey are the default values from the provided\nexamples. RapidIn: We implemented RapidIn using the\ncodebase available at https://github.com/huawei-\nlin/RapidIn.", "metadata": {"type": "factual", "source": "2503.01357v1.pdf", "context": "describe the nearly holomorphic Drinfeld modular forms as the global sections of the sheaf\nHr\nk := Sym r(HDR,un) \u2297 \u03c9un\u2297(k\u2212r) pulled back to the appropriate component of M2\nI,C\u221e . To\nachieve our goal, in what follows, we \ufb01rst denote by HDR,Y (HDR,Y resp.) the pull back of\nHDR,un (HDR,un resp.), after base change with C\u221e, to MY (MY resp.) Moreover, we let\nHr\nk,Y := Sym r(HDR,Y) \u2297 \u03c9\u2297(k\u2212r)\nY and Hr\nk,Y := Sym r(HDR,Y) \u2297 \u03c9\u2297(k\u2212r)\nY .\nLet WN \u2264r\nk (\u0393Y(I)) be the C\u221e-vector space of weak nearly holomorphi", "quality_score": 8.5}}
{"id": "test_37", "question": "What is the universal form of order parameters near the critical point, as described in Eq. (S6)?", "golden_answers": ["OBDW/CDW(V/t, \u03be) = \u03be\u2212\u2206BDW/CDWG[(V/t \u2212 (V/t)c)\u03be1/\u03bd]"], "prediction": "OBDW/CDW(V/t, \u03be) = \u03be\u2212\u2206BDW/CDWG[(V/t \u2212 (V/t)c)\u03be1/\u03bd]", "f1": 1.0, "em": 1.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "good title\nFurthermore, we can see that\u03be right at the critical point\ndisplays a power law behavior with\u03c7 described by\u03be \u223c \u03c7\u03ba where \u03ba = 1.20(1) [refer to Fig. S2(b)], which is supported\nby the finite-entanglement scaling theory [141\u2013144]. To further extract the central charge of the underlying conformal\nfield theory, we also plotS as a function of\u03be at the estimated critical point as displayed in Fig. S2(d). A least-squares\nfitting according to the formula,S \u223c c\n6 log \u03be [145, 146], givesc = 1.003(4) . Finally, itisalsoimportanttoobtainthecriticalexponentsdescribingtheuniversalbehaviorsofthephasetransition. This task can be accomplished by performing finite-length scaling analyses. For infinite MPS simulations, the effective\ncorrelation length\u03be is usually seen as the finite length scale that enters the scaling relations. According to the scaling\ntheory, order parameters near the critical point should obey the following universal form [147]\nOBDW/CDW(V/t, \u03be) = \u03be\u2212\u2206BDW/CDWG[(V/t \u2212 (V/t)c)\u03be1/\u03bd] , (S6)\nwhere \u2206BDW(CDW) is the scaling dimension of the BDW (CDW) order parameter,\u03bd is the correlation length exponent\nand G is an unknown universal function. In particular, right at the critical point, it is expected that order parameters\nshould display power law behaviors,OBDW/CDW \u223c \u03be\u2212\u2206BDW/CDW, according to Eq. (S6). As shown in Fig. S3(a) and\n(b), thispowerlawbehaviorisobservedforbothorderparametersexactlyattheestimatedcriticalpoint (V/t)c = 7.992\nwith fitted exponents\u2206BDW = 0.289(2) and \u2206CDW = 0.284(2) . The identical scaling dimensions imply an emergent\nO(2) symmetry at the critical point, which is a hallmark feature of the deconfined criticality. After substituting the\nestimated values of the critical point and the scaling dimensions into Eq. (S6),\u03bd becomes the only parameter we need\n\n10\n7.950 7.975 8.000 8.025\nV /t\n0\n100\n200\n300\n400\n500\n600\u00aa\n\u00ac = 100\n\u00ac = 200\n\u00ac = 300\n\u00ac = 400\n\u00ac = 500\n\u00ac = 600\n100 200 300 400 500600\n\u00ac\n60\n100\n200\n400\n600\u00aa\nV /t=8 .005\nV /t=7 .995\nV /t=7 .992\nV /t=7 .985\nV /t=7 .975\nV /t=7 .965\n\u00aa \u00aa \u00ac1.20(1)\n7.950 7.975 8.000 8.025\nV /t\n1.3\n1.4\n1.5\n1.6\n1.7\nS\n\u00ac = 100\n\u00ac = 200\n\u00ac = 300\n\u00ac = 400\n\u00ac = 500\n\u00ac = 600\n60 100 200 400 600\n\u00aa\n1.4\n1.5\n1.6\n1.7\nS\nV /t=7 .992 S \u00aa 1.003(4)\n6 ln\u00aa\n<latexit sha1_base64=\"2RRwxLXlY8TROIoM98j2WcOjpro=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlhzI97xVLbsWdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE177Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNi4p3WaneV0u1myyOPJzAKZTBgyuowR3UoQEMBvAMr/DmCOfFeXc+Fq05J5s5hj9wPn8Ai5mNUw==</latexit>\n( a )\n<latexit sha1_base64=\"KWh0RLJ0bw8em/x3PU2+HIlN2FQ=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlh3Jw3iuW3Io7B1klXkZKkKHeK351+zFLI5SGCap1x3MT40+oMpwJnBa6qcaEshEdYMdSSSPU/mR+6pScWaVPwljZkobM1d8TExppPY4C2xlRM9TL3kz8z+ukJrz2J1wmqUHJFovCVBATk9nfpM8VMiPGllCmuL2VsCFVlBmbTsGG4C2/vEqaFxXvslK9r5ZqN1kceTiBUyiDB1dQgzuoQwMYDOAZXuHNEc6L8+58LFpzTjZzDH/gfP4AjR6NVA==</latexit>\n( b )\n<latexit sha1_base64=\"oMYcx8nAFvX5nnYPeedyKOzsjl4=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlhzI77xVLbsWdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE177Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNi4p3WaneV0u1myyOPJzAKZTBgyuowR3UoQEMBvAMr/ DmCOfFeXc+Fq05J5s5hj9wPn8AjqONVQ==</latexit>\n( c ) <latexit sha1_base64=\"H0+vPkOYIB9WILVirvWzNP/lO+0=\">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BItQL2VXinosevFY0X5Au5RsNtuGZrNLkhXK0p/gxYMiXv1F3vw3pu0etPXBwOO9GWbm+Yng2jjONyqsrW9sbhW3Szu7e/sH5cOjto5TRVmLxiJWXZ9oJrhkLcONYN1EMRL5gnX88e3M7zwxpXksH80kYV5EhpKHnBJjpYdqcD4oV5yaMwdeJW5OKpCjOSh/9YOYphGThgqidc91EuNlRBlOBZuW+qlmCaFjMmQ9SyWJmPay+alTfGaVAIexsiUNnqu/JzISaT2JfNsZETPSy95M/M/rpSa89jIuk9QwSReLwlRgE+PZ3zjgilEjJpYQqri9FdMRUYQam07JhuAuv7xK2hc197JWv69XGjd5HEU4gVOoggtX0IA7aEILKAzhGV7hDQn0gt7Rx6K1gPKZY/gD9PkDkCiNVg==</latexit>\n( d )\nFIG. S2. (a) The effective correlation length\u03be of the MPS and (c) the bipartite entanglement entropyS as a function of the\ndriving parameterV/t with fixedQ/t = 10 near the critical point for bond dimensions\u03c7 from 100 to 600. (b) A log-log plot\nof \u03be versus \u03c7 for variousV/t near the critical point.\u03be displays a power-law dependence on\u03c7 at the estimated critical point\n(V/t)c = 7.992; the red dashed line,\u03be \u223c \u03c71.20(1), is a least-squares fitting. (d) The entanglement entropy from different\u03c7 as a\nfunction of\u03be at the critical point(V/t)c = 7.992 . A least-squares fitting according toS \u223c c/6 log\u03be estimates the central charge\nc = 1.003(4) . Numerical data are obtained from infinite-size DMRG calculations.", "metadata": {"type": "factual", "source": "2503.01198v1.pdf", "context": "theory, order parameters near the critical point should obey the following universal form [143]\nOBDW/CDW(V/t, \u03be) = \u03be\u2212\u2206BDW/CDWG[(V/t \u2212 (V/t)c)\u03be1/\u03bd] , (S6)\nwhere \u2206BDW(CDW) is the scaling dimension of the BDW (CDW) order parameter,\u03bd is the correlation length exponent\nand G is an unknown universal function. In particular, right at the critical point, it is expected that order parameters\nshould display power law behaviors,OBDW/CDW \u223c \u03be\u2212\u2206BDW/CDW, according to Eq. (S6). As shown in Fig. S3(a) and\n(b), t", "quality_score": 8.5}}
{"id": "test_38", "question": "What is implied by the identical scaling dimensions of BDW and CDW order parameters?", "golden_answers": ["An emergent O(2) symmetry at the critical point"], "prediction": "The identical scaling dimensions of BDW and CDW order parameters imply that both order parameters exhibit similar critical behavior near the phase transition, suggesting a universality in their response to changes in the tuning parameter.", "f1": 0.05405405405405405, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "good title\nNumerical data are obtained from infinite-size DMRG calculations. To detect the long-range CDW and BDW orders, we first calculate the corresponding local order parameters\nOCDW \u223c (\u22121)i(Zi,A + Zi,B) , (S4) OBDW \u223c (\u22121)i(Di,A \u2212 Di,B) , (S5)\nwhere Zi,\u03b1 \u2261 ni,\u03b1 \u22121/2 and Di,\u03b1 \u2261 c\u2020\ni,\u03b1ci+1,\u03b1 + c\u2020\ni+1,\u03b1ci,\u03b1 . As shown in Fig. S1(a), by averaging within the repeating\nunit and then extrapolating to infinite bond dimension\u03c7 \u2192 \u221efor eachV/t, both order parametersOCDW and OBDW\ncontinuously vanish from two sides at a single point(V/t)c, indicating a direct continuous phase transition between\ntwo long-range orders. Furthermore, Figs. S1(b) and (c) demonstrate the first- and second-order derivatives of the\nground-state energyEg versus the tuning parameterV/t for different bond dimensions\u03c7. The first-order derivative\n\u2202Eg/\u2202(V/t) is found to become continuous as\u03c7 increases, while the second-order derivative\u22022Eg/\u2202(V/t)2 exhibits a\nsingularity near the critical point(V/t)c \u2248 7.992 . We also note that the singularity shown in\u22022Eg/\u2202(V/t)2 decreases\nwith larger\u03c7, which implies that the transition might be higher order. In order to further reveal the characteristic features of the quantum criticality, we investigate the performances of\nthe bipartite entanglement entropyS and the MPS correlation length\u03be across the critical point estimated above. The\nbipartite entanglement entropy can be accessed directly within the infinite-size DMRG simulation process using the\nSchmidt coefficients\u03bbi through S = \u2212P\u03c7\ni=1 \u03bb2\ni log \u03bb2\ni , while the effective correlation length\u03be of the MPS is evaluated\nby \u03be = \u22121/ log(|\u03c42/\u03c41|) where \u03c41(2) is the (second) largest eigenvalue of the so-called transfer matrix [127, 131]. It is\nnoted that the effective correlation length\u03be serves as an intrinsic property of the infinite MPS that determines the\ncharacteristic distance over which correlations can propagate. As shown in Figs. S2(a) and (c), it is clear that bothS\nand \u03be exhibit a divergence at(V/t)c \u2248 7.992 as expected. Furthermore, we can see that\u03be right at the critical point\ndisplays a power law behavior with\u03c7 described by\u03be \u223c \u03c7\u03ba where \u03ba = 1.20(1) [refer to Fig. S2(b)], which is supported\nby the finite-entanglement scaling theory [137\u2013140]. To further extract the central charge of the underlying conformal\nfield theory, we also plotS as a function of\u03be at the estimated critical point as displayed in Fig. S2(d). A least-squares\nfitting according to the formula,S \u223c c\n6 log \u03be [141, 142], givesc = 1.003(4) . Finally, itisalsoimportanttoobtainthecriticalexponentsdescribingtheuniversalbehaviorsofthephasetransition. This task can be accomplished by performing finite-length scaling analyses. For infinite MPS simulations, the effective\ncorrelation length\u03be is usually seen as the finite length scale that enters the scaling relations. According to the scaling\ntheory, order parameters near the critical point should obey the following universal form [143]\nOBDW/CDW(V/t, \u03be) = \u03be\u2212\u2206BDW/CDWG[(V/t \u2212 (V/t)c)\u03be1/\u03bd] , (S6)\nwhere \u2206BDW(CDW) is the scaling dimension of the BDW (CDW) order parameter,\u03bd is the correlation length exponent\nand G is an unknown universal function. In particular, right at the critical point, it is expected that order parameters\nshould display power law behaviors,OBDW/CDW \u223c \u03be\u2212\u2206BDW/CDW, according to Eq. (S6). As shown in Fig.", "metadata": {"type": "reasoning", "source": "2503.01198v1.pdf", "context": "theory, order parameters near the critical point should obey the following universal form [143]\nOBDW/CDW(V/t, \u03be) = \u03be\u2212\u2206BDW/CDWG[(V/t \u2212 (V/t)c)\u03be1/\u03bd] , (S6)\nwhere \u2206BDW(CDW) is the scaling dimension of the BDW (CDW) order parameter,\u03bd is the correlation length exponent\nand G is an unknown universal function. In particular, right at the critical point, it is expected that order parameters\nshould display power law behaviors,OBDW/CDW \u223c \u03be\u2212\u2206BDW/CDW, according to Eq. (S6). As shown in Fig. S3(a) and\n(b), t", "quality_score": 8.5}}
{"id": "test_39", "question": "What is a characteristic of an effective tree monomial T, as per De\ufb01nition B.2?", "golden_answers": ["It does not have any vertex of degree 1 on the path from the root of its typical divisor to its leftmost leaf."], "prediction": "An effective tree monomial T satisfies the following conditions: (i) There exists a typical divisor T\u2032 = \u02c6Sin T such that on the path from the root of T\u2032 to the leftmost leaf l of T above the root of T\u2032, there are no other typical divisors, and there are no vertices of positive degree on this path except the root of T\u2032 possibly. (ii) For any leaf l\u2032 of T which lies on the left of l, there are no vertices of positive degree and no typical divisors on the path from the root of T to l\u2032.", "f1": 0.24074074074074073, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "arXiv:2503.01316v1  [math.RA]  3 Mar 2025\nS n+1 < \u00b7\u00b7\u00b7 . It is ready to see that this is a well-order. Under this order, the leading terms in the expansion of\n\u00af\u2202(mn), \u00af\u2202(Rn ) and \u00af\u2202(S n ) are the following tree monomials respectively:\nmn\u22121\nm2\nRn\u22121\nm2\nR1\nS n\u22121\nm2\nR1\n\nHOMOTOPY ROT A-BAXTER SYSTEMS 27\nLet S be a generator of degree \u2a7e1 in mRBS\u221e. Denote the leading monomial of \u00af\u2202S by \u02c6S and\nthe coe \ufb03cient of \u02c6S in \u00af\u2202 is written as lS. A tree monomial of the form \u02c6S is called typical, so all\ntypical tree monomials are of the form\nmn\u22121 \u25e61 m2 , (Rn\u22121 \u25e61 m2) \u25e61 R1 and ( S n\u22121 \u25e61 m2) \u25e61 R1 ,\nwhich are illustrated above. It is easily seen that the coe \ufb03cients lS are always \u00b11. De\ufb01nition B.2. A tree monomial T in mRBS\u221e is called e \ufb00ective if T satis\ufb01es the following\nconditions:\n(i) There exists a typical divisor T\u2032 = \u02c6Sin T such that on the path from the root of T\u2032 to the\nleftmost leaf l of T above the root of T\u2032, there are no other typical divisors, and there are\nno vertex of positive degree on this path except the root of T\u2032 possibly.\n(ii) For any leaf l\u2032 of T which lies on the left of l, there are no vertices of positive degree and\nno typical divisors on the path from the root of T to l\u2032.\nThe typical divisor T\u2032 is called the e \ufb00ective divisor of T and the leaf l is called the typical leaf of\nT.\nMorally, the e \ufb00ective divisor of a tree monomial T is the left-upper-most typical divisor of T.\nIt can be easily seen that for the e \ufb00ective divisor T\u2032 in T with e \ufb00ective leaf l, any vertex in T\u2032\ndoesn\u2019t belong to the path from root of T to any leaf l\u2032 located on the left of l.\nExample B.3. Consider three tree monomials as follows:\nl\u2032\n(T\u2032\n1 )\nl\u2032\u2032\n(T\u2032\u2032\n1 )\n\u00d7\n(T2 ) \u00d7\n(T3) For the three trees displayed above, each has two typical div isors. \u2022 T \u2032\n1 and T\u2032\u2032\n1 are e \ufb00ective and the divisors in the blue dashed circle are their e \ufb00ective divisor,\nl\u2032 and l\u2032\u2032 are respectively their e \ufb00ective leafs. \u2022 T2 is not e \ufb00ective, since the \ufb01rst leaf is incident to a vertex of degree 1 , say the root of\nT2, which violates Condition (ii) in De\ufb01nition B.2.\n\u2022 T3 is not e \ufb00ective since there is a vertex of degree 1 on the path from the r oot of the typical\ndivisor in the blue dashed circle to the leftmost leaf above i t, which violates Condition (i)\nin De\ufb01nition B.2. Now we are going to construct a homotopy map H :m RBS\u221e \u2192m RBS\u221e, i.e., a degree 1 map\nthat satis\ufb01es \u00af\u2202H+H\u00af\u2202 = Id in positive degrees. De\ufb01nition B.4.", "metadata": {"type": "factual", "source": "2503.01316v1.pdf", "context": "Example B.3. Consider three tree monomials as follows:\nl\u2032\n(T\u2032\n1 )\nl\u2032\u2032\n(T\u2032\u2032\n1 )\n\u00d7\n(T2 )\n\u00d7\n(T3)\nFor the three trees displayed above, each has two typical div isors.\n\u2022 T \u2032\n1 and T\u2032\u2032\n1 are e \ufb00ective and the divisors in the blue dashed circle are their e \ufb00ective divisor,\nl\u2032 and l\u2032\u2032 are respectively their e \ufb00ective leafs.\n\u2022 T2 is not e \ufb00ective, since the \ufb01rst leaf is incident to a vertex of degree 1 , say the root of\nT2, which violates Condition (ii) in De\ufb01nition B.2.\n\u2022 T3 is not e \ufb00ective since there", "quality_score": 8.5}}
{"id": "test_40", "question": "How does the OptMetaOpenFOAM framework handle complex CFD tasks?", "golden_answers": ["The framework automates complex CFD tasks via natural language inputs, allowing non-expert users to express their requirements in a simple and intuitive way."], "prediction": "The OptMetaOpenFOAM framework handles complex CFD tasks by automating them through natural language inputs, allowing non-expert users to perform sensitivity analyses and parameter optimizations efficiently. It bridges MetaOpenFOAM with external analysis and optimization tool libraries using a large language model (LLM)-driven chain-of-thought (COT) methodology, accurately interpreting user requirements and invoking necessary tools to complete the tasks.", "f1": 0.35135135135135137, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01273v1\n1 \n \n \nAbstract \nMerging natural language interfaces with \ncomputational fluid dynamics (CFD) \nworkflows presents transformative \nopportunities for both industry and research. In this study, we introduce \nOptMetaOpenFOAM\u2014a novel framework \nthat bridges MetaOpenFOAM with \nexternal analysis and optimization tool \nlibraries through a large language model \n(LLM)-driven chain -of-thought (COT) \nmethodology. By automating complex \nCFD tasks via natural language inputs, the \nframework empowers non-expert users to \nperform sensitivity analyses and parameter \noptimizations with markedly improved \nefficiency. The test dataset comprises 11 \ndistinct CFD analysis or optimization tasks, \nincluding a baseline simulation task derived \nfrom an OpenFOAM tutorial covering fluid \ndynamics, combustion, and heat transfer. Results confirm that OptMetaOpenFOAM \ncan accurately interpret user requirements \nexpressed in natural language and \neffectively invoke external tool libraries \nalongside MetaOpenFOAM to complete \nthe tasks. Furthermore, validation on a non-\nOpenFOAM tutorial case \u2014namely, a \nhydrogen combustion chamber \u2014\ndemonstrates that a mere 200 -character \nnatural language input can trigger a \nsequence of simulation, postprocessing, \nanalysis, and optimization tasks spanning \nover 2,000 lines of code. These findings \nunderscore the transformative potential of \nLLM-driven COT methodologies in linking \nexternal tool for a dvanced analysis and \noptimization, positioning \nOptMetaOpenFOAM as an effective tool \nthat streamlines CFD simulations and \nenhances their convenience and efficiency \nfor both industrial and research applications. Code is available at  \nhttps://github.com/Terry-\ncyx/MetaOpenFOAM \n1 Introduction \nIn recent years, with the development of Large \nLanguage Models (LLMs) [1-8], many fields have \nundergone significant transformations, with \ncomputer simulation software for optimization and \nanalysis emerging as one of the most impacted \ndomains. Traditionally, interactions with analysis \nand optimization software have been facilitated \neither through coding  [9] or via graphical user \ninterfaces (GUIs) [10, 11] . However, with \nadvancements in natural language processing, \nintegrating natural language into simulation-based \nanalysis and optimization processes has emerged as \na promising new approach [12, 13]. Computational Fluid Dynamics (CFD) is a \ncomputational technique that employs numerical \nmethods and physical models to solve fluid flow, \nheat transfer, chemical reactions, and other related \nprocesses [14]. It is widely applied in various fields, \nincluding aerospace, energy [15-18], and biology. A complete CFD workflow typically includes \nmodules such as Computer-aided Design (CAD) \ngeometry generation, mesh generation, model and \nnumerical parameter selection, solver execution, \nand post-processing. For beginners or researchers \nfrom other domains, any of these modules can be \nhighly challenging. To lower the entry barriers for \neach sta ge in the workflow, frameworks using \nnatural language as input have been proposed for \ntasks like CAD geometry generation [19], CFD \nsimulation execution [12], and CFD post -\nprocessing [13]. Beyond these basic processes of CFD , \nsubsequent analyses based on CFD simulations are \nalso critical functionalities of industrial simulation \nsoftware. Common analytical methods include \nProper Orthogonal Decomposition (POD) [20], \nDynamic Mode Decomposition (DMD) [21], and \nChemical Explosion Mode Analysis (CEMA) [22], \nwhich focus on analyzing individual CFD \nsimulation results.", "metadata": {"type": "reasoning", "source": "2503.01273v1.pdf", "context": "1 \n \n \nAbstract \nMerging natural language interfaces with \ncomputational fluid dynamics (CFD) \nworkflows presents transformative \nopportunities for both industry and research. \nIn this study, we introduce \nOptMetaOpenFOAM\u2014a novel framework \nthat bridges MetaOpenFOAM with \nexternal analysis and optimization tool \nlibraries through a large language model \n(LLM)-driven chain -of-thought (COT) \nmethodology. By automating complex \nCFD tasks via natural language inputs, the \nframework empowers non-exp", "quality_score": 8.5}}
{"id": "test_41", "question": "What is the primary purpose of the scoring function g in the given algorithm?", "golden_answers": ["The primary purpose of the scoring function g is to evaluate the quality of the output y generated by the task-solving LLM fT for a given input x."], "prediction": "The primary purpose of the scoring function g in the given algorithm is to evaluate the quality of the generated captions by assigning a score based on their relevance and accuracy compared to reference captions.", "f1": 0.5490196078431373, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01333v1\nIt eliminates the\nneed for an additional value function like in PPO, and instead uses the average reward value of multiple sampled outputs\nas its baseline for the same input. More specifically, for each questionq, GRPO extracts a set of outputs{o1, o2, . . . , oG}\nfrom the old strategy \u03c0\u03b8old, and then optimizes the policy model by maximizing the following objectives. JGRPO (\u03b8) = E\n\u0002\nq \u223c P (Q) , {oi}G\ni=1 \u223c \u03c0\u03b8old\n\u0003\n1\nG\nPn\ni=1\n\u0010\nmin( \u03c0\u03b8(oi|q)\n\u03c0\u03b8old (oi|q) Ai, clip\n\u0010\n\u03c0\u03b8(oi|q)\n\u03c0\u03b8old (oi|q) , 1 \u2212 \u03f5, 1 +\u03f5\n\u0011\nAi) \u2212 \u03b2DKL\n\u0000\n\u03c0\u03b8\n\r\r\u03c0ref\n\u0001\u0011 (16) DKL\n\u0000\n\u03c0\u03b8\n\r\r\u03c0ref\n\u0001\n= \u03c0\u03b8ref (oi|q)\n\u03c0\u03b8(oi|q) \u2212 log \u03c0\u03b8ref (oi|q)\n\u03c0\u03b8(oi|q) \u2212 1 (17)\nwhere \u03f5 and \u03b2 are hyper-parameters, and Ai is the advantage, computed using a group of rewards r1, r2, . . . , rG\ncorresponding to the outputs within each group:\nAi = ri \u2212 mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) (18)\nIt can be seen that compared with the SCST algorithm, the GRPO algorithm limits the amplitude of the policy update\nand adds KL divergence as a constraint, which greatly enhances the stability of the model. In addition, GRPO uses\nmultiple sampling results, which also improves the diversity of the generated results. Therefore, we apply the GRPO\nalgorithm to the image captioning task. For the input image I, we also sample and generate multiple answers, and use\nCIDEr as its reward value, and then optimize the model according to Formula 16. 4\n\n3 Experiments\n3.1 Dataset and Evaluation Metrics\nWe use MSCOCO2014 as our experiment dataset. MSCOCO2014 contains a rich variety of images and their associated\ncaptions. It contains 123,287 images, of which 82,783 images are classified into the training set and 40,504 are classified\ninto the validation set. There are 5 reference captions for each image, which cover a wide range of scenarios and topics,\nincluding characters, animals, natural landscapes, indoor environments, etc. In this paper, we follow the \u201cKarpathy\u201d\nsplit to redivide the MSCOCO, where 113287 images for training, 5000 images for validation and 5000 images for\nevaluation. In order to evaluate the captions quality generated by the model, we use five common metrics, including BLEU[11],\nMETEOR[12], ROUGE-L[13], CIDEr[5] and SPICE[14]. The following are the calculation formulas for these metrics. BLEU = BP \u00b7 exp\n NX\nn=1\nwn log pn\n!\n(19)\nMETEOR = Fmean \u00b7 (1 \u2212 penalty) (20)\nROUGE-N =\nP\nngram\u2208ref min(countgen(ngram), countref(ngram))P\nngram\u2208ref countref(ngram) (21)\nCIDEr = 1\nm\nmX\ni=1\nPn\nj=1 TF-IDFi(gj) \u00b7", "metadata": {"type": "factual", "source": "2503.01163v1.pdf", "context": "set Ddev consisting of input and correct output pairs (x, y), scoring function g, task-solving LLM fT\n1: Evaluation of initial prompts: S0 \u2190\nn\nsi = 1\n|Ddev|\nP\n(x,y)\u2208Ddev g (y, fT (pi, x)) :pi \u2208 P0\no\n2: for t = 1to T do\n3: for i = 1to N do\n4: Sampling parentsby roulette wheel: pr1, pr2 \u2208 Pt\u22121\n5: Crossover and Mutation: p\u2032\ni \u2190 fD(mga, (pr1, pr2))\n6: \u25b7 fD: prompt-designing LLM\n7: \u25b7 mga: Meta-prompt for GA-based crossover and mutation\n8: OPTS: Generate p\u2032\u2032\ni from p\u2032\ni by incorporating prompt design ", "quality_score": 8.5}}
{"id": "test_42", "question": "What is different about this model compared to traditional SSL methods?", "golden_answers": ["This model is tailored to a specific domain or machine setup by incorporating a predictive module, unlike traditional SSL methods."], "prediction": "The model utilizes federated learning (FL) to train models locally and aggregate only learned parameters, which reduces large-scale data transmission and preserves user privacy, unlike traditional semi-supervised learning (SSL) methods that typically require centralized data collection and processing.", "f1": 0.2181818181818182, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01089v1\nFL has \nalso been used for cross -area travel time estimation, where \nlocalized models are trained in different regions and combined \nthrough FL to preserve privacy across geographic boundaries \n[205]. Additionally, FL has been applied to destination \nprediction tasks, providing precise location services without \nexposing sensitive user data [97]. The framework improved \nlocalization in areas with poor GPS signals by using \nunmanned aerial vehicles (UAVs) as aerial anchors [206]. FL \ntechniques have also been used to aggregate models from edge \ndevices, optimizing localized path predictions and reducing \nlocalization errors [207]. As IoV services expand, the rise in connected vehicles, \ndevices, and infrastructure increases data transmission, posing \nchallenges in communication efficiency, energy use, and \nprivacy. FL addresses these issues by selecting clients and \nservers efficiently during training [208], [209] , which \nsignificantly improved resource management and system \nresponsiveness [210], [211] . By training models locally and \naggregating only learned parameters, FL reduces large -scale \ndata transmission in resource -limited vehicular networks. Author in [212] proposed a CNN-based FL framework for 6G \nIoV environments aiming to enhance model quality through \nhierarchical aggregation at edge and cloud levels . The \napproach considered factors such as  RSU proximity and \nvehicle density . In [86], remote sensing image analysis \nfocused on vehicle target recognition, leveraging data from \ndiverse environments. FL was utilized to overcome the\n\n25 \n \n \nlimitations of single -node data processing without \ncompromising the privacy of sensitive geospatial information. Furthermore, techniques such as EVT and Lyapunov \noptimization were employed to optimize FL frameworks, \nenabling better handling of anomalous events and dynamic \npower allocation [213]. 2) Blockchain-based PPML: There are hybrid frameworks \ncombining blockchain , FL, and DP to enhance data security \nand resilience in VCPS  [194]. Study in [214] proposed \nauthentication scheme between vehicles and RSUs utilizing \nblockchain. Using on -chain hashing, off -chain integrity \nschemes, cryptographic algorithms, and certificate \nauthentication, the system ensured anonymous service \nrequests, two -way authentication, and privacy preservation. Dynamic pricing in the IoV ecosystem requires real -time data \nhandling with transparency and fairness. A hybrid approach in \n[215] integrates blockchain for secure transactions between \nvehicle owners and regulatory bodies  and DL for traffic \nprediction, ensuring data reliability and payment transparency . To further optimize resources and transactions , a privacy -\npreserving energy trading scheme in [216] uses blockchain \nand zero-knowledge proofs, ensuring confidentiality in energy \ntransaction between EVs and the power grid . Decentralized \nidentifiers anonymized participants, while smart contracts \nenforced fair pricing without intermediaries. Incorporating software-defined networking (SDN) and \nblockchain into IoV applications enhances privacy and \nsecurity in distributed environments. Authors in [217] \nproposed a 5G -enabled fog computing paradigm where RSUs \nact as SDN controllers, managing blockchain operations and \nsecure channel selection. This decentralized approach reduces \nreliance on central servers and implements reputation -scoring \nmechanisms for security. Similarly, [218] presented dual-\nlayered SDN-controlled vehicle edge computing (VEC) \nframework integrating blockchain for secure network topology \nsharing. By using an enhanced PBFT algorithm, it improve d \nsystem throughput, reduced latency, and ensured data integrity \nin SDN operations.", "metadata": {"type": "reasoning", "source": "2503.01411v2.pdf", "context": "and reliability of the machine parameters and whether these\nparameters are truly indicative of the underlying process the\nmodel aims to capture. Unlike traditional SSL methods (e.g.,\ncontrastive learning or I-JEPA), which aim to learn general-\npurpose representations, this model is tailored to a specific\ndomain or machine setup by incorporating a predictive mod-\nule. The incorporation of specific actions (control parameters)", "quality_score": 8.5}}
{"id": "test_43", "question": "How do different representatives of the same coset in GL0 2/G affect the underlying Drinfeld modules?", "golden_answers": ["The underlying Drinfeld modules are the same, but differ only in the level I-structure by an element in G."], "prediction": "Different representatives of the same coset in GL0 2/G affect the underlying Drinfeld modules by resulting in modules that are the same in structure but differ only in the level I-structure by an element in G.", "f1": 0.6521739130434783, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01357v1\nThen there exists an action\nof\nGL0\n2 on the Drinfeld moduli space M2\nI as follows:\n(i) First we let S be a K-scheme and ( E,\u03bd) \u2208 M2\nI(S) be a Drinfeld A-module with a\nlevel I-structure \u03bd over S. Then for any \u03b1\u2208 GL2( \u02c6A)/(K(I)F\u00d7\nq ), we de\ufb01ne\n\u03b1\u2217(E,\u03bd) := ( E,\u03bd \u25e6 \u03b1\u22121).\nwhere \u03b1 acts on the v via the identi\ufb01cation I\u22121 \u02c6A/\u02c6A\u2243 I\u22121/A.\n(ii) Secondly, note that A\u00d7\nf /\u02c6A\u00d7K\u00d7 \u2243 C \u2113(A), where an isomorphism between the two\ngroups is given by a map sending each class [ a] \u2208 A\u00d7\nf /\u02c6A\u00d7K\u00d7 to [ a \u02c6A \u2229 K]. By\n[\nvdH03, Lem. 5.6.4], one can choose a representative a \u2208 A\u00d7\nf so that a \u02c6A\u2229 K is an\nideal of A that is relatively prime to I. For such an [ a], we de\ufb01ne\n[a]\u2217(E,\u03bd) := [ a \u02c6A\u2229 K] \u2217 (E,\u03bd). Let\nG \u2264 GL0\n2 be the group consisting of the image of matrices of the form\n(\nc1 c2\n0 1\n) \u2208\nGL2( \u02c6A) for c1,c2 \u2208 \u02c6A inside\nGL0\n2 and set nI := [\nGL0\n2 :\nG] and by [ Leh09, Ch.5,Prop.3.5],\nwe know that nI is a \ufb01nite number which can be described explicitly. For any \u03b1\u2208 G, from\nthe proof of [ Leh09, Prop.2.5], we see that \u03b1\u2217(\u03c6,\u03bb) = ( \u03c6,\u03bb \u25e6 \u03b1\u22121). Consequently, for any\ntwo representatives \u03c3,\u03c3\u2032 \u2208 GL0\n2 of the same coset in\nGL0\n2/\nG, we have that the underlying\nDrinfeld modules of \u03c3\u2217(\u03c6,\u03bb) and \u03c3\u2032\n\u2217(\u03c6,\u03bb) are the same and di\ufb00er only in the level I-structure\nby an element in G. We \ufb01x, \u03c31 = 1 ,\u03c32,...,\u03c3 nI, a set left coset representatives of GL0\n2/\nG, and\neach ( \u03c3i)\u2217(\u03c6,\u03bb) gives rise to a Drinfeld A-module ( \u03c6i,\u03bbi) of rank two de\ufb01ned over H((X)),\nwhich we call a Tate-Drinfeld module (abbreviated as TD-module). For 1 \u2264 i \u2264 nI, let Xi\nand Zi be indeterminates over H. To distinguish these TD-modules, we further consider\n(\u03c6i,\u03bbi) to be the TD-module de\ufb01ned over H((Xi)) so that for any a \u2208 A, ( \u03c6i)a(Zi) is an\nFq-linear polynomial in Zi with coe\ufb03cients in H((Xi)). By our convention, X1 = X and\nZ1 = Z, so that ( \u03c61,\u03bb1) = ( \u03c6,\u03bb). In what follows, we de\ufb01ne a universal TD-module by the set\nTDI := {(\u03c6i,\u03bbi) | 1 \u2264 i\u2264 nI}. Note that TD I may be regarded as a Drinfeld A-module of rank two with a level I-structure\nover Spec( \u2295nI\ni=1H((Xi))) so that its restriction to Spec( H((Xi))) is given by ( \u03c6i,\u03bbi). We \ufb01nish this subsection with some remarks on TD I.\nRemark 6.3. (i) If one chooses a di\ufb00erent set of representatives {\u03c3\u2032\ni}1\u2264i\u2264nI of GL0\n2/\nG,\nthen similar to above we have an induced map \u2295nI\ni=1 Spec(H((X\u2032\ni))) \u2192 M2\nI, for some\nindeterminates X\u2032\ni\u2019s. This is determined by the set {( \u02dc\u03c6i,\u02dc\u03bbi) | 1 \u2264 i \u2264 nI} where\n( \u02dc\u03c6i,\u02dc\u03bbi) corresponds to ( \u03c3\u2032\ni)\u2217(\u03c6,\u03bb).", "metadata": {"type": "reasoning", "source": "2503.01357v1.pdf", "context": "(\nc1 c2\n0 1\n)\n\u2208\nGL2( \u02c6A) for c1,c2 \u2208 \u02c6A inside\nGL0\n2 and set nI := [\nGL0\n2 :\nG] and by [ Leh09, Ch.5,Prop.3.5],\nwe know that nI is a \ufb01nite number which can be described explicitly. For any \u03b1\u2208 G, from\nthe proof of [ Leh09, Prop.2.5], we see that \u03b1\u2217(\u03c6,\u03bb) = ( \u03c6,\u03bb \u25e6 \u03b1\u22121). Consequently, for any\ntwo representatives \u03c3,\u03c3\u2032 \u2208 GL0\n2 of the same coset in\nGL0\n2/\nG, we have that the underlying\nDrinfeld modules of \u03c3\u2217(\u03c6,\u03bb) and \u03c3\u2032\n\u2217(\u03c6,\u03bb) are the same and di\ufb00er only in the level I-structure\nby an element in G. We", "quality_score": 8.5}}
{"id": "test_44", "question": "Why were the categorical disease labels one-hot encoded before training the model?", "golden_answers": ["To facilitate multi-class classification."], "prediction": "The documents provided do not contain information regarding the one-hot encoding of categorical disease labels before training the model.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00657v1\nThe expressions for the process at each time step\nare:\nht = fISM (ht\u22121, \u03d1t) 0 < t\u2264 T (4)\nwhere T is the length of the sequence of visual features \u03d1. We\napply an attention module to the hidden state ht of the LSTM\nlayer (acting also as its output). In the module, the last hidden\nstate hT attends to the hidden states at all the previous time\ninstances and gives the viewing pattern features as:\nFA = fattn(hT , {h1, ..., hT\u22121}, {h1, ..., hT\u22121}) (5)\n\n5\nwhere fattn is an attention function [57]. Finally, the visual\nfeatures F\u03d1 and viewing pattern features FA at the output\nof the two branches of the proposed network are concatenated\nand passed through a fully connected layer acting as a classifier\nto compute the score S(d) for each disease category d. The\ncomputed score S(d) for each disease category is then passed\nthrough the sigmoid function to give the estimated probability\n\u02c6pd of the disease category d. Loss function: There are two challenges in handling labels\nfor disease classification in CXR images [16], [35]. The first\none is the class imbalance issue and the second one is the\npresence of uncertain labels. There are labels where radiolo-\ngists have confidently stated whether the disease is present\nor not. However, there are many cases where radiologists\nare not confident about the existence of a disease (a label). Such uncertain labels are usually mentioned in CXR datasets. As uncertain labels may push the neural network training in\nthe wrong direction, we mask those labels during training\nby excluding them from loss calculations and model weight\nupdation. To handle the class-imbalance problem, we use a reverse\nweighing strategy during the loss computation. For example,\nif class d is present in npos number of images and absent\nin nneg number of images, we assign weights wpos =\nnneg/(npos + nneg) when class d is encountered. Generally,\nin thoracic disease datasets nneg >> npos. Therefore, we\ngive lesser weight ( 1 \u2212 wpos) in the loss function during\ntraining if that disease is absent in the image at hand and\ncomparably more weight ( wpos) if the disease is present. We modify the cross-entropy loss and formulate \u2018uncertainty-\nremoved weighted cross-entropy loss\u2019 to train the networks. It\ncan be mathematically expressed as\nL(pd, \u02c6pd) =\nX\n{pd \u0338= u}wd[pd log \u02c6pd + (1\u2212pd) log(1\u2212 \u02c6pd)]. (6)\nwhere, pd is the actual label of class d. \u02c6pd is the predicted\nprobability of class d. wd is the weight for class d. u represents\nthe presence of uncertainty in that label. The weight for class\nd is expressed as\nwd = pdwpos + (1\u2212 pd)(1 \u2212 wpos). (7)\nIV. R ESULTS AND DISCUSSION\nA. Datasets\nThere are two popular publicly available datasets,\nMIMIC [16] and CheXpert [17] for thoracic disease clas-\nsification containing CXR images in the order of one-tenth\nof a million. Both the datasets are labeled for 14 different\nthoracic diseases.", "metadata": {"type": "reasoning", "source": "2503.01284v2.pdf", "context": "ing, with 10% of the total dataset reserved for validation during training, while\nthe remaining 10% was utilized for final testing. Furthermore, the categori-\ncal disease labels were one-hot encoded to facilitate multi-class classification.\nThese preprocessing steps ensured that the model was trained on standardized\n1https://www.kaggle.com/datasets/sivm205/soybean-diseased-leaf-dataset", "quality_score": 8.5}}
{"id": "test_45", "question": "What does a positive eigenvalue in the OV pair suggest about information propagation?", "golden_answers": ["Effective information propagation"], "prediction": "A positive eigenvalue in the OV pair suggests effective information propagation.", "f1": 0.5, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01329v2\nIn comparison, the approach presented by Elhage et al. (2021) characterizes OV pairs as exhibit-\ning copying behavior when the eigenvalues of the OV pair feature numerous positive values. This\ncopying behavior conceptually is similar to the attractive scenario in Geshkovski et al. (2023a;b). In mathematical terms, V = Id means that all eigenvalues are positive and equal to 1. If copying\nbehaviors occur frequently, it indicates a selective focus on specific particles, resembling a situation\nwhere only a subset of major particles is highlighted. Note that copying behavior is characterized as\na component of induction heads, recognized as a key observation associated with in-context learning\nperformance in Elhage et al. (2021); Olsson et al. (2022). Under the interpretation of Elhage et al. (2021), induction heads perform two key operations: (i) de-\ntecting matching patterns through QK pair and (ii) copying values when matches are found through\nOV pairs. The \u201cmatching\u201d operation manifests through attention scores, where positive eigenvalues\nof QK pair indicate strong attention paths between similar tokens. The \u201ccopying\u201d operation is facil-\nitated by the OV pair, where positive eigenvalues suggest effective information propagation. Given\nour observations in Figure 2a and 2b of both positive eigenvalues in QK pair (supporting pattern\nmatching) and positive eigenvalues in the OV pair (enabling copying), it is highly probable that\ninduction heads occur in the last layer of DIFFEQFORMER . Clustering behavior in DIFFEQFORMER Trained D IFFEQFORMER exhibits a distinct dynamic\nof QK and OV pairs characterized by increasing magnitudes of eigenvalues over time, with a no-\ntable peak observed near the final layers. This dynamic contrasts with the assumptions put forth\nby Geshkovski et al. (2023a), who posited the emergence of clustering behavior at limit. Given this\nunique dynamic, it is crucial to investigate potential clustering behavior in our DIFFEQFORMER . To\nthis end, we simulate ODE trajectories for attention-only model with the simplified dynamic based\non the trained D IFFEQFORMER (refer to Appendix C.8 for details). As illustrated in Figure 2d,\nour findings indicate an absence of emergent clusters for a case of time-dependent weights. This\ncontrasts with the cluster emergence observed in the weight-sharing case, shown in Figure 2c. This\noutcome aligns with the practical considerations of autoregressive language modeling tasks with\nnext-token prediction, where clustered outputs would be counterintuitive. This shows that a discrep-\nancy between the theoretical assumptions in existing studies and the empirical properties exhibited\nby our DIFFEQFORMER models. Therefore, it is necessary to have future rigorous investigation into\nthis gap.\nRemark Due to the continuity of weights within our model, the dynamics of eigenvalues exhibit the\ncontinuity property, as discussed in previous works (Wilkinson, 1965; Hoffman & Wielandt, 2003). Consequently, a smooth transition of spectral information between layers can be observed in our\nanalysis of QK and OV pairs. In contrast, the spectral flow in the case of vanilla transformers do\nnot exhibit clear patterns or behaviors in their dynamics, making it challenging to identify induction\nheads (see Appendix E).", "metadata": {"type": "factual", "source": "2503.01329v1.pdf", "context": "itated by the OV pair, where positive eigenvalues suggest effective information propagation. Given\nour observations in Figure 2a and 2b of both positive eigenvalues in QK pair (supporting pattern\nmatching) and positive eigenvalues in the OV pair (enabling copying), it is highly probable that\ninduction heads occur in the last layer of DIFFEQFORMER .\nClustering behavior in DIFFEQFORMER Trained D IFFEQFORMER exhibits a distinct dynamic\nof QK and OV pairs characterized by increasing magnitudes of ei", "quality_score": 8.5}}
{"id": "test_46", "question": "What is the definition of the structure factor N(q, \u03c9) in the context of the generalized Kondo model?", "golden_answers": ["N(q, \u03c9) = 1 L \u2211\u2113 ei(\u2113\u2212L/2)q \u27e8\u27e8n\u2113nL/2\u27e9\u27e9\u2212\u03c9 , (A1)"], "prediction": "The structure factor N(q, \u03c9) in the context of the generalized Kondo model is defined as the charge structure factor that can be evaluated exactly for non-interacting spinless electrons, represented by the equation \u03c9Sff(q) = \u03c9ff(k1) \u2212 \u03c9ff(k2), where q = mod(k1 + k2, L), k1 > kF, and k2 < kF, and the free-fermion band \u03c9ff(q).", "f1": 0.11111111111111109, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01277v2\n4\nof the main text, i.e., for the generalized Kondo (gK)\nmodel with S = 1/2 localized spins, U/t = JH/t = 20,\nL = 200 sites, and Tz\ntot = 0 magnetization sector. Here\nn\u2113 = n\u2113\u2191 +n\u2113\u2193. In Fig. 17, we present N(q, \u03c9) for various\nelectron densities n. It is important to note that the total\nenergy span of N(q, \u03c9), and even the bottom of N(q, \u03c9),\nlies much above the spin excitations bandwidth WS. Our results indicate a perfect agreement between\nN(q, \u03c9) obtained within the gK model in the JH \u226b t\nlimit and the free-fermion solution. Specifically, for non-\ninteracting spinless electrons, one can evaluate the charge\nstructure factor N(q, \u03c9) exactly [117, 118]. Such calcula-\ntions are equivalent to the Stoner continuum of the form\n\u03c9Sff(q) = \u03c9ff(k1) \u2212 \u03c9ff(k2) , (A2)\nwhere q = mod ( k1 + k2, L), k1 > kF, and k2 < kF, and\nthe free-fermion band \u03c9ff(q), Eq. (6). Note that within\n\n20\n0\n1\n2\n3\n4\n5\nFrequency \u03c9/t 0 0 .01\n(a) N(q, \u03c9 ) n = 1.33 (b) N(q, \u03c9 ) n = 1.40 (c) N(q, \u03c9 ) n = 1.50\nWS\n0 0 .2 0 .4 0 .6 0 .8 1\nWavevector q/\u03c0\n0\n1\n2\n3\n4\n5\nFrequency \u03c9/t\n(d) N(q, \u03c9 ) n = 1.60\n0 0 .2 0 .4 0 .6 0 .8 1\nWavevector q/\u03c0\n(e) N(q, \u03c9 ) n = 1.66\n0 0 .2 0 .4 0 .6 0 .8 1\nWavevector q/\u03c0\n(f) N(q, \u03c9 ) n = 1.80\nFigure 17. Dynamical charge structure factor N(q, \u03c9) of the gK model with S = 1 /2 localized spin in the JH \u226b t limit\n(U/t = JH/t = 20, Tz\ntot = 0 magnetization sector) for various electron doping levels,n = 1.33 , 1.40 , 1.50 , 1.60 , 1.66 , 1.80 [panels\n(a) to (f), respectively]. The red dashed line represents the borders of the Stoner continuum calculated from noninteracting\nbands, Eq. (A2). The white dashed line in all panels represents the span of the spin excitations WS. See Fig. 5(a). In all\npanels: L = 200, \u03b4\u03c9/t = 5 \u00b7 10\u22123 and \u03b7 = 2\u03b4\u03c9. Stoner-like considerations, one of the bands in (A2) rep-\nresents \u03c3 =\u2191 electrons, while the second band represents\n\u03c3 =\u2193 electrons. The above perfect agreement between\nthe noninteracting solution \u03c9Sff(q) and full many-body\ncalculations of N(q, \u03c9) within the gK model (2) in the\nJH/t \u226b 1 limit indicates that the charge fluctuations are\nindifferent to the incoherent band of excitations.\nAppendix B: Hubbard and Hund interaction\ndependence\nIn Sec. IV A, we demonstrated that the Hubbard in-\nteraction U opens a small gap in the incoherent part of\nthe single-particle spectral function A(q, \u03c9). Here, we\nprovide a detailed analysis of this phenomenon. Further-\nmore, we present additional results of A(q, \u03c9) and the\ndynamical spin structure factor S(q, \u03c9) for various val-\nues of the Hubbard U and Hund JH interaction.", "metadata": {"type": "factual", "source": "2503.01277v1.pdf", "context": "structure factor defined as\nN(q, \u03c9) = 1\nL\nX\n\u2113\nei(\u2113\u2212L/2)q \u27e8\u27e8n\u2113nL/2\u27e9\u27e9\u2212\n\u03c9 , (A1)\nfor the system parameters discussed in Fig. 1 and Fig. 4\nof the main text, i.e., for the generalized Kondo (gK)\nmodel with S = 1/2 localized spins, U/t = JH/t = 20,\nL = 200 sites, and Tz\ntot = 0 magnetization sector. Here\nn\u2113 = n\u2113\u2191 +n\u2113\u2193. In Fig. 17, we present N(q, \u03c9) for various\nelectron densities n. It is important to note that the total\nenergy span of N(q, \u03c9), and even the bottom of N(q, \u03c9),\nlies much above the spin ", "quality_score": 8.5}}
{"id": "test_47", "question": "What type of controllers is shown in Fig. 2 for force-feedback-enabled teleoperation?", "golden_answers": ["Three types: trigger, roller, and parallel clip"], "prediction": "The type of controllers shown in Fig. 2 for force-feedback-enabled teleoperation is not specified in the provided text.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01301v1\nlow effort Unstable at travel limits\n(thumb tip only)\nTrigger Low effort, stable control Limited range of motion\nParallel Clip Stable control High effort, limited range\nof motion\nTABLE II: Comparison of Three Force Feedback Controllers\nas example) with force feedback can reduce the execution time\nby about 4 seconds in most of the tasks (Fig. 4). When using a controller without force feedback, the oper-\nator needs extra time to confirm they complete a successful\ngrasping when they are required to control the displacement\nof the gripper more precisely rather than simply let it close. While the linear force feedback, which is introduced in III,\nserves as a real-time supplement for the operator to adjust\ntheir movement on the controller. Moreover, we found that\neven simple 0-1 feedback which represents whether the gripper\nis in contact with objects can be important information for\noperators to improve efficiency. C. Discussion and Limitation\nThe value of the pipeline, simulation teleoperation demon-\nstration combined with real world data fine-tuning, is by using\na high-fidelity visual engine and force feedback teleoperation\ndevice, human operators can collect task data efficiently with-\nout the need for high-cost, large-scale hardware equipment\nand environment deployment. Only small number of real-\nworld demonstration is needed for fine-tuning to improve\nits performance in specific scenarios. While this method is\nverified to enable few-shot sim2real tranfer and reduces human\n\nlabor as well as resources needed for collecting large-scale\nreal-world demonstration, whether zero-shot transfer can be\nachieved with nearly photorealistic rendering is still an open\nquestion. In terms of force feedback, we argue that it is more\nimportant in simulation than in real teleoperation, since real\nworld contains more information like the visual deformation\nof real objects, the sound produced by contact, etc. which can\nhardly be simulated. So that human operators can infer the\nstate of interaction in real world while it\u2019s far more diffucult\nto do such inference in simulation environment. Besides, we\nobserved that incorporating a linear force transmission model\n(Algorithm 1) enhances the imitation learning policy\u2019s ability\nto handle soft and fragile objects more effectively. Algorithm 1 Contact Force Estimation\n1: for contact ci in simulation do\n2: ni \u2190 ci.geometry.name\n3: if ni = \u201dfinger left\u201d OR ni = \u201dfinger right\u201d then\n4: c.push(ci)\n5: end if\n6: end for\n7: if \u201dfinger left\u201d in c AND \u201dfinger right\u201d in c then\n8: qe0 \u2190 fingerpos.read ()\n9: end if\n10: F \u2190 k(qe \u2212 qe0) Besides, as we add grasping force as an addtional input\nof the model, which was mentioned in III, segmentation\ncharacteristics are shown in tasks. Take picking and placing\nas example, If the object is not grasped successfully at the\nexpected position, the model will stay in place and try back\nand forth instead of following the trajectory to try to place the\nobject in the box. Although this is also what is expected to\nbe understood from the wrist camera information, we believe\nthat the grasping force is a feature that is easier to learn here.", "metadata": {"type": "factual", "source": "2503.01301v1.pdf", "context": "Fig. 2: Three types of the single dof controllers for force-feedback-\nenabled teleoperation. The arrows annotate the motion direction of\nthe controllers\u2019 degree of freedom. Both the trigger and the roller are\nconcentric with the servo motor, while the parallel clip has a simple\nscissor structure to transform the force.\nwhere \u02c6Fe represents the force feedback applied to the operator.\nIn the experimental section, we further analyze the advantages\nand disadvantages of using these three different te", "quality_score": 8.5}}
{"id": "test_48", "question": "What is a necessary condition for providing a feasible solution to the D-MCLP problem?", "golden_answers": ["The attacker problem A-MCLP must be solved optimally for a given set X of facility locations."], "prediction": "Condition (4.4b) ensures that (\u00af\u03bd, \u00af\u03be) is a feasible solution to the primal problem.", "f1": 0.07692307692307691, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01042v2\nIf (\u00af\u03bd, \u00af\u03be, \u00af\u03c8) \u2208 M +(Rd) \u00d7 M+(T \u00d7\nRd \u00d7 A) \u00d7 C1,2\nb (T \u00d7 Rd) satis\ufb01es the following linear equations:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u222b\nRd\ng(x,\u00b5T)\u00af\u03bd(dx) +\n\u222b\nT\u00d7Rd\u00d7A\nf(t,x,a,\u00b5 t)\u00af\u03be(dt,dx,da) =\n\u222b\nRd\n\u00af\u03c8(0,x)\u03c1(dx),\n\u222b\nRd\n\u03c8(T,x)\u00af\u03bd(dx) \u2212\n\u222b\nRd\n\u03c8(0,x)\u03c1(dx)\n=\n\u222b\nT\u00d7Rd\u00d7A\n((\nL\u00b5 \u03c8\n)\n(t,x,a ) + ( \u2202t\u03c8)(t,x)\n)\n\u00af\u03be(dt,dx,da), \u2200\u03c8\u2208 W ,\ng(x,\u00b5T) \u2265 \u00af\u03c8(T,x), \u2200x\u2208 Rd,\n\u2202t \u00af\u03c8(t,x) +\n(\nL\u00b5 \u00af\u03c8\n)\n(t,x,a ) + f(t,x,a,\u00b5 t) \u2265 0, \u2200(t,x,a ) \u2208 T \u00d7 Rd \u00d7 A,\n(4.4a)\n(4.4b)\n(4.4c)\n(4.4d)\nthen (\u00af\u03bd, \u00af\u03be) \u2208 arg min(\u03bd,\u03be)\u2208DP (\u00b5 ) J\u00b5\nP(\u03bd,\u03be) and \u00af\u03c8\u2208 arg max\u03c8\u2208DP \u2217 (\u00b5 ) J\u00b5\nP\u2217 (\u03c8). 11\n\nProposition 4.4 follows directly from Proposition 4.3: Condition ( 4.4b) ensures that (\u00af \u03bd, \u00af\u03be) is\na feasible solution to the primal problem, Conditions ( 4.4c) and ( 4.4d) guarantee that \u00af\u03c8 is a\nfeasible solution to the dual problem, and Condition ( 4.4a) implies (\u00af\u03bd, \u00af\u03be) and \u00af\u03c8 yield the same\nvalue, which ensures the optimality of (\u00af \u03bd, \u00af\u03be, \u00af\u03c8) due to the weak duality property. To provide a veri\ufb01cation theorem for the NEs of the MFG, obser ve that the consistency\ncondition in De\ufb01nition 2.1 can be enforced by replacing \u00b5 in Proposition 4.4 with the state\nmarginal law of \u00af\u03be. Consequently, we can obtain a veri\ufb01cation theorem for an NE of the MFG\nthrough solutions of a primal-dual system. Theorem 4.5 (Primal-dual formulation of MFG) . Suppose (H. 1) holds. Let \u00b5 \u2217 \u2208 P (Rd|T) be\nnarrowly continuous, \u03be\u2217 \u2208 M +(T \u00d7 Rd \u00d7 A), and \u03c8\u2217 \u2208 C1,2\nb (T \u00d7 Rd). If (\u00b5 \u2217,\u03be\u2217,\u03c8\u2217) satis\ufb01es the\nfollowing primal-dual system:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u222b\nRd\ng(x,\u00b5\u2217\nT)\u00b5\u2217\nT(dx) +\n\u222b\nT\u00d7Rd\u00d7A\nf(t,x,a,\u00b5 \u2217\nt)\u03be\u2217(dt,dx,da) =\n\u222b\nRd\n\u03c8\u2217(0,x)\u03c1(dx),\n\u222b\nRd\n\u03c8(T,x)\u00b5\u2217\nT(dx) \u2212\n\u222b\nRd\n\u03c8(0,x)\u03c1(dx)\n=\n\u222b\nT\u00d7Rd\u00d7A\n((\nL\u00b5 \u2217\n\u03c8\n)\n(t,x,a ) + ( \u2202t\u03c8)(t,x)\n)\n\u03be\u2217(dt,dx,da), \u2200\u03c8\u2208 W ,\ng(x,\u00b5\u2217\nT) \u2265 \u03c8\u2217(T,x), \u2200x\u2208 Rd,\n\u2202t\u03c8\u2217(t,x) +\n(\nL\u00b5 \u2217\n\u03c8\u2217)\n(t,x,a ) + f(t,x,a,\u00b5 \u2217\nt) \u2265 0, \u2200(t,x,a ) \u2208 T \u00d7 Rd \u00d7 A,\u222b\nT\u00d7Rd\n\u03c8(t,x)\u00b5\u2217\nt(dx)dt=\n\u222b\nT\u00d7Rd\u00d7A\n\u03c8(t,x)\u03be\u2217(dt,dx,da), \u2200\u03c8\u2208 W ,\n(4.5a)\n(4.5b)\n(4.5c)\n(4.5d)\n(4.5e)\nthen \u03c8\u2217 \u2208 arg max\u03c8\u2208DP \u2217 (\u00b5 \u2217 ) J\u00b5 \u2217\nP\u2217 (\u03c8), and there exists a process X\u2217 such that (\u00b5 \u2217,X\u2217,\u03b3\u2217) \u2208\nP(Rd|T) \u00d7 A cl(\u00b5 \u2217) is an NE, with \u03b3\u2217 \u2208 P (A|T \u00d7 Rd) being the disintegration kernel of \u03be\u2217 in\nLemma\n3.2. Remark 4.2.", "metadata": {"type": "factual", "source": "2503.01350v1.pdf", "context": "4 Matheuristics 14\n4 Matheuristics\nThe proposed heuristic approach is based on generating feasible solutions. In this sense, it\nis worth mentioning that to provide a feasible solution of D-MCLP, the attacker problem\nA-MCLP must be solved optimally for a given set X of facility locations. Otherwise, the\nsolution may not be feasible. However, as the next lemma shows, finding the optimal edge\nlength increases for a given X is NP-hard.\nLemma 3 Given a setX of facility locations, solving A-MCLP is NP", "quality_score": 8.5}}
{"id": "test_49", "question": "Why is it impractical to find the optimal edge length increases for a given X in the context of solving A-MCLP?", "golden_answers": ["Because finding the optimal edge length increases for a given X is NP-hard, as shown in Lemma 3."], "prediction": "Finding the optimal edge length increases for a given X is impractical because it is NP-hard, even on star networks, assuming uniformity in downgrading costs and upper bounds as well as integrality of the input parameter values.", "f1": 0.52, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01350v1\nTo achieve this, we follow the steps outlined below:\nIdentification of redundant edges: When solving the D-MCLP, any edges e \u2208 E in\nthe original network with lengths exceeding R can be safely removed without im-\npacting the optimal solution. Consequently, their associated costs and upper bounds\ncan also be removed, i.e., ce and ue. Tightening downgrading upper bounds ( u): The attacker\u2019s objective is to compro-\nmise the leader\u2019s coverage. Thus, when the distance from the facility to the client\nequals R, the attacker achieves its goal. Consequently, if there exists an edge e \u2208 E\nwhose length plus the upper bound exceeds R, i.e., \u2113e + ue \u2265 R the upper bound\ncan be tightened as follows:\nue = R \u2212 \u2113e.\nTherefore, removing all these edges eliminates numerous constraints from the formu-\nlation. Additionally, reducing the upper bounds of the \u03b3 variables improves the bounds\nof the formulation. In the following, unless stated otherwise, we refer to BL-D-MCLP as\nthe bilevel formulation where these enhancements have been applied. As reported in Section 5, this strategy is useful for solving exactly our problem on the\ngeneral purpose solver for bilevel programming (Fischetti et al., 2017), since it notably\nreduces the number of constraints. Furthermore, it also benefits the matheuristic that is\nintroduced in the following section. 4 Matheuristics 14\n4 Matheuristics\nThe proposed heuristic approach is based on generating feasible solutions. In this sense, it\nis worth mentioning that to provide a feasible solution of D-MCLP, the attacker problem\nA-MCLP must be solved optimally for a given set X of facility locations. Otherwise, the\nsolution may not be feasible. However, as the next lemma shows, finding the optimal edge\nlength increases for a given X is NP-hard. Lemma 3 Given a setX of facility locations, solving A-MCLP is NP-hard even on star\nnetworks for non-uniform weights assuming uniformity in downgrading costs and upper\nbounds as well as integrality of the input parameter values. Proof. See Appendix A. Therefore, we first focus on improving the formulation of the attacker problem. Let a\nset X of facility locations be given. Then, we define the set V := {i \u2208 V : \u03c1i = 1}, i.e., the\ncovered nodes in the original network. Note that \u03c1i can be computed easily for a given\nX, e.g. using Dijkstra\u2019s algorithm starting from each x \u2208 X. Below, for any m \u2208 V , we\ndefine some sets that can be used to reduce the number of constraints of the formulation. V m := {i \u2208 V : d(i, m) < R},\nA[V m] := {(k, l) \u2208 A : k, l\u2208 V m}. The first set includes the nodes that can be covered by m (before downgrading) while the\nsecond comprises the arcs that can be used for this covering. For the ease of exposition,\nwe rewrite the formulation for the middle level problem:\nmax\nX\ni\u2208V\nwi\u03b7i\ns.t. (19), (23) ,\nR\u03b7i \u2264 \u03c0i, i \u2208 V , (40)", "metadata": {"type": "reasoning", "source": "2503.01350v1.pdf", "context": "4 Matheuristics 14\n4 Matheuristics\nThe proposed heuristic approach is based on generating feasible solutions. In this sense, it\nis worth mentioning that to provide a feasible solution of D-MCLP, the attacker problem\nA-MCLP must be solved optimally for a given set X of facility locations. Otherwise, the\nsolution may not be feasible. However, as the next lemma shows, finding the optimal edge\nlength increases for a given X is NP-hard.\nLemma 3 Given a setX of facility locations, solving A-MCLP is NP", "quality_score": 8.5}}
{"id": "test_50", "question": "What is the purpose of using t-SNE in Figure 4?", "golden_answers": ["to visualize the representations of input sentences and analyze the impact of DFT on aligning cross-lingual representations"], "prediction": "The purpose of using t-SNE in Figure 4 is to visualize the feature space of image pairs, illustrating improved clustering and alignment of tactile features across domains before and after applying the M2M model, demonstrating its effectiveness in reducing gaps.", "f1": 0.1923076923076923, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01058v1\nAdditionally, we visualize the feature space of image pairs using t-SNE\n(Fig. 3e). While original image pairs from any combination show clear separation,\npost-translation pairs align closely in latent space, indicating that generated images\nbecome effectively indistinguishable from their target domain counterparts. Additional\nvisualized image pairs are available in Extended Data Figure. Homogeneous tactile force sensation\nRobot hands equipped with homogeneous tactile sensors have been extensively studied\nin both the research community [63, 64] and commercial products [65]. In this context,\nwe evaluate our proposed model for transferring force sensation across homogeneous\ntactile sensors, particularly GelSight sensors with varying configurations. We fabri-\ncate three GelSight sensors with different illumination and marker patterns ( Array-I,\nCircle-I, Diamond-I ) and introduce elastomer shifts to replicate long-term usage by\nmoving or rotating Array-I and Circle-I, resulting in Array-II and Circle-II. The real-\nworld data collection setup is shown in Fig. 4a, and the fabrication process for silicone\nelastomers with different marker patterns is illustrated in Fig. 4c. The detailed data\ncollection trajectory is described in the Methods. Fig. 4b demonstrates the movement\nof a single contact point, covering both force increase and decrease phases in the normal\nand shear directions. For the experiments, we capture sequential tactile images, force\n9\n\nFig. 3 Marker-to-marker translation in simulation . a, The pretrained M2M translation model\nis developed through three key steps: arbitrary marker deformation simulation, curation of a marker\ndeformation simulation dataset, and pretraining of the M2M translation model. b, The pipeline (i-\nvii) for simulating marker deformation generates synthetic datasets with arbitrary deformed markers\nthat are used to pre-train and evaluate the M2M model. c-d, Quantitative evaluation of the marker-\nto-marker translation is presented using heatmaps of FID and KID (d), comparing performance\nbefore (i) and after (ii) applying the M2M model on a simulation dataset comprising 132 different\ncombinations. e, Feature space visualization using t-SNE illustrates improved clustering and alignment\nof tactile features across domains before (i) and after (ii) applying the M2M model, demonstrating its\neffectiveness in reducing gaps. Notably, the row labels denote S while column labels denote T in c-e. 10\n\nlabels, and contact poses. In addition to collecting data using a 3D support mounted\non a robot arm (Fig. 4a) for Array-II, Circle-II, and Diamond-I, we also collect data\nusing a simpler method by grasping the indenters with a ROBOTIQ two-finger grip-\nper to compare the effects of installation methods on force prediction. The collected\nforce range covers normal forces from 0 N to 18 N and shear forces from -4 N to 4 N. To generate marker-based binary tactile images from RGB images, we utilize\nmarker segmentation (see Methods), with the resulting marker images shown in\nFig. 4d. Approximately 180,000 force-image pairs are collected per sensor for the homo-\ngeneous test, in which more than 30 tactile images are continuously collected at each\ncontact point. For fine-tuning, we use only four contact images (the final images during\nthe force increase and decrease phases), resulting in 17,280 location-paired images per\nsensor, which are ten folds less compared with force-image pairs.", "metadata": {"type": "factual", "source": "2503.01275v1.pdf", "context": "\u4e3b\u5b9e\u9a8c\n(a)  Before DFT (b)  After DFT\nFigure 4: t-SNE visualizations of sentence representations\nfrom FLORES-200 dataset by LLaMA-2 before and after\napplying DFT.\nAnalysis of Representation Alignment\nWe used the t-SNE (Van der Maaten and Hinton 2008)\nmethod to visualize the representations of input sentences\nto analyze the impact of DFT on aligning cross-lingual rep-\nresentations.\nSpecifically, we encoded parallel English and Chinese\nsentences from the FLORES-200 dataset and obtain sen-\ntence repre", "quality_score": 8.5}}
{"id": "test_51", "question": "How do the layer sparsity settings of EauDeDQN compare to those of PolyPruneDQN?", "golden_answers": ["The layer sparsity settings of EauDeDQN and PolyPruneDQN are identical, both having 0.5, 0.8 for each layer."], "prediction": "EauDeDQN adapts the sparsity schedule to the training length and achieves higher final sparsity levels for larger networks compared to PolyPruneDQN, which does not show the same adaptability and may suffer from overfitting in larger networks.", "f1": 0.20408163265306123, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01437v1\nEauDeDQN EauDeCQL EauDeSAC\nvs PolyPruneDQN vs PolyPruneCQL vs PolyPruneSAC\nTraining time \u00d71, 39 \u00d71, 17 \u00d71, 08\nGPU vRAM usage +0, 73 Gb +0, 65 Gb +0, 01 Gb\nFLOPs for a gradient update \u00d71.55 \u00d71.55 \u00d74.03\nFLOPs for sampling an action \u00d71.01 (offline) \u00d71.00\n\nCover Page\n1 20 40 60 80\nNumber of Frames (in millions)\n0.00\n0.25\n0.50\n0.75\n1.00Sparsity Schedule 0.7 0.8 0.9 1.0\nFinal Sparsity Level\nBeamRiderMsPacmanQbertPongEnduroSpaceInvadersAssaultCrazyClimberBoxingVideoPinball\nPolyPruneDQN DQN\nShort Training: Small network Medium network Long Training: Small network\n0.0 0.6 1.2 1.8 2.4 3.0\nEnvironment Steps (in millions) 0.00\n0.25\n0.50\n0.75\n1.00Sparsity Schedule 0.4 0.6 0.8 1.0\nFinal Sparsity Level\nHopper\nAnt\nHalfCheetah\nWalker2d\nHumanoid\nHumanoidStandup\nPolyPruneSAC SAC\nShort Training: Small network Medium network Long Training: Small network\nFigure 11: EauDeDQN (top) and EauDeSAC (bottom) adapt the sparsity schedule to the training\nlength. For small networks, increasing the training length leads to higher final sparsity levels (blue\nand green curves), except for the games Pong, SpaceInvaders, and CrazyClimber. Similarly to\nFigure 5 and 8, larger networks are pruned at a higher final sparsity level (blue and orange curves),\nwith an exception for Pong and Humanoid.\n0 10 20 30 40 50\n0.5\n1\n2\nCumulated loss 0 10 20 30 40 50\n0.00\n0.25\n0.50\n0.75\n1.00Sparsity Schedule\nNumber of Batch Updates (\u00d7 62.5K)\nEauDeCQL PolyPruneCQL CQL   | Small network Large network\nFigure 12: Left: In the offline setting, the larger networks suffer from overfitting as the cumulated\nlosses (reported at every target update and averaged over T updates) increase over time. Right:\nEauDeCQL adapts the sparsity schedule to the network size. Indeed, sparsity levels are lower for\nthe small network towards the end of the training.\n0.6 0.7 0.8 0.9 1.0\nFinal Sparsity Level\n0.00\n0.25\n0.50\n0.75\n1.00\nIQM SAC \nNorm Score\nPolyPruneSAC\nEauDeSAC's Umax (regime)\n3 10 30 100\n3 5 10\n0.01\n0.05\n0.1\nSmax\nIQM SAC\nNorm Score\n3 5 10\nFinal Sparsity\nLevel\n0.0\n0.5\n1.0\nPopulation size (K)\nFigure 13: We evaluate EauDeSAC on 6 MuJoCo environments for 1M with the small network. Left: PolyPruneSAC suffers when its final sparsity level is too high as opposed to EauDeSAC,\nwhich yields high returns for all considered values of Umax. Right: EauDeSAC remains stable\nacross different values of Smax and population size K (see Equation 3), showcasing its robustness\nw.r.t. hyperparameter changes. The number of subsampled networks M is set to\n\u0006K\n2\n\u0007\n. The default\nhyperparameters of EauDeQN are indicated with a white cross. Cover Page\n1 10 20 30 400.00\n0.25\n0.50\n0.75\n1.00\nSparsity\nSchedule\nNormal Regime\n1 10 20 30 40\nNumber of Frames (in millions)\nAmbitious Regime\n1 10 20 30 40\nAggressive Regime\nEauDeDQN Small network EauDeDQN Large network PolyPruneDQN\n0.0 0.5 1.00.00\n0.25\n0.50\n0.75\n1.00\nSparsity\nSchedule\nNormal Regime\n0.0 0.5 1.0\nEnvironment Steps (in millions)\nAmbitious Regime\n0.0 0.5 1.0\nAggressive Regime\nEauDeSAC Small network EauDeSAC Large network PolyPruneSAC\nFigure 14:", "metadata": {"type": "reasoning", "source": "2503.01437v2.pdf", "context": "Eau De Q-Network\nSupplementary Materials\nThe following content was not necessarily subject to peer review.\n0\n5\nVideoPinball\nIQM Return\n0\n5\nBoxing\n0\n2\nCrazyClimber\n0\n2\nAssault\n0.0\n0.5\nSpaceInvaders\n0\n1\nEnduro\n0\n1\nPong\n0.0\n0.2\nQbert\n0.1\n0.2\n0.3\nMsPacman\n1 10 20 30 40\n0.00\n0.25BeamRider\n0.5\n0.8\nGlobal Sparsity EauDeDQN \nLayer Sparsity\nPolyPruneDQN \nLayer Sparsity\n0.5\n0.8\n0.5\n0.8\n0.5\n0.8\n0.5\n0.8\n0.5\n0.8\n0.5\n0.8\n0.5\n0.8\n0.5\n0.8\n1 10 20 30 40\n0.5\n0.8\n1 10 20 30 401 10 20 30 40\nNumber of Frames (in mil", "quality_score": 8.25}}
{"id": "test_52", "question": "Why are graph neural networks (GNNs) particularly well suited for soybean disease detection tasks?", "golden_answers": ["GNNs can model relational dependencies between samples, enabling context-aware predictions by aggregating information from neighboring images."], "prediction": "Graph neural networks (GNNs) are particularly well suited for soybean disease detection tasks because they can model relational dependencies between samples, allowing them to leverage relationships between images, such as symptom similarity or shared environmental conditions, which provide valuable diagnostic cues. By treating images as nodes and defining edges based on pairwise similarities, GNNs aggregate information from neighboring images, enabling context-aware classification that incorporates both local features and global relational patterns.", "f1": 0.3218390804597701, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01284v3\nNumerous studies have high-\nlighted their effectiveness in soybean leaf disease classifica-\ntion [1]\u2013[9]. However, most existing approaches \u2014 whether\nCNNs or transfer learning techniques are used [10], [11]\n\u2014 focus on extracting features from individual images,\noverlooking critical relational information between images. This becomes particularly problematic when diseases present\nvisually similar symptoms triggered by different factors,\nsuch as nutrient deficiencies, pest damage, or environmental\nstress, often leading to misclassifications. Moreover, these\nconventional models offer limited explainability, providing\nlittle insight into which leaf regions drive predictions and\nreducing interpretability and trust among agricultural experts. To address these limitations, graph neural networks\n(GNNs) have emerged as a complementary approach capa-\nble of modeling relational dependencies between samples. GNNs are particularly well suited for cases where rela-\ntionships between images, such as symptom similarity or\nshared environmental conditions, provide valuable diagnostic\ncues [12], [13]. By treating images as nodes and defining\nedges on the basis of pairwise similarities, GNNs aggre-\ngate information from neighboring images, enabling context-\naware classification incorporating local features and global\nrelational patterns. However, GNNs alone lack the ability\nto extract fine-grained spatial features directly from raw\nimages \u2014 a key strength of CNNs. Therefore, combining\nCNNs and GNNs into a hybrid framework offers a synergistic\nadvantage: CNNs capture localized spatial features within\nindividual images, whereas GNNs enrich these represen-\ntations with relational context across images. This hybrid\napproach is particularly valuable for soybean leaf disease\nclassification, where local lesion characteristics and broader\nsymptom similarity across fields, varieties, and conditions are\nessential for accurate and interpretable diagnosis. To address these gaps, we propose an interpretable hybrid\nsequential CNN-GNN architecture that sequentially com-\nbines MobileNetV2 for efficient spatial feature extraction and\ngraph sample and aggregation (GraphSAGE), a GNN archi-\ntecture, for relational dependency modeling between soybean\nleaf images. By constructing a similarity graph where nodes\nrepresent leaf images and edges encode pairwise feature sim-\nilarity, GraphSage [14] aggregates information from neigh-\nboring nodes, enriching the feature representations with a\nrelational context. This fusion of local spatial learning and\nglobal relational learning enhances classification accuracy\nwhile ensuring computational efficiency, making the model\nsuitable for real-time field deployment. Additionally, we\nincorporate Grad-CAM and Eigen-CAM visualizations to\nprovide interpretable heatmaps that highlight the specific leaf\nregions influencing each classification decision, bridging the\ngap between model predictions and expert validation. To the\nbest of our knowledge, this is the first interpretable CNN-\nGNN hybrid framework applied to soybean leaf disease de-\ntection, addressing critical gaps in relational modeling, model\ntransparency, and computational efficiency in plant disease\nclassification research. We make the following key contributions in this work:\n1) Sequential CNN-GNN Architecture:We propose a\nnovel pipeline combining a pretrained MobileNetV2\nfor local feature extraction and a GraphSAGE model\nfor global relational reasoning, enhancing our model\u2019s\nability to capture fine-grained disease symptoms and\ninter-symptom dependencies.", "metadata": {"type": "reasoning", "source": "2503.01284v3.pdf", "context": "Jahin et al.: Soybean Disease Detection via Interpretable Hybrid CNN-GNN\nmance in image classification tasks by automatically learn-\ning spatial features from raw images, eliminating the need\nfor manual feature extraction. Numerous studies have high-\nlighted their effectiveness in soybean leaf disease classifica-\ntion [1]\u2013[9]. However, most existing approaches \u2014 whether\nCNNs or transfer learning techniques are used [10], [11]\n\u2014 focus on extracting features from individual images,\noverlooking cri", "quality_score": 8.25}}
{"id": "test_53", "question": "Why does PVU need to develop a special alignment module for dot product operations?", "golden_answers": ["Because the alignment must be applied to all elements within the intermediate vector, whereas in addition/subtraction modules, it is performed only for pairs of operands."], "prediction": "PVU needs to develop a special alignment module for dot product operations because the alignment must be applied to all elements within the intermediate vector, whereas in the addition/subtraction module, the alignment operation is performed for two operands as a pair.", "f1": 0.6206896551724138, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01313v1\nBefore per-\nforming the accumulation, PVU need to align all the elements\nwithin the intermediate variable. In the addition/subtraction\nmodule, the alignment operation is performed for two operands\nas a pair, while in dot product operations, the alignment must\nbe applied to all elements within the intermediate vector. Therefore, PVU have developed an alignment module specifi-\ncally adapted for dot product operations, with an internal im-\nplementation logic identical to that of the addition/subtraction\nalignment module. After the alignment is completed, PVU convert all the\nmantissas to two\u2019s complement format based on the sign bits,\nand reuse the CSA for accumulation. Since the CSA operates\nwith a larger bit width during computation and performs\na rounding operation after all calculations are completed,\nthis significantly improves the precision of the dot product\ncomputation. F . Standardization\nAs mentioned earlier, the Posit mantissa has an implicit\nbit. Before encoding the final result into Posit format, PVU\nneed to normalize all the mantissas in the PIR. PVU treat\nthe highest bit of the actual mantissa as the implicit bit and\nperform normalization to ensure that this bit is always 1.\nPVU first invoke the LZC module to calculate the number\nof leading zeros in the mantissa. Then, using the number of\nleading zeros and the configured decimal point position, PVU\ncalculate the adjustment required for the exponent. Finally,\nPVU use a barrel shifter to adjust the exponent so that the\nvalue before the decimal point is 1. Lastly, PVU save the mantissa bit width plus 1 bit, perform\nRNE rounding on the lower bits, and adjust the exponent\nvalues in the PIR one by one. It is important to note that the\noutput of the dot product operation is a scalar, while the output\nof other operations is a vector. Therefore, the normalization\nmodule has both scalar and vector versions, but the internal\nlogic is the same. G. Encode\nAfter the computation is completed, PVU need to encode\nthe various parts of the PIR into the specified Posit format. In fact, encoding is the inverse process of decoding, and\nPVU can handle it using the reverse process of decoding. The\nchallenge lies in extracting the Regime part, as its dynamic bit\nwidth property means that the bit width of the Regime part\nafter computation is unknown. Therefore, PVU first extract\nthe Exponent part, which has a fixed bit width, and then\nextract the Regime part. In the Regime value computation\npart, to maintain the inversion bit property, PVU initialize the\nRegime value as 1 (00...01), then determine whether to invert it\nbased on its sign. Finally, PVU calculate the Regime bit width\ninversely using the formula (1), completing the encoding of\nthe Regime part. Finally, PVU perform RNE rounding on the mantissa,\nconcatenate the three parts, and convert them into two\u2019s\ncomplement representation to complete the Posit encoding. Note that since the result of the dot product operation is a\nscalar, PVU have also written a scalar encoding unit, which\nshares the same internal logic as the vector encoding unit.", "metadata": {"type": "reasoning", "source": "2503.01313v1.pdf", "context": "Equation (6), PVU decompose the dot product calculation into\ntwo parts. The first part is the vector multiplication operation,\nwhere the result is stored in an intermediate vector. The second\npart is the accumulation operation, where the elements of the\nintermediate vector are summed to obtain the final dot product\nresult.\nFirst, PVU reuse the existing vector multiplication module,\nand store the result in an intermediate variable. Before per-\nforming the accumulation, PVU need to align all the e", "quality_score": 8.25}}
{"id": "test_54", "question": "Why would a sample of old pulsars be biased toward low kick velocities?", "golden_answers": ["Because their Galactic trajectories become more eccentric as a result of the kicks, making them more likely to be observed near their Galactic apocentre where they have reduced speeds relative to their initial velocities. Additionally, NSs that receive high kicks migrate outwards more quickly and therefore become less likely to be observed as they age."], "prediction": "A sample of old pulsars would be biased toward low kick velocities because high kicks displace neutron stars (NSs) to relatively large offsets, making them less likely to be observed after a certain amount of time. This introduces a selection effect, as pulsars that receive large kicks and escape the Galaxy are not observable, leading to a predominance of pulsars with lower kick velocities in the sample.", "f1": 0.3684210526315789, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01429v1\nWe note that the oldest pulsar in the samples has a char-\nacteristic age of \u03c4c = 575 Myr, but we set the range of the oldest\nbin to 1 Gyr for consistency with the simulation introduced in\nSect. 3. The eccentricity distributions shown in Fig. 3 peak at\n\u02dce \u22481 for the young pulsars. For older pulsars this peak starts\nto shift to lower eccentricities, in fact the oldest pulsars peak at\n\u02dce \u22480. The small eccentricities indicate that if older pulsars were\nobserved to have small velocities, then these cannot be explained\nby the deceleration due to galactic drift (as discussed by Hansen\n& Phinney 1997; Disberg et al. 2024a) because this would re-\nquire eccentric orbits. This means that the shift is likely due to\nthe fact that high kicks displace NSs to relatively large o ffsets,\nmaking them less likely to be observed after a certain amount\nof time and therefore introducing a selection effect (as discussed\nby Cordes 1986; Lyne & Lorimer 1994). Besides this general\nshift to lower values of \u02dce, the eccentricity distributions encode\nthe natal NS kicks (see e.g. the peak at \u02dc e \u2248 0.45 for the old-\nest pulsars). Lastly, we note that applying the method of Disberg\net al. (2024b) to these pulsars will result in more accurate kick\nestimates for older pulsars (i.e. \u03c4c \u2273 10 Myr) compared to meth-\nods that only estimate their current velocities (e.g. Hobbs et al.\n2005; Verbunt et al. 2017; Igoshev 2020). At the cost of hav-\ning to consider selection e ffects (such as NSs that receive large\nkicks and escape the Galaxy not being observable after a certain\namount of time), this method e ffectively expands the sample of\npulsars with relatively accurate kick estimates, making it more\nrobust against Poisson noise. 3. Method\nHaving evaluated the eccentricities of the pulsars\u2019 trajectories,\nwe employed the method of Disberg et al. (2024b) to estimate\ntheir kicks. In order to do this, we expanded their simulation\nwhich determines the relationship between Galactic eccentric-\nity and kick magnitude (Sect. 3.1). Then, we discussed how we\ninferred kicks based on the simulation and the eccentricity esti-\nmates (Sect. 3.2). 3.1. Simulation In order to relate the estimated eccentricities to kicks, we simu-\nlated populations of objects receiving kicks with di fferent mag-\nnitudes and analysed the eccentricities of the resulting orbits. For\neach simulation, we seeded 103 objects in a Gaussian annulus at\nz = 0 kpc, described by:\nP(R) \u221dexp\n \n\u2212(R \u2212Rd)2\n2\u03c3d2\n!\n. (7)\nFaucher-Gigu\u00e8re & Kaspi (2006) first proposed this distribution\nfor pulsars, which was fitted by Sartore et al. (2010) to the pulsar\ndistribution of Yusifov & K\u00fc\u00e7\u00fck (2004) resulting in Rd = 7.04\nkpc and \u03c3d = 1.83 kpc. We note that a completely different ini-\ntial distribution (i.e. an exponential disc) likely results in similar\nkick estimates (Disberg et al. 2024b).", "metadata": {"type": "reasoning", "source": "2503.01429v2.pdf", "context": "sequence masses leading to NS formation (e.g. Ertl et al. 2016;\nM\u00fcller et al. 2016; Kresse et al. 2021). In particular, Burrows\net al. (2024) find two classes of progenitors: low mass and low\ncompactness that lead to kicks of\u223c100\u2212200 km/s and high mass\nand high compactness that lead to kicks of \u223c300\u22121000 km /s.\nThese two classes could hypothetically correspond to the two\nMaxwellians found by Verbunt et al. (2017) and Igoshev (2020).\nThe methods of Hobbs et al. (2005), Verbunt et al. (2017),\nand ", "quality_score": 8.25}}
{"id": "test_55", "question": "Why does the background in the image appear to be a light gray color?", "golden_answers": ["To ensure that the focus remains solely on the woman and to accentuate the details of her face and hair."], "prediction": "The background is a solid light gray color to ensure that the focus remains solely on the woman.", "f1": 0.5161290322580646, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01261v2\nThe background is a solid \nlight gray color, which ensures that the focus remains solely on the \nwoman. The simplicity of the background further accentuates the \ndetails of the woman's face and hair. The image does not contain any \ntext or other discernible objects. The relative position of the woman to \nthe background suggests she is the main subject of this image. The \nimage does not provide any information about the location or setting. The image is a portrait, focusing on the woman's face and upper body.\nOrigin Caption\nShe has arched eyebrows. She is smiling, and young and is wearing \nlipstick. This woman is wearing heavy makeup. She has wavy hair, and \nmouth slightly open. She has mouth slightly open, arched eyebrows, and wavy hair and is \nwearing heavy makeup. She is smiling. This person is attractive and has blond hair, mouth slightly open, and \narched eyebrows. This person has arched eyebrows, wavy hair, and mouth slightly \nopen. She wears lipstick. She is attractive. Long Text\nIn the image, there's an older man who is the main subject. He has a \nbald head and is wearing glasses. His chin is resting on his hand, \nsuggesting a moment of contemplation or deep thought. He is dressed \nin a beige jacket, which adds a professional or formal tone to the image. The background is a blurred gray color, which puts the focus entirely \non the man. There are no other discernible objects or texts in the image. The man's position relative to the background suggests he is standing \nin front of it. The image does not provide any information about the \nlocation or setting. It's a simple yet intriguing portrait of an older man, \ncaptured in a moment of quiet reflection. This person has receding hairline. The man has narrow eyes, receding hairline, and eyeglasses. This man has narrow eyes, big nose, receding hairline, and gray hair. The man has big nose, eyeglasses, and narrow eyes. The person has receding hairline. The image captures a close-up of a woman's face, her gaze directed \nstraight at the camera. Her hair, a vibrant shade of blonde, frames her \nface, adding a touch of warmth to the overall composition. Her eyes, a \nstriking shade of blue, stand out against her complexion, and her lips, \npainted a soft pink, add a subtle contrast. The background, though \nblurred, gives the impression of a room with a window, suggesting an \nindoor setting. The focus on the woman's face and the blurred \nbackground create a depth of field effect, drawing attention to her \nexpressive features. There are no discernible texts or other objects in the \nimage. The relative position of the woman to the background suggests \nshe is in the foreground of the scene. The image does not provide any \ninformation about the actions of the objects or their precise locations. The image is devoid of any aesthetic descriptions, focusing solely on the \nfactual elements present. She is wearing lipstick. She is young and has brown hair, and wavy \nhair. This woman is wearing lipstick. She has arched eyebrows.", "metadata": {"type": "reasoning", "source": "2503.01261v2.pdf", "context": "The image captures a close-up of a young woman with striking blue \neyes and blonde hair. Her hair, styled in loose waves, cascades down \nher shoulders, adding a touch of elegance to her appearance. She is \nwearing a white tank top, which contrasts beautifully with her blonde \nhair. Her gaze is directed straight at the camera, creating a sense of \nconnection with the viewer. A slight smile graces her face, adding a \nwarm and friendly aura to the overall image. The background is a solid \nlight gra", "quality_score": 8.25}}
{"id": "test_56", "question": "What is the definition of gs, as defined in the text?", "golden_answers": ["{X \u2208 g : Ad(ets)(X) is bounded ast \u2192 \u221e}"], "prediction": "GSP is defined as a novel technique called Geo-Semantic-Parsing, which is able to achieve state-of-the-art results by adopting machine learning and artificial intelligence (AI) techniques to extract geographic information from semantic knowledge graphs.", "f1": 0.04878048780487805, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01386v1\nIn addition, also other important tasks such as the monitoring of epidemics [23]\nand crime prevention [24, 25, 26] would benefit from the availability of additional geotagged OSN\ncontent, let alone those situations in which geographic information is relevant per se, such as in\ndemographic studies [27]. Given the great importance of geotagged data for DSS, much effort has been recently devoted\nto tasks such as geotagging and geoparsing [28, 29]. In detail, geotagging is defined as the generic\ntask of associating geographic coordinates to a given document or to a portion of a document (e.g.,\na token). Instead, geoparsing is a more complex task that can be used to perform geotagging and\nthat involves parsing a text, identifying location mentions and complementing them with their corre-\nsponding geographic coordinates [30]. There exists also other approaches to geotagging that are not\nnecessarily based on free text analysis, such as those based on OSN account information [31] or on\nsocial relationships [32]. In this work, we focus on the geoparsing task, and we propose a novel technique called Geo-\nSemantic-Parsing (GSP). GSP is able to achieve state-of-the-art results by adopting machine learning and\nartificial intelligence (AI) techniques to extract geographic information from the rich data contained in\nsemantic knowledge graphs, such asDBpedia and GeoNames. In particular, in a first step GSP leverages\na semantic annotator to identify relevant portions of the input text (i.e., the document to geoparse) and\nto link them to pertinent entities in a reference knowledge graph. Then, it exploits several different\nstrategies to traverse the knowledge graph by navigating links between entities. The result of this\nsecond step is an expanded set of candidate entities, that are likely to contain relevant geographic\ninformation for the task. Finally, among the geographic information of all retrieved entities, we select\nthose with which to geotag the input document by means of a regression model, that we trained on\nlabeled data. The combination of powerful AI techniques and the rich, structured, interconnected data\ncontained in multiple knowledge graphs allowsGSP to achieve F1 = 0.66, whereas other state-of-the-art\ntechniques and baselines obtain F1 \u2264 0.55. More in detail, one of the reasons why our solution achieves unprecedented results is because it\nmitigates the problem of toponymic polysemy \u2013 that is, the fact that the same toponym can refer to\ndifferent places according to the context in which it appears 1. The majority of traditional geoparsing\ntechniques adopt heuristics to disambiguate toponyms matched in a gazetteer, a solution that might\nprove ineffective, especially at world-scale. As a consequence, the application of such techniques is often\nconstrained to geographically-limited areas, in order to achieve satisfactory performance [33]. Instead,\nGSP mitigates this issue by performing semantic annotation of the input text \u2013 an operation that\nintrinsically performs disambiguation of tokens based on their context. In addition, our experimental\nresults demonstrate that the expansion and selection steps of GSP also allow to correct some of the\nerrors made by the semantic annotator.", "metadata": {"type": "factual", "source": "2503.01289v1.pdf", "context": "g0\ns = {X \u2208 g : Ad(ets)(X) = X, \u2200t \u2208 R}, gs = {X \u2208 g : Ad(ets)(X) is bounded ast \u2192 \u221e},\nand the subgroups\nLs = {g \u2208 G : Ad(g)(s) = s}, P s = {g \u2208 G : etsge\u2212ts is bounded ast \u2192 \u221e}.\nThese subgroups ofG have g0\ns and gs, respectively, as Lie algebras.Ps is parabolic andLs is a\nLevi factor forPs. We also define the character\u03c7s : gs \u2192 C given by\u03c7s(x) = B(s, x), whereB\nis the Killing form ong.\nNow, for aG-bundle E and a reduction\u03c3 \u2208 H0(C, E(G/Ps)) of structure group toPs, we define\nthe degree of the re", "quality_score": 8.25}}
{"id": "test_57", "question": "How would a rapid rearrangement of magnetic fields in SGR 1935+2154 be accompanied?", "golden_answers": ["A rapid rearrangement would likely be accompanied by high-energy activity, such as crustal yielding leading to a local relaxation in the magnetic field."], "prediction": "A rapid rearrangement would likely be accompanied by high-energy activity, such as crustal yielding leading to a local relaxation in the magnetic field.", "f1": 1.0, "em": 1.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01409v2\nTwo of the components identified as important by Kojima\n(2024) for the failure problem, namely those in the r\u03b8 (left)\nand \u03b8\u03b8 \u2212 \u03d5\u03d5 (right) directions, are shown in Figure 6 for\nBp = 0.5b. Attention has been restricted to the \u2018crust\u2019 0 .9 \u2a7d\nr \u2a7d 1 and, for simplicity, only poloidal stresses are shown\n(i0 = 0). The ratios span the range 0 .87 \u2272 R \u2272 1.08 with\nthe lower and upper limits reached near polar ( \u03b8 \u2248 0, \u03c0) and\nequatorial (\u03b8 \u2248 \u03c0/2) colatitudes, respectively, at the base of\nthe crust ( r \u2248 0.9). The fact that R exceeds unity near the\nequator implies one may expect more frequent failures there\nrelative to the Maxwell case, with the reverse applying near\nthe poles, since the elastic stress should counterbalance the\nelectromagnetic stress. This is likely to skew the anticipated\nwaiting time distribution for flares to later times, as Hall\ndrift tends to tangle field lines near the pole and hence build\nstresses there (see figure 2 in Perna & Pons 2011). This will\nbe investigated in future work. 5.5 Magnetic (anti)-glitches\nAs first described by Ioka (2001), rapid changes to the\nquadrupolar ellipticity of a magnetar may bring about glitch-\nlike activity. By angular momentum conservation, a sudden\nreequilibriation \u03f5i \u2192 \u03f5f would lead to a fractional change in\nf of order\n\u03b4f f \u2248 2\n3 (\u03f5i \u2212 \u03f5f) . (49)\nWhile this model was discussed by Garcia & Ranea-Sandoval\n(2015) in the context of theanti-glitch seen in 1E 2259+586, a\npositive glitch is also possible if \u03f5i > \u03f5f (Mastrano, Suvorov &\nMelatos 2015). In the latter case, a dynamical decay (growth)\nin the poloidal (toroidal) sector could be expected. Such a\nrapid rearrangement would likely be accompanied by high-\nenergy activity; for instance, crustal yielding could lead to\na local relaxation in the magnetic field and hence a small\nchange in \u03f5. In the BI theory, maximum values for the toroidal elliptic-\nity also imply an upper limit to the size of a magnetically-\ninduced glitch. For SGR 1935+2154 with Bp \u2248 4 \u00d7 1014 G,\nresults from Tab. 4.1 indicate a maximum shift of \u2248 5\u00d710\u22126\nfor b = 1016 G. Using equation (49), this implies a maximum\nglitch magnitude of \u03b4f \u2248 (b/1016 G)2 \u00b5Hz for P = 3.25 s.\nMNRAS 000, 1\u201315 (?)\n\n12 Arthur G. Suvorov & Jos\u00b4 e A. Pons\nFigure 6.Dimensionless ratios of BI-to-Maxwell stresses (48) in the r\u03b8 (left) and \u03b8\u03b8 \u2212 \u03d5\u03d5 (right) directions within a \u2018crust\u2019 for the field\n(27), with redder shades indicating a greater ratio R. The classical polar strength is fixed as Bp = b/2. This is about half the size of the large glitch seen in this\nobject prior to the release of a fast radio burst in 2020 Oc-\ntober ( \u03b4f \u2248 1.8+0.7\n\u22120.5 \u00b5Hz; Younes et al. 2023) suggesting a\nfloor b \u2a7e\n\u221a\n2 \u00d7 1016 G, assuming magnetic rearrangement\nwas responsible for the burst and glitch. 6 DISCUSSION", "metadata": {"type": "reasoning", "source": "2503.01409v2.pdf", "context": "in the poloidal (toroidal) sector could be expected. Such a\nrapid rearrangement would likely be accompanied by high-\nenergy activity; for instance, crustal yielding could lead to\na local relaxation in the magnetic field and hence a small\nchange in \u03f5.\nIn the BI theory, maximum values for the toroidal elliptic-\nity also imply an upper limit to the size of a magnetically-\ninduced glitch. For SGR 1935+2154 with Bp \u2248 4 \u00d7 1014 G,\nresults from Tab. 4.1 indicate a maximum shift of \u2248 5\u00d710\u22126\nfor b = 1016 ", "quality_score": 8.25}}
{"id": "test_58", "question": "What is one of the tasks required of a UA V?", "golden_answers": ["The model must navigate to a designated gate associated with a notable figure."], "prediction": "To traverse a race track composed of multiple sequential gates.", "f1": 0.10526315789473685, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01378v1\nThroughout each flight,\nthe UA V\u2019s velocity and the head profile (Vx, Vy, Vz, omega)\nwas continuously logged, providing a comprehensive record\nof its dynamic behavior. The dataset was divided into training and test subsets,\nensuring an equitable distribution of samples across all three\ncategories in both sets. The training subset was employed\nto optimize the VLA model parameters, while the test\nsubset forms the foundation of the CognitiveDroneBench\nbenchmark for performance evaluation. B. Model Training\nThe VLA model was fine-tuned using our custom training\ndataset based on the OpenVLA architecture. The collected\ndata was structured in accordance with the Reinforcement\nLearning Dataset (RLDS) format to ensure seamless com-\npatibility with OpenVLA. This organization facilitated effi-\ncient management of actions, images, and task instructions,\nrendering the dataset suitable for both imitation learning and\ntask-specific action prediction. Subsequently, the dataset was\nutilized to fine-tune an OpenVLA-7b model employing a\nparameter-efficient Low-Rank Adaptation (LoRA) approach,\napplying rank-32 adapters to optimize memory usage while\nadjusting a minimal set of trainable weights. The training\nconfiguration consisted of a batch size of 64, a learning\nrate of 5 \u00d7 10\u22124, and 4000 gradient steps, with image\naugmentation disabled. Training was conducted on four NVIDIA A100 GPUs, al-\nlowing for efficient large-scale optimization while maintain-\ning real-time deployability on UA V hardware. Checkpoints\nwere saved at regular intervals, with evaluations performed\nevery 500 steps to monitor model performance. For a detailed\noverview of the training progression, please refer to the\nlearning curve in Figure 4. V. E VALUATION\nA. CognitiveDroneBench\nThe evaluation of cognitive capabilities in UA Vs poses\nunique challenges that are even more pronounced than\nthose encountered in robotic manipulators. To address these\nchallenges and to enable an objective comparison of VLA\nmodels for UA Vs, we developed an open-source simulation\nbenchmark named CognitiveDroneBench. As illustrated in\nFigure 1, the benchmark is implemented within a high-\nfidelity physical simulation environment that accurately repli-\ncates the dynamics and physics of UA V flight. In our benchmark, the drone is required to traverse a race\ntrack composed of multiple sequential gates. At each stage\nof the track, the drone receives a first-person view (FPV)\nimage along with a task-specific textual instruction. The core\nobjective is to select the correct gate by solving an embedded\ncognitive task, thereby generating a 4D action command that\ndictates its movement. The tasks are categorized into three\ndistinct types: Human Recognition , Symbol Understand-\ning, and Reasoning. For conventional VLA models, such as RaceVLA and the\nbase version of CognitiveDrone, the decision-making process\ninvolves directly processing the FPV image and associated\nprompt to determine the appropriate action. In contrast,\nthe CognitiveDrone-R1 variant incorporates an additional\nreasoning stage using a VLM. This VLM processes the task\ninstruction and visual data to disambiguate and simplify the\ndirective, effectively reducing the complexity of the task\nbefore passing it to the VLA model for high-frequency\ncontrol. Task performance is automatically validated: passing\nthrough the correct gate earns the drone 1 point.", "metadata": {"type": "factual", "source": "2503.01378v1.pdf", "context": "the individuals based on external characteristics specified\nwithin the textual prompt. Additionally, the UA V is tasked\nwith navigating to a designated gate associated with a notable\nfigure.\nSymbol Understanding: The model is required to differ-\nentiate between a variety of symbols, including alphanumeric\ncharacters (e.g., numbers and letters), corporate logos, and\npictorial representations of animals.\nReasoning: In this category, the UA V must execute tasks\nnecessitating logical deduction. Exam", "quality_score": 8.25}}
{"id": "test_59", "question": "Why is the compositionality gap in language models an important area of study, according to the paper by Ofir Press et al. (2023)?", "golden_answers": ["The compositionality gap refers to the difference between the performance of language models on individual components versus their overall performance, and measuring and narrowing this gap can help improve the reliability and trustworthiness of these models."], "prediction": "The compositionality gap in language models is an important area of study because it highlights the limitations of these models in understanding and generating language that involves complex structures and relationships. Addressing this gap can lead to improvements in the models' ability to handle nuanced language tasks, thereby enhancing their overall performance and applicability in real-world scenarios.", "f1": 0.4235294117647059, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00995v1\nIn: The\nTwelfth International Conference on Learning Representations (2024)\n[31] Sta\u0144, I.: Why is the linguistic context important while work-\ning on textual data? (Nov 2023), https://medium.com/@izab.sta/\nwhy-is-the-linguistic-context-important-ba9069cf34de\n[32] Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H.W.,\nChowdhery, A., Le, Q.V., Chi, E.H., Zhou, D., , Wei, J.: Challenging big-\nbench tasks and whether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261 (2022)\n[33] Tuora, R., Zwierzchowska, A., Zawadzka-Paluektau, N., Klamra, C.,\nKobyli\u0144ski, L.: Poquad - the polish question answering dataset - description\nand analysis. In: Proceedings of the 12th Knowledge Capture Conference\n2023. p. 105\u2013113. K-CAP \u201923, Association for Computing Machinery, New\nYork, NY, USA (2023), https://doi.org/10.1145/3587259.3627548\n[34] Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj,\nA., He, X., Jiang, Z., et al.: Mmlu-pro: A more robust and challenging multi-\ntask language understanding benchmark. arXiv preprint arXiv:2406.01574\n(2024)\n[35] Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., et al.:\nQwen2 technical report. arXiv preprint arXiv:2407.10671 (2024)\n[36] Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin,\nZ., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging\nllm-as-a-judge with mt-bench and chatbot arena. In: Proceedings of the\n37th International Conference on Neural Information Processing Systems. NIPS \u201923, Curran Associates Inc., Red Hook, NY, USA (2024)\n[37] Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D.,\nHou, L.: Instruction-following evaluation for large language models. arXiv\npreprint arXiv:2311.07911 (2023)\n[38] Zhuo, T.Y., Vu, M.C., Chim, J., Hu, H., Yu, W., Widyasari, R., Yusuf,\nI.N.B., Zhan, H., He, J., Paul, I., et al.: Bigcodebench: Benchmarking code\ngeneration with diverse function calls and complex instructions. arXiv\npreprint arXiv:2406.15877 (2024)", "metadata": {"type": "reasoning", "source": "2503.01332v1.pdf", "context": "Teaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334.\nMoran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror,\nDafna Shahaf, and Gabriel Stanovsky. 2024. State\nof what art? a call for multi-prompt llm evaluation.\nTransactions of the Association for Computational\nLinguistics, 12:933\u2013949.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to ", "quality_score": 8.25}}
{"id": "test_60", "question": "Why don't the speakers interrupt each other in the provided audio?", "golden_answers": ["They speak at different times and their overlapping is minimal."], "prediction": "The speakers don't interrupt each other in the provided audio because the first speaker has not reached a clear conclusion or natural pause in their narrative, indicating they are still in the middle of their explanation. Additionally, the timing of the second speaker's attempt to interject is mistimed, as it occurs just as the first speaker is transitioning to a new but related point, suggesting that the first speaker is likely to continue speaking. In spontaneous conversation, speakers typically signal the end of their turn with a clear pause or concluding statement, which is not present in this case.", "f1": 0.06185567010309278, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01174v1\nThe second speaker\u2019s overlap is a mistimedattempt to start their turn, and the first speaker willcontinue, forcing the second speaker to wait for anatural break or pause before speakingQwen2-Audio-Instruct\nB\nInterruption Qwen-Audio-Chat Based solely on the provided audio, it is morelikely that the second speaker\u2019s overlap is a suc-cessful interruption, and they will take over theconversation. This is because the audio endsabruptly as the second speaker begins to speak,suggesting that the first speaker was interrupted. Additionally, the second speaker\u2019s tone soundsconfident and assertive, which is characteristic ofsomeone who is trying to take over a conversation. On the other hand, the first speaker\u2019s response isshort and does not indicate that they are about topause or continue speaking. Whisper+GPT-4o During spontaneous human conversation, inter-ruptions occur frequently and often correspond tobreaks in the information flow between conver-sation partners. Interruptions can happen due toa variety of reasons like urgency, disagreement,etc. Interruptions can be broadly classified intotwo categories: competitive and cooperative in-terruptions. Occurrences of competitive interrup-tions are usually disrupting to the flow of con-versation between speakers while cooperative in-terruptions are more supportive to the flow. Youare examining a short audio clip from a conver-sation between two people. The audio ends sud-denly RIGHT as the second speaker begins to talk,overlapping with the first speaker. It\u2019s impossibleto tell from this brief excerpt if the first speakerwas about to pause, was having trouble finding aword, or would have continued speaking. BasedONLY on the provided audio, is it more likelythat: (a) The second speaker\u2019s overlap is a suc-cessful interruption, and they will take over theconversation? (b) The second speaker\u2019s overlap isa mistimed attempt to start their turn, and the firstspeaker will continue, forcing the second speakerto wait for a natural break or pause before speak-ing? Justify your choice, considering the fac-tors that might lead to a successful interruptionor encourage someone to wait their turn, but fo-cus on how the SPECIFIC TIMING of this over-lap makes one outcome more likely than the other. Based on the provided audio excerpt, it is morelikely that: (b) The second speaker\u2019s overlap is amistimed attempt to start their turn, and the firstspeaker will continue, forcing the second speakerto wait for a natural break or pause before speak-ing. Justification: 1. **Context and Flow**: Thefirst speaker is in the middle of a detailed expla-nation about weather patterns and their impact onsnowfall. They have not yet reached a clear con-clusion or natural pause in their narrative. Thesentence\u00a8But I have played\u00a8suggests that the firstspeaker is about to continue with a related pointor anecdote. 2. **Timing of Overlap**: The over-lap occurs just as the first speaker is transitioningto a new but related point (\u00a8But I have played\u00a8).This timing suggests that the first speaker is notfinished and is likely to continue speaking. Thesecond speaker\u2019s attempt to interject at this mo-ment is mistimed because it interrupts the flow ofthe first speaker\u2019s ongoing thought. 3. **Con-versational Dynamics**: In spontaneous conver-sation, speakers often signal the end of their turnwith a clear pause or a concluding statement.", "metadata": {"type": "reasoning", "source": "2503.01174v1.pdf", "context": "with (a) Yes or (b) No.\n(b) No\nInterruption\nSALMONN Do the 2 speakers interrupt\neach other anywhere in the\nprovided audio? Briefly\nexplain your answer. You\nmust reply with (a) Yes or\n(b) No.\nNo, the 2 speakers\ndo not interrupt each\nother in the provided\naudio.\nQwen2-\nAudio-\nInstruct\nNo, the speakers do\nnot interrupt each\nother. They speak at\ndifferent times and\ntheir overlapping is\nminimal.\nQwen-Audio-\nChat\nYes, the two speakers\ninterrupt each other.\nWhisper+GPT-\n4o\nDo the 2 speakers interrupt\ne", "quality_score": 8.25}}
{"id": "test_61", "question": "Why are comonads Mk idempotent, and what does this imply about their relationship to coreflective subcategories?", "golden_answers": ["Their comultiplications \u03b4 are natural isomorphisms. Idempotent comonads on a category C correspond precisely to coreflective subcategories of C."], "prediction": "Comonads Mk are idempotent because their comultiplications \u03b4 are natural isomorphisms. This idempotency implies that they correspond precisely to coreflective subcategories of a category C, meaning that the inclusion functor from the coreflective subcategory to C has a right adjoint.", "f1": 0.6296296296296297, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01247v1\n\u2a7d k with root a, and\n(M) for all x, y\u2208 A, x \u227a y if and only if RA(x, y) for some unique R \u2208 \u03c3. \u2022 morphisms: homomorphisms of Kripke models. 1Equivalently, it is a monotone map that preserves the height of elements. 10 SAMSON ABRAMSKY, THOMAS LAURE, AND LUCA REGGIO\nNote that the tree order of a synchronization tree is \u201cdefinable\u201d and hence preserved\nby any homomorphism of Kripke models. Thus the forgetful functor Lk : EM(Mk) \u2192\nStruct\u2022(\u03c3) is fully faithful and can be identified with the inclusion into Struct\u2022(\u03c3) of\nthe full subcategory defined by synchronization trees of height \u2a7d k. Its right adjoint\nFk : Struct\u2022(\u03c3) \u2192 EM(Mk) sends a Kripke model to its k-unravelling.\nRemark 3.5. In fact, the modal comonads Mk are idempotent, meaning that their\ncomultiplications \u03b4 are natural isomorphisms. In turn, idempotent comonads on a\ncategory C correspond precisely to coreflective subcategoriesof C, i.e. full subcategoriesD\nsuch that the inclusion functor D ,\u2192 C has a right adjoint. 3.3. Logical equivalences. In [2, 6] it was shown how game comonads can be used to\ncapture, in a categorical fashion, preservation of various fragments and extensions of the\nlogics FOk, Lk and MLk. We recall here the case of the existential positive fragments\nand the full fragments, and outline the way in which the equality symbol is handled in\nthe framework of game comonads. Existential positive fragments. Given a collection of formulas L \u2286 L\u221e,\u03c9, consider its\nexistential positive fragment\n\u2203+L\nconsisting of formulas with no universal quantifiers and no negations; for modal logic,\nthis corresponds to barring the use of modalities \u25a1R and negations. The next result\nshows that preservation of existential positive fragments is captured by the homomorph-\nism preorder in the Eilenberg\u2013Moore category for the corresponding comonad (for a\nproof, cf. [2, 6] and also [7]).2 Recall that, for a comonad Gk on a category C, we denote\nby Fk : C \u2192 EM(Gk) the right adjoint to the forgetful functor Lk : EM(Gk) \u2192 C.\nTheorem 3.6. The following are equivalent for all (pointed) structures A and B:\n(i) A \u21db\u2203+FOk B if, and only if, there exists an arrow FkA \u2192 FkB in EM(Ek). (ii) A \u21db\u2203+Lk B if, and only if, there exists an arrow FkA \u2192 FkB in EM(Pk).\n(iii) (A, a) \u21db\u2203+MLk (B, b) if, and only if, there exists an arrow Fk(A, a) \u2192 Fk(B, b)\nin EM(Mk). The equality symbol. In their basic form, game comonads capture fragments of logics\nwithout equality. This is sufficient for modal logic, since the image of the standard\ntranslation is contained in the equality-free fragment of FO, and was also sufficient for\nTheorem 3.6, since the fragments \u2203+FOk and \u2203+Lk admit equality elimination. To\nmodel logics with equality, such as the fragments FOk and Lk, we proceed as follows. Given a relational vocabulary \u03c3, consider a fresh binary relation symbol I and the\nexpanded vocabulary\n\u03c3I := \u03c3 \u222a {I}.", "metadata": {"type": "reasoning", "source": "2503.01247v2.pdf", "context": "10 SAMSON ABRAMSKY, THOMAS LAURE, AND LUCA REGGIO\nNote that the tree order of a synchronization tree is \u201cdefinable\u201d and hence preserved\nby any homomorphism of Kripke models. Thus the forgetful functor Lk : EM(Mk) \u2192\nStruct\u2022(\u03c3) is fully faithful and can be identified with the inclusion into Struct\u2022(\u03c3) of\nthe full subcategory defined by synchronization trees of height \u2a7d k. Its right adjoint\nFk : Struct\u2022(\u03c3) \u2192 EM(Mk) sends a Kripke model to its k-unravelling.\nRemark 3.5. In fact, the modal comonads M", "quality_score": 8.25}}
{"id": "test_62", "question": "How does the introduction of dynamic adaptation mechanism parameters \u03b1 and \u03b2 in Equation-18 affect the network's output compared to a classic ResNet?", "golden_answers": ["The new network can adaptively adjust the influence of both the residual term and the input direct connection term, allowing for more robustness and flexibility."], "prediction": "The introduction of dynamic adaptation mechanism parameters \u03b1 and \u03b2 in Equation-18 allows for a more flexible influence of the residual term and the input direct connection term on the network's output, enhancing the model's ability to adapt to different weight parameters during forward propagation. This improves gradient transfer efficiency and learning ability, providing stronger robustness and flexibility for deep feature extraction compared to a classic ResNet, which uses fixed weights in its residual connections.", "f1": 0.31460674157303364, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01217v1\nThe output Y combines the candidate activation state \u02c6Y and the original input Xin, and the update gate \u03a6 controls\nthe information fusion ratio between the two parameters. When calculating the candidate activation state \u02c6H, the\nexponential average attention mechanism uses the Laplace attention function(Equation-15) as the core component. Based on the concept of reduced-bias, it can be improved the reduced-bias Laplace function as Equation-16 to adapt to\nthe introduction of deviation inside.\nFla(x; \u00b5, \u03c3) =\n1 + erf\n\u0010\nx\u2212\u00b5\n\u03c3\n\u221a\n2\n\u0011\n2 (15)\nFrla(x; \u00b5, \u03c3) = Norm(\n1 + erf\n\u0010\nx\u2212\u00b5\n\u03c3\n\u221a\n2\n\u0011\n2 + Xin) (16)\n7\n\nPreprint. This is a preprint paper under reviewing. The Laplace attention function maps the attention score of QKT to a smooth range by dynamically adjusting the\nparameters \u00b5 and \u03c3, providing a more stable gradient and a more flexible attention distribution. This smoothing range is\nmapped to (0, 1) using the erf function, which improves the global context modeling ability of the model, and ultimately\nimproves the expression effect and task performance of Y .\n3.5 Reduced-bias Module\nCompared with the traditional residual connection layer, proposed work have improved on the basis of the classic\nresidual network structure (Residual Network) [20], and added a dynamic adjustment mechanism for the network\nweights before and after the residual connection. Specifically, the proposed Reduced-bias module fits the sensitivity and influence of different weight parameters on\ndownstream gradient updates by constructing an adaptive weight adjustment strategy during the forward propagation\nprocess. In the verification feedback stage, the module can select the optimal parameter configuration through dynamic\noptimization, thereby enhancing the gradient transfer efficiency and learning ability of the model. In addition, this\nimprovement can effectively alleviate the information transfer bias problem caused by fixed weights in traditional\nresidual connections, and provide stronger robustness and flexibility for deep feature extraction and learning of the\nmodel. Classic resnet can be expressed as Equation-17. yl = F(xl, Wl) + xl (17)\nwhere xl is the input of the l-th layer module. F(\u00b7) represents the operation of the network in this layer. Wl is a\nparameter of the l-th module. Among Equation-17, dynamic adaptation mechanism parameters \u03b1 and \u03b2 is introduced. The new network can be\nexpressed as Equation-18. yl = \u03b1l \u00b7 F(xl, Wl) + \u03b2l \u00b7 xl (18)\nwhere \u03b1 controls the influence of the residual term, and \u03b2 controls the influence of the input direct connection term. Besides, l \u2208 L, L represents all the layer connected by Reduced-biased module. tl = \u03c3\n\u0012 \u2202L\n\u2202F(xl, Wl) \u00b7 Wt + bt\n\u0013\n(19)\nwhere L is the loss function, W\u03b1 and W\u03b2 are learnable weight matrices, b\u03b1 and b\u03b2 are bias terms, and \u03c3(\u00b7) is the\nactivation function. By deriving Equation-19, yl can be got by Equation-20. yl = \u03c3\n\u0012 \u2202L\n\u2202F(xl, Wl) \u00b7 W\u03b1 + b\u03b1\n\u0013\n\u00b7 F(xl, Wl)\n+ \u03c3\n\u0012 \u2202L\n\u2202xl\n\u00b7 W\u03b2 + b\u03b2\n\u0013\n\u00b7 xl\n(20)", "metadata": {"type": "reasoning", "source": "2503.01217v1.pdf", "context": "residual connections, and provide stronger robustness and flexibility for deep feature extraction and learning of the\nmodel.\nClassic resnet can be expressed as Equation-17.\nyl = F(xl, Wl) + xl (17)\nwhere xl is the input of the l-th layer module. F(\u00b7) represents the operation of the network in this layer. Wl is a\nparameter of the l-th module.\nAmong Equation-17, dynamic adaptation mechanism parameters \u03b1 and \u03b2 is introduced. The new network can be\nexpressed as Equation-18.\nyl = \u03b1l \u00b7 F(xl, Wl) + \u03b2l ", "quality_score": 8.25}}
{"id": "test_63", "question": "What is \u03a6p in Eq. (8)?", "golden_answers": ["min v\u2208V (Rp) A\u03f5(v) subject to some constraints."], "prediction": "\u03a6p is not explicitly defined in the provided document.", "f1": 0.0, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00288v1\nFor each P \u2208 P \u2264m, choose a value \u03b4P \u2208 {\u2212 1, 1} and let\nS(n,m, (\u03b4)m) :=\n{\nQ\u2208 P n | if P \u2208 P \u2264m, then\n(P\nQ\n)\n= \u03b4P\n}\n. Recall that \u03a0 q(m) = \u2211 m\nj=1 \u03c0q(j) and P(m) = \u220f\nP \u2208P\u2264m\nP. Proposition 8.3. For integer m\u2265 1 and large enough integer n\u2265 qm1/ 2\n, we have\n\u2211\nQ\u2208S(n,m, (\u03b4)m)\n(\n\u2212L\u2032(1,\u03c7 Q)\nL(1,\u03c7 Q)\n)\n= \u03c0q(n)\n2\u03a0 q(m)\n\u2211\nP \u2208P\u2264m\nr\u22651 odd\n(\u22121)\n(q\u22121)ndP\n2 \u03b4P\nlog(qdP )\n(qdP )r + Oq\n(\nn2q\nn\n2 +2m)\n. Proof. For each f \u2208 M , let \u03b4f := \u220f\nP |f \u03b4P . For a given Q\u2208 P n, one has\n(8.3)\n\u2211\nf|P(m)\n\u03b4f \u03c7f (Q) =\n\u220f\nP \u2208P\u2264m\n(1 + \u03b4P \u03c7P (Q)) =\n{\n2\u03a0 q(m) if Q\u2208 S(n,m, (\u03b4)m),\n0 otherwise . As shown in [23, Lemma 6.1], for large enough n\u2265 qm1/ 2\n, we have\n(8.4) # S(n,m, (\u03b4)m) = qn\n2\u03a0 q(m)n + Oq\n(\nq\nn\n2 +m)\n. For any real number y >1 and any Q\u2208 P n it follows from [17, Equation (6.8.4)] that\n(8.5) \u2212L\u2032(1,\u03c7 Q)\nL(1,\u03c7 Q) =\n\u2211\nf\u2208M\u2264\u230alogq(y)\u230b\n\u03c7Q(f)\u039b( f)\nN(f) + Oq\n(log(qn)\ny\n1\n2\n)\n. Now, let y:= q2ndP(m) so that \u03c1(n,m ) := log q(y) = 2 ndP(m). From (8.4) we have\n(8.6) # S(n,m, (\u03b4)m) log(qn)\nqndP(m)\n\u226aq\n1\nqn(dP(m)\u22121)2\u03a0 q(m) . By (8.3), the quadratic reciprocity law, and ( \u22121)\n(q\u22121)nrdP\n2 = ( \u22121)\n(q\u22121)ndP\n2 for r\u2265 1 odd,\n(8.7)\n\u2211\nQ\u2208S(n,m, (\u03b4)m)\n\u2211\nh\u2208M\u2264\u03c1 (n,m )\n\u03c7Q(h)\u039b( h)\nN(h) = 1\n2\u03a0 q(m)\n\u2211\nf|P(m)\n\u03b4f\n\u2211\nP \u2208P,r \u22651\nrdP \u2264\u03c1(n,m )\n(\u22121)\n(q\u22121)ndP\n2\nlog(qdP )\n(qdP )r\n\u2211\nQ\u2208Pn\n(Prf\nQ\n)\n. We now separately compute the contribution for squares and non- squares Prf\u2019s in (8.7). Since P(m) is square-free, f is too. Therefore, Prf is a square if and only if f = P and r\nis odd. Thus, the contribution of the squares is\n(8.8) \u03c0q(n)\n2\u03a0 q(m)\n\u2211\nP \u2208P\u2264m\n(\u22121)\n(q\u22121)ndP\n2 \u03b4P log(qdP )\n\u2211\nr\u22651 odd\nrdP \u2264\u03c1(n,m )\n1\n(qdP )r . Now, observe that for any P \u2208 P \u2264m we have\n\u2211\nr\u22651 odd\nrdP >\u03c1(n,m )\n1\n(qdP )r \u2264\n\u221e\u2211\nr=\n\u230a\n\u03c1 (n,m )\ndP\n+1\n\u230b\n1\n(qdP )r \u226aq\n1\nq\u03c1(n,m )+dP\n. 25\n\nTherefore, using (2.3) and (8.2), we can rewrite (8.8) as\n\u03c0q(n)\n2\u03a0 q(m)\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u2211\nP \u2208P\u2264m\nr\u22651 odd\n(\u22121)\n(q\u22121)ndP\n2 \u03b4P\nlog(qdP )\n(qdP )r + Oq\n\uf8eb\n\uf8ed 1\nq\u03c1(n,m )\n\u2211\nP \u2208P\u2264m\ndP\nqdP\n\uf8f6\n\uf8f8\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n= \u03c0q(n)\n2\u03a0 q(m)\n\u2211\nP \u2208P\u2264m\nr\u22651 odd\n(\u22121)\n(q\u22121)ndP\n2 \u03b4P\nlog(qdP )\n(qdP )r + Oq\n(m\nn \u00b7 1\n2\u03a0 q(m)qn(2dP(m)\u22121)\n)\n.(8.9)\nNow, suppose Prf is not a square.", "metadata": {"type": "factual", "source": "2503.01276v2.pdf", "context": "\u03a6p = min\nv\u2208V (Rp)\nA\u03f5(v) subject to some constraints. (8)\nThe constraints are detailed in Eqs. (3) and (4). Hereafter, \u03a6 p is used to represent the local\nsolutions at macropoint x\u2217\np, regardless of the specific type of local solutions. Since solving the cell\nproblems for every macroscopic point using a very fine mesh remains a huge challenge, we design\na hierarchical multicontinuum homogenization method to overcome this difficulty. For a given\nmacroscopic point x\u2217\np, we assume that the local solu", "quality_score": 8.25}}
{"id": "test_64", "question": "What is a limitation of using deep learning methods for long-range ENSO prediction?", "golden_answers": ["They require 'big data' to train models with enough parameters, which can result in them inheriting some of the biases in the training data."], "prediction": "Many deep learning methods require \"big data,\" currently only obtainable through large climate model ensembles, which can result in them inheriting some of the biases in the training data.", "f1": 0.6122448979591836, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01412v1\nDespite these limitations, by aggregating forecasts from\nthe world\u2019s leading climate prediction centres, the IRI plume remains the operational\nbenchmark for ENSO forecasting An up-to-date version of the plume can be found at\nthe IRI website (https://iri.columbia.edu/our-expertise/climate/forecasts/\nenso/current/?enso tab=enso-sst table). In recent years, the advent of deep learning has sparked a resurgence of interest in\ndata-driven ENSO forecasting. A prominent example is the work of Ham et al. (2019),\nwho trained a convolutional neural network (CNN) using a transfer-learning approach\n\u2013 first on large collections of climate model simulations from the CMIP5 ensemble, and\nthen on ocean reanalysis data \u2013 to predict the Ni\u02dc no3.4 indexn months ahead based on\nsea surface temperatures and oceanic heat content from the current and previous 2 months. This deep learning model outperformed state-of-the-art dynamical models at lead times\nbeyond 6 months, achieving a pattern correlation with the observed index above 0.5 out\nto 17 months. A follow-up study applied a multitask learning framework to further im-\nprove forecast accuracy by addressing the seasonally varying nature of ENSO precursors\n(Ham et al., 2021). Other researchers have explored more advanced architectures and\nregularisation strategies to push predictive skill to even longer lead times. For instance,\nforecasts generated by the 3D transformer model of Zhou and Zhang (2023) were found\nto be skilful in predicting the Ni\u02dc no3.4 index at up to 18 months lead time, although bi-\nases in the training data (coming from biases in the underlying CMIP6 climate models\ngenerating the data) led to reduced skill in other regions of the Pacific. A few studies\nhave also attempted long-range ENSO prediction using only observational and reanal-\nysis data. Notably, Patil et al. (2023) developed a deep CNN model trained on observed/reanalysed\nsea surface and vertically-averaged subsurface temperatures, with skilful forecasts ob-\ntained out to 20 months lead time. Their CNN model featured multiple forms of reg-\nularisation including dropout, as well as average pooling to reduce the number of model\nparameters. Similar to Ham et al. (2021), it also contained heterogeneous parameters\nfor each target season to account for seasonal variations in precursors, establishing it as\na prime example of the state-of-the-art performance that is obtainable with deep learn-\ning for long-range ENSO prediction. While these results demonstrate the promise of modern machine learning for multi-\nyear ENSO forecasting, they also highlight persistent challenges. Many deep learning meth-\nods require \u201dbig data\u201d, currently only obtainable through large climate model ensem-\nbles, to train models with enough parameters to capture complex spatiotemporal pat-\nterns, which can result in them inheriting some of the biases in the training data. In con-\ntrast, methods trained solely on the limited observational record risk overfitting unless\nthey are specifically designed for the \u201dsmall data\u201d regime. To address these challenges,\nrecent work has proposed entropic learning techniques which are based on sparsified, prob-\nabilistic approximations of the data that employ the principle of maximum entropy from\ninformation theory to avoid overfitting to noisy/uninformative features (Horenko, 2020,\n2022; Vecchi et al., 2022; Horenko et al., 2023).", "metadata": {"type": "factual", "source": "2503.01412v1.pdf", "context": "to be skilful in predicting the Ni\u02dc no3.4 index at up to 18 months lead time, although bi-\nases in the training data (coming from biases in the underlying CMIP6 climate models\ngenerating the data) led to reduced skill in other regions of the Pacific. A few studies\nhave also attempted long-range ENSO prediction using only observational and reanal-\nysis data. Notably, Patil et al. (2023) developed a deep CNN model trained on observed/reanalysed\nsea surface and vertically-averaged subsurface temper", "quality_score": 8.25}}
{"id": "test_65", "question": "What is the structure of the provided text?", "golden_answers": ["The provided text appears to be a series of hexadecimal code blocks separated by forward slashes, suggesting it may represent a binary or encoded format."], "prediction": "The structure of the provided text includes an introduction to the image-text encoder, a description of the cross-attention mechanism, details on image-text contrastive learning (ITC), image-text matching (ITM), and image-grounded text generation (ITG). Each section outlines specific methodologies, equations, and loss functions related to the tasks of aligning visual and textual representations, classifying image-text pairs, and generating text based on images.", "f1": 0.10526315789473684, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01019v3\nThen we concatenated Q and Xt to\nform the input of the image-text encoder, denoted as EQ,\nencoding it to get output embeddings:\nEQ([Q, Xt]) = [fq, ft] (6)\n= [fq, ft\n[SPE ], ft\nlocal] (7)\nWe employed a cross-attention mechanism [1] to facilitate\ninteraction between learnable embeddings and preliminary\nvisual embeddings. This design enables fq to function as\nthe final visual representation. To ensure clarity over the\nfusion of learnable embeddings and textual representations,\nwe implemented distinct masking strategies within the self-\nattention layers (Figure 2, right panel). Image-text contrastive learning (ITC) The task aims to\nalign visual and textual representations by maximizing their\nmutual information through a contrastive approach. To ac-\ncomplish this, we replaced w[SP E] with w[CLS] to fa-\ncilitate global textual representations denoted as ft\n[CLS] \u2208\nRdt . Furthermore, we implemented uni-modal masking\n(Figure 2, right panel) to enable learnable embeddings\nQ and textual embeddings Xt to attend exclusively to\nthemselves.fq and ft\n[CLS] are then linearly projected to\nrepresentations as:\ngq = Hq\nitc(fq) (8)\ngt = Ht\nitc(ft\n[CLS]) (9)\nwhere Hq\nitc, Ht\nitc are ITC heads. We computed the pairwise\nsimilarity between each visual and textual representationgq\n4\n\nand gt and chose the highest one as the image-text similarity\nto calculate bi-directional contrastive loss:\nL(q|t)\nitc = 1\nN\nNX\nk=1\n\u2212log( exp(max < gq\nk, gt\nk > /\u03c4)\nPN\nn=1 exp(max < gq\nk, gtn > /\u03c4)\n)\n(10)\nL(t|q)\nitc = 1\nN\nNX\nk=1\n\u2212log( exp(max < gq\nk, gt\nk > /\u03c4)\nPN\nn=1 exp(max < gq\nn, gt\nk > /\u03c4)\n)\n(11)\nwhere \u03c4 \u2208 R is a scaling temperature parameter initialized\nto 0.07, N is mini-batch size and <\u00b7, \u00b7> represents the co-\nsine similarity. The overall ITC loss is defined as:\nLitc = 1\n2(L(q|t)\nitc + L(t|q)\nitc ) (12)\nWe here expanded visual representation space from a con-\nventional single vector to a set of vectors e.g.fq \u2208 RLq\u00d7dq\nwhich is different from [26, 63]. Image-text matching (ITM) This task aims to learn a\nprecise alignment by classifying image-text pairs as either\npositive or negative. We implemented a bi-directional mask\n(Figure 2, right panel) that allows all learnable embeddings\nand word token embeddings to attend to one another. The\nresulting output of learned embeddings denoted as fq, cap-\nture enriched multi-modal information. These tokens were\nthen fed into a two-class linear classifier, Hitm, where the\noutputs are averaged across learned embeddings to generate\na logit and compute the Image-Text Matching (ITM) loss:\nLitm = 1\nN\nNX\nk=1\n\u2212log(p(Yk| \u02c6Yk)) (13)\n\u02c6Y = 1\nLq\nLqX\ni=1\nHitm(fq\ni ), (14)\nY represents ground truth labels within mini-batch by hard\nnegative samples mining, as stated in [34]. Image-grounded text generation (ITG) This task is to\ngenerate text conditioned on paired images. To achieve a\ncoherent and precise generation of medical reports within\na unified VLP framework, we chose CLM [5, 47] where\neach word token attends only to preceding tokens, follow-\ning a GPT-style language model architecture [5].", "metadata": {"type": "factual", "source": "2503.01200v1.pdf", "context": "/uni00000036/uni0000004c/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000047\n/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000051/uni00000050/uni0000000c\n/uni00000013/uni00000011/uni00000013\n/uni00000015/uni00000011/uni00000018\n/uni00000018/uni00000011/uni00000013\n/uni0000001a/", "quality_score": 8.25}}
{"id": "test_66", "question": "What is the main contribution of the proposed method according to Tables II to IV?", "golden_answers": ["The proposed method achieves top performance in regression and F1 scores."], "prediction": "The main contribution of the proposed method is its ability to achieve the highest AUC value, indicating the strongest ability to discriminate between abnormal and normal samples, while also demonstrating exceptional computational efficiency and lower computational complexity compared to baseline methods.", "f1": 0.12765957446808512, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "\uf020\nThis approach allows the model to capture \ninformation from different time steps and enables parallel \nprocessing of time series data during the training and inference \nphases. As a result, the model achieves exceptional \ncomputational efficiency and parallel processing capabilities. As shown in Table II, although the number of parameters of the \nproposed model is not the smallest, it is smaller than that of all \nmethods except GLSL. Furthermore, the proposed model \noutperforms all baseline methods in terms of the computational \ncomplexity and inference time measures. To assess the ability of each model to discriminate \nbetween abnormal and normal samples at different thresholds, \nwe also tested the AUC metric for each model. AUC (Area \nUnder the Curve) is the area under the ROC curve, where the \nROC curve plots the False Positive Rate (FPR) on the x -axis \nand the True Positive Rate (TPR) on the y -axis. As the \nthreshold changes from high to low, we obtain a series of (FPR, \nTPR) points that form the ROC curve. The AUC summarizes \nthe TPR and FPR metrics  and reflects the overall performance \nof the model at different thresholds. The higher the AUC, the \nbetter the model's ability to discriminate between abnormal and \nnormal samples. After performing statistical tests, the AUC \nvalues of each model are shown in Table 2. From Table 2, it can \nbe seen that the AUC value of the model we designed is the \nhighest, indicating that our model has the strongest ability to \nperforming between abnormal and normal samples. D. Ablation Experiments \nTo examine the effectiveness and performance of each \ncomponent in the proposed model, we conducted ablation \nexperiments. By systematically removing each component and \nobserving the changes in various metrics, we were able to  \ndetermine the impact of each component. The results of the \nablation experiments are presented in Table III. Here, Pre \nrepresents precision, Rec represents recall, F1 represents the F1 \nscore, Par represents the number of parameters (in millions), \nand FLOPs represent the computational cost in terms of \nfloating-point operations (in millions). TABLE \u2162 \nAblation Experimental Results \n \nScheme 1 is the complete model proposed in this paper. This model employs DWT for time series trend and seasonal \ndecomposition and then inputs them separately into the trend \nencoder and seasonal encoder, with the seasonal encoder using \nMethod Pre (%) Rec (%) F1 (%) AUC Par (M) FLOPs (M) Time (ms) \nCNN-LSTM 79.5 70.0 74.5 0.70 27.9 14,699.7 817.5 \nMTAD-GAT 77.5 87.0 82.0 0.81 1.1 749.2 848.8 \nGAT-GRU 93.3 87.5 90.3 0.84 36.5 14,445.6 13,017.0 \nGLSL 94.5 87.0 90.6 0.93 0.6 3,075.1 2,909.3 \nours 94.7 92.3 93.5 0.97 0.9 148.6 271.0 \nScheme DWT FDAM MFDGCN Pre (%) Rec (%) F1 (%) Par (M) FLOPs (M) \n1 \u221a \u221a \u221a 94.7 92.3 93.5 0.9 148.6 \n2 \u00d7 \u221a \u221a 94.5 92.2 93.4 1.8 324.9 \n3 \u00d7 \u00d7 \u221a 89.1 84.8 86.8 1.8 324.9 \n4 \u00d7 \u00d7 \u00d7 79.0 84.7 81.6 1.7 240.6\n\n8  IEEE SENSORS JOURNAL, VOL. XX, NO. XX, MONTH X, XXXX  \n \nFDAM for feature extraction. Both encoders use the MFDGCN \nfor spatial feature extraction.", "metadata": {"type": "factual", "source": "2503.01217v2.pdf", "context": "across scene tasks, enhancing CNER robustness. As shown in\nTables II to IV, proposed method achieves top performance\nin regression and F1 scores. Ablation studies in Table V\nconfirm that hierarchical exponential attention and reduced-\nbias modules expand data depth, excelling in conventional\ntasks in Figure 4 and domain tasks in Figures 5 and 6.\n2) The Hierarchical Reduced-bias EMA architecture enables\nprecise CNER feature extraction. Inspired by DAE [27], the", "quality_score": 8.25}}
{"id": "test_67", "question": "How does the Transformer module adopt the design of Restormer [36] to reduce computational cost?", "golden_answers": ["The Transformer module adopts a simplified design that enables its use across multiple levels, which reduces the computational cost."], "prediction": "The Transformer module adopts the design of Restormer by utilizing a lightweight architecture that incorporates efficient attention mechanisms and residual connections, which help in reducing the computational cost while maintaining performance.", "f1": 0.37209302325581395, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "session 23_paper.pdf\nThe proposed training and compression techniques reduce the parameter size by 15.9-25.5\u00d7 with minimal accuracy loss. When performing inference with these models, T-REX requires 31 to 65.9\u00d7 less EMA and exhibits 1.2 to 3.4\u00d7 higher hardware utilization. We compared T-REX with the previous accelerators. For those works that do not consider EMA, we estimated the energy cost at 3.7pJ/b and the latency cost at 6.4GB/s, both based on the LPDDR3 SDRAM [22,23]. T-REX achieves 68 to 567\u00b5s/token and 0.41 to 3.95\u00b5J/token, marking signi\ufb01cant improvements across several workloads over prior works. Acknowledgement:  This work was supported in part by an SRC AIHW program (Task 3160.002) and by COGNISENSE, one of seven centers in JUMP 2.0, an SRC program sponsored by DARPA. Figure 23.1.1: Challenges in transformer processing and proposed solutions. Figure 23.1.2: Overall architecture of T-REX. Figure 23.1.3: Factorizing training and compressions with hardware support. Figure 23.1.4: Dynamic batching technique for variable input token length. Figure 23.1.5: Input and output buffers based on two-direction accessible register \ufb01le. Figure 23.1.6: Measurement result and comparison table. ISSCC 2025 / February 19, 2025 / 8:00 AM\n407  DIGEST OF TECHNICAL PAPERS  \u2022\n23\n\n\u2022  2025 IEEE International Solid-State Circuits Conference\nISSCC 2025 PAPER CONTINUATIONS AND REFERENCES\n979-8-3315-4101-9/25/$31.00 \u00a92025 IEEE\nFigure 23.1.7: Chip photograph and performance summary. References: [1] Y. Wang et al., \u201cA 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing,\u201d ISSCC, pp. 464-465, 2022. [2] F. Tu et al., \u201cA 28nm 15.59uJ/Token Full-Digital Bitline-Transpose CIM-Based Sparse Transformer Accelerator with Pipeline/Parallel Recon\ufb01gurable Modes,\u201d ISSCC, pp. 466-467, 2022. [3] S. Liu et al., \u201cA 28nm 53.8TOPS/W 8b Sparse Transformer Accelerator with In-Memory Butter\ufb02y Zero Skipper for Unstructured-Pruned NN and CIM-Based Local-Attention-Reusable Engine,\u201d ISSCC, pp. 250-251, 2023. [4] Y. Wang et al., \u201cA 28nm 77.35TOPS/W Similar Vector Traceable Transformer Processor with Principal-Component-Prior Speculating and Dynamic Bit-wise Stationary Computing,\u201d IEEE Symp. VLSI Circuits, C16-5, 2023. [5] H. You et al., \u201cViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design,\u201d IEEE HPCA, 2023. [6] P. Dong et al., \u201cHeatViT: Hardware-Ef\ufb01cient Adaptive Token Pruning for Vision Transformers,\u201d IEEE HPCA, 2023. [7] J. Dass et al., \u201cViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention,\u201d IEEE HPCA, 2023. [8] B. Keller et al., \u201cA 17-95.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm,\u201d IEEE Symp. VLSI Circuits, C2-1. 2022. [9] S. Moon et al., \u201cA 127.8TOPS/W Arbitrarily Quantized 1-to-8b Scalable-Precision Accelerator for General-Purpose Deep Learning with Reduction of Storage, Logic and Latency Waste,\u201d ISSCC, pp. 330-331, 2023. [10] F. Tu et al., \u201cMulTCIM: A 28nm 2.24uJ/Token Attention-Token-Bit Hybrid Sparse Digital CIM-Based Accelerator for Multimodal Transformers,\u201d ISSCC, pp. 248-249, 2023. [11] H. Mun et al., \u201cA 28 nm 66.8 TOPS/W Sparsity-Aware Dynamic-Precision Deep-Learning Processor,\u201d IEEE Symp. VLSI Circuits, C16-1, 2023. [12] B. Keller et al., \u201cA 95.6-TOPS/W Deep Learning Inference Accelerator With Per-Vector Scaled 4-bit Quantization in 5 nm,\u201d IEEE JSSC, vol. 58, no. 4, pp. 1129-1141, 2023. [13] Y. Qin et al., \u201cFACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction,\u201d IEEE/ACM ISCA, 2023.", "metadata": {"type": "reasoning", "source": "2503.01265v1.pdf", "context": "calization of suspected lesion regions within these scans. This\ncomponent operates exclusively during the training phase,\ngenerating blurred region-of-interest (ROI) prompts through\nits convolution-based stochastic scaling algorithm, which is\ndetailed in Algorithm 1. It defines a series of convolution\n1The Transformer module adopts the design of Restormer [36], a simplified\nTransformer that reduces the computational cost, enabling its use across\nmultiple levels.", "quality_score": 8}}
{"id": "test_68", "question": "What advantage does our explainability framework, FM Explainer, have over other methods like SHAP and LIME?", "golden_answers": ["It captures both individual and interaction effects, which other methods may overlook."], "prediction": "FM Explainer captures both linear individual contributions and quadratic pairwise interaction effects, providing a more detailed and precise understanding of feature interactions, which is crucial for diagnosing anomalies in water systems, unlike SHAP and LIME that primarily focus on independent feature contributions.", "f1": 0.2641509433962264, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01229v1\nWhile attribution focuses on identifying the most impactful\nfeatures, explainability more broadly refers to understanding\nhow a model arrives at its predictions. In recent years,\nexplainability methods have gained traction across various\nfields, particularly in security-related ML applications. These\nmethods provide a means to interpret the behavior of complex\nmodels, which are often referred to as \u201cblack box\u201d due\nto the difficulty in tracing how they make decisions [29]. Using explainability techniques in anomaly detection models\nhelps operators and the public trust these systems, enhancing\ntransparency, reliability, and security management in critical\ninfrastructure.\nChallenges. Although attribution and explainability techniques\nhave been widely used, applying them to water systems\npresents specific challenges. Traditional model-agnostic\nmethods like LIME [24], SHAP [25], and LEMNA [26]\nfocus on individual feature importance by providing linear\napproximations of feature contributions. While SHAP and\nLEMNA can account for some interactions, their outputs are\nstill primarily based on independent feature contributions. In water systems, however, sensors and actuators are tightly\ninterconnected, and understanding these interactions is crucial\nfor identifying the root causes of anomalies [8]. Neglecting\nthese connections could cause attribution methods to overlook\nkey factors behind complex, targeted attacks [30]. In terms\nof explainability, while deep learning models like CNNs\n[31] and LSTMs [32] are effective at detecting novel threats\n[21], their complexity makes understanding their decisions\na significant challenge. In time-sensitive situations, this lack\nof transparency reduces effectiveness, as operators need to\nquickly understand how decisions are made. Explainability\nmethods that focus only on individual features, without\naccounting for interactions, can make it harder to diagnose\nsystem failures or instabilities. Developing explanation\ntechniques that account for feature interactions is important\nfor securing water systems against sophisticated threats, such\nas multi-asset attacks. Scientific Contributions. This work addresses the challenges\nof attribution and explainability in water systems by using\nFactorization Machines (FM) [33], originally developed for\nrecommendation systems [34], as a model-agnostic explainer\nfor existing anomaly detectors. FM identifies both individual\nasset contributions and interactions between components, such\nas sensors and actuators, which can be crucial for diagnosing\nanomalies. By capturing these interactions, our approach of-\nfers more accurate attribution, improving the detection and\nmitigation of complex attacks on water infrastructure. FM provides explainability by capturing both linear feature\ncontributions and second-order interactions (quadratic effects),\noffering a more detailed and precise understanding than tradi-\ntional linear methods. Additionally, FM operates efficiently in\nlinear time [33], making it ideal for real-time applications. Our\napproach improves interpretability and security management\nwhile retaining the underlying detection models. In brief, the\npaper makes the following scientific and pragmatic contribu-\ntions:\n\u2022 We introduce a novel model-agnostic framework that\nleverages Factorization Machines (FM) to enhance attack\nattribution and explainability in water systems. \u2022 We model both linear individual contributions and\nquadratic pairwise interaction effects in water systems\nusing second-order FM equation, enabling more precise\nattack attribution while preserving computational effi-\nciency. \u2022 We validate our proposed approach as a effective plug-\nand-play attack attribution method that seamlessly in-\ntegrates with existing deep learning anomaly detection\nmodels in water systems.", "metadata": {"type": "reasoning", "source": "2503.01229v1.pdf", "context": "in single-feature scenarios, outperforming SHAP (AvgRank\n0.1539) and significantly better than LIME (AvgRank 0.3664).\nAt Top-1 accuracy, SHAP outperforms FM. However, this\nadvantage is confined to the Top-1 metric, as FM remains\nhighly competitive or superior across Top-5 and Top-10 met-\nrics. Both FM and LEMNA consistently outperform SHAP\nand significantly outperform LIME, which struggles across\nall metrics. These results show that, beyond the performance\nboost in multi-feature attacks, our FM ", "quality_score": 8}}
{"id": "test_69", "question": "What is the primary focus of the image?", "golden_answers": ["The face and upper body of the woman."], "prediction": "The primary focus of the image is the woman's face and upper body.", "f1": 0.625, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01261v2\nThe background is a solid \nlight gray color, which ensures that the focus remains solely on the \nwoman. The simplicity of the background further accentuates the \ndetails of the woman's face and hair. The image does not contain any \ntext or other discernible objects. The relative position of the woman to \nthe background suggests she is the main subject of this image. The \nimage does not provide any information about the location or setting. The image is a portrait, focusing on the woman's face and upper body.\nOrigin Caption\nShe has arched eyebrows. She is smiling, and young and is wearing \nlipstick. This woman is wearing heavy makeup. She has wavy hair, and \nmouth slightly open. She has mouth slightly open, arched eyebrows, and wavy hair and is \nwearing heavy makeup. She is smiling. This person is attractive and has blond hair, mouth slightly open, and \narched eyebrows. This person has arched eyebrows, wavy hair, and mouth slightly \nopen. She wears lipstick. She is attractive. Long Text\nIn the image, there's an older man who is the main subject. He has a \nbald head and is wearing glasses. His chin is resting on his hand, \nsuggesting a moment of contemplation or deep thought. He is dressed \nin a beige jacket, which adds a professional or formal tone to the image. The background is a blurred gray color, which puts the focus entirely \non the man. There are no other discernible objects or texts in the image. The man's position relative to the background suggests he is standing \nin front of it. The image does not provide any information about the \nlocation or setting. It's a simple yet intriguing portrait of an older man, \ncaptured in a moment of quiet reflection. This person has receding hairline. The man has narrow eyes, receding hairline, and eyeglasses. This man has narrow eyes, big nose, receding hairline, and gray hair. The man has big nose, eyeglasses, and narrow eyes. The person has receding hairline. The image captures a close-up of a woman's face, her gaze directed \nstraight at the camera. Her hair, a vibrant shade of blonde, frames her \nface, adding a touch of warmth to the overall composition. Her eyes, a \nstriking shade of blue, stand out against her complexion, and her lips, \npainted a soft pink, add a subtle contrast. The background, though \nblurred, gives the impression of a room with a window, suggesting an \nindoor setting. The focus on the woman's face and the blurred \nbackground create a depth of field effect, drawing attention to her \nexpressive features. There are no discernible texts or other objects in the \nimage. The relative position of the woman to the background suggests \nshe is in the foreground of the scene. The image does not provide any \ninformation about the actions of the objects or their precise locations. The image is devoid of any aesthetic descriptions, focusing solely on the \nfactual elements present. She is wearing lipstick. She is young and has brown hair, and wavy \nhair. This woman is wearing lipstick. She has arched eyebrows.", "metadata": {"type": "factual", "source": "2503.01261v2.pdf", "context": "The image captures a close-up of a young woman with striking blue \neyes and blonde hair. Her hair, styled in loose waves, cascades down \nher shoulders, adding a touch of elegance to her appearance. She is \nwearing a white tank top, which contrasts beautifully with her blonde \nhair. Her gaze is directed straight at the camera, creating a sense of \nconnection with the viewer. A slight smile graces her face, adding a \nwarm and friendly aura to the overall image. The background is a solid \nlight gra", "quality_score": 8}}
{"id": "test_70", "question": "How is the delta function \u03b4(t1 \u2212 t2) regularized for discrete time slices?", "golden_answers": ["as \u03b4n1n2 1 \u2206t, where \u2206t = t N and n = 0, 1, 2, . . . , N\u2212 1"], "prediction": "The delta function \u03b4(t1 \u2212 t2) is regularized as \u03b4(t1 \u2212 t2) = \u03b4(n1\u2206t \u2212 n2\u2206t) = \u03b4n1n2 / \u2206t.", "f1": 0.19354838709677422, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01279v1\nNext, we consider taking discrete time slices tn = n\u2206t,\nwhere \u2206t = t\nN and n = 0, 1, 2, . . . , N\u2212 1. At each time\nstep, we have a random noise term \u03b7ij(n\u2206t). For two\ntimes t1 = n1\u2206t and t2 = n2\u2206t, we choose to regularize\nthe delta function as follows:\n\u03b4(t1 \u2212 t2) = \u03b4(n1\u2206t \u2212 n2\u2206t) = \u03b4n1n2\n1\n\u2206t . (12)\nSo that Eq.(10) becomes\nE(\u03b7ij(n\u2206t)\u03b7kl(m\u2206t)) = 1\n\u2206t\u03bbij\u03b4il\u03b4jk\u03b4nm . (13)\n\n3\nOne can treat Uk as a D2k-dimensional matrix gener-\nated by the operator Lk, with the row and column in-\ndices labeled explicitly as I = {i1, i2, i3, . . . , i2k} and\nJ = {j1, j2, j3, . . . , j2k}\n[Uk]IJ = Ui1j1 U\u2217\ni2,j2 \u00b7\u00b7\u00b7 Ui2k\u22121j2k\u22121 U\u2217\ni2k,j2k (14)\nwhere we take the order of {i} and {j} on the right-hand\nside for convenience. Then, the equation of motion of Uk\nis simply\n\u2202tUk(t) = Lk.Uk(t) . (15)\nTo illustrate the calculation, we first consider the sim-\nplest case of k = 1:\nU1(t) := E[U(t) \u2297 U\u2217(t)] , U1;ij;i\u2032j\u2032 = [exp (L1t)]ij;i\u2032j\u2032 .\n(16)\nOne can obtain the expression for L1 through direct cal-\nculation. From the definition, we have\nUij(t + \u2206t)U\u2217\nkl(t + \u2206t)\n=\nDX\nu,v=1\n\u0010\ne\u2212iH(t)\u2206t\n\u0011\niu\n\u0010\neiH\u2217(t)\u2206t\n\u0011\nkv\nUuj(t)U\u2217\nvl(t) . (17)\nNext, we employ the Taylor series expansion and take the\nnoise ensemble average on both sides, retaining terms up\nto linear order in \u2206 t. Thus, we obtain\nE[Uij(t + \u2206t)U\u2217\nkl(t + \u2206t)]\n= E\n\" DX\nu,v=1\n\u0010\ne\u2212iH(t)\u2206t\n\u0011\niu\n\u0010\neiH\u2217(t)\u2206t\n\u0011\nkv\nUuj(t)U\u2217\nvl(t)\n#\n= E\n\" DX\nu,v=1\n\u0012\nI \u2212 iH(t)\u2206t \u2212 1\n2H2(t)\u2206t2\n\u0013\niu\n\u0012\nI + iH\u2217(t)\u2206t \u2212 1\n2H\u22172(t)\u2206t2\n\u0013\nkv\nUuj(t)U\u2217\nvl(t)\n#\n=\nDX\nu,v=1\nE\n\u0014\n(\u03b4iu \u2212 iEi\u03b4iu\u2206t \u2212 i\u03b7iu(t)\u2206t) (\u03b4kv + iEk\u03b4kv\u2206t + i\u03b7\u2217\nkv(t)\u2206t)\n\u0015\nE[Uuj(t)U\u2217\nvl(t)]\n=\nDX\nu,v=1\n\u0014\n\u03b4iu\u03b4kv \u2212 iEi\u03b4iu\u03b4kv\u2206t + iEk\u03b4iu\u03b4kv\u2206t \u2212 1\n2\nX\ns\n\u03bbis\u03b4iu\u03b4kv\u2206t \u2212 1\n2\nX\ns\n\u03bbks\u03b4iu\u03b4kv\u2206t + \u03bbik\u03b4ik\u03b4uv\u2206t\n\u0015\n\u00d7 E[Uuj(t)U\u2217\nvl(t)]\nwhere I is the identity matrix. We have utilized the\nfact that white noise has no time correlation, such that\nE(A(t)B(t\u2032)) = E(A(t))E(B(t\u2032)) for t \u0338= t\u2032. By comparing\nthe result with the differential equation given in Eq. (15)\nand recognizing L1, we obtain\nL1;ij;i\u2032j\u2032 = wij\u03b4ii\u2032\u03b4jj\u2032 + \u03bbii\u2032\u03b4ij\u03b4i\u2032j\u2032 . (18) Here wij \u2261 \u2212iEi + iEj \u2212 1\n2\nPD\nk=1 (\u03bbik + \u03bbjk). For sim-\nplicity we use the graph representation\nL1;ij;i\u2032j\u2032 = wij\ni\ni\u2032\nj\nj\u2032 + \u03bbii\u2032\ni\nj\ni\u2032\nj\u2032 (19)\nwhere each line with two indices i, jat the endpoints\nof the graph represents a \u201cpropagator\u201d i.e, \u03b4ij. Simi-\nlarly, one can derive expressions for other operators Lk\nwith k \u2265 1. One may regard Lk as the effective time-\nindependent Hamiltonian of an auxiliary system, while\nUk represents the imaginary time evolution. In principle,\nto obtain the dynamics, one needs to diagonalize Lk.", "metadata": {"type": "reasoning", "source": "2503.01279v2.pdf", "context": "effective time evolution on 2 k-contours:\nUk \u2261 E[U\u2297k\nt \u2297 U\u2217\u2297k\nt ] \u2261 eLkt . (11)\nNext, we consider taking discrete time slices tn = n\u2206t,\nwhere \u2206t = t\nN and n = 0, 1, 2, . . . , N\u2212 1. At each time\nstep, we have a random noise term \u03b7ij(n\u2206t). For two\ntimes t1 = n1\u2206t and t2 = n2\u2206t, we choose to regularize\nthe delta function as follows:\n\u03b4(t1 \u2212 t2) = \u03b4(n1\u2206t \u2212 n2\u2206t) = \u03b4n1n2\n1\n\u2206t . (12)\nSo that Eq.(10) becomes\nE(\u03b7ij(n\u2206t)\u03b7kl(m\u2206t)) = 1\n\u2206t\u03bbij\u03b4il\u03b4jk\u03b4nm . (13)", "quality_score": 8}}
{"id": "test_71", "question": "How does the time tAB required to reach a given MSD of g change as pB approaches zero?", "golden_answers": ["The time increases by a factor 1 /pA relative to the time tA"], "prediction": "As pB approaches zero, the time tAB required to reach a given MSD of g increases by a factor of 1/pA relative to the time tA required with the pure A dynamics.", "f1": 0.4444444444444444, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01446v1\nBut while it reduces to\nEq. (11) for simple diffusion, it fails for bounded MSD like\nthose of the monomers relative to the ring CM, where af-\nter a suitably delayed time any of our dynamic processes\nwould be predicted to independently contribute an MSD\nof limt\u2192\u221e g2(t) = 2\u27e8R2\ng\u27e9.\nTo arrive at an alternative formulation, we note that\nfor our definition of the relaxation time, \u03c4X = \u27e8R2\ng\u27e9/DX,\nthe null model suggests\n1\n\u03c4AB\n= pA\n\u03c4A\n+ pB\n\u03c4B\n. (A3)\nNow consider\n1\ntAB(g) = pA\ntA(g) + pB\ntB(g) , (A4)\nwhere tX(g) = g\u22121\nX (t) denotes the inverse function of\ngX(t), i.e. the time t required with dynamics X to reach\nan MSD of g. tAB(g) defined by Eq. (A4) and its in-\nverse function gAB(t) are well-behaved in a number of\nlimiting cases. For simple diffusion, gX(t) \u223c DXt so\nthat tX(g) \u223c g/DX, Eq. (A4) reduces to Eq. (11). By\nconstruction, limpB\u21920 tAB(g) = tA(g)/pA: as accepting\nonly a fraction pA of the A moves has the effect to re-\nduce the effectively elapsed time by a corresponding fac-\ntor, the time tAB required to reach a given MSD of g\nincreases by a factor 1 /pA relative to the time tA re-\nquired with the pure A dynamics. In particular, we thus\nfind for the inverse function lim pB\u21920 gAB(t) = gA(pAt)\nas required. Plausibly, if both processes reach g equally\nfast, tA(g) = tB(g), then Eq. (A4) suggests that the time\nrequired by the mixed process\ntAB(g) = tA(g) tB(g)\npAtB(g) + pBtA(g) (A5)\nreduces to tAB(g) = tA(g) = tB(g) for arbitrary pA +\npB = 1. In contrast, if one of the processes reaches g\nmuch faster than the other, tA(g) \u226a tB(g), then over\na wide range of pA-values tAB(g) \u2248 tA(g) pA\nis essentially\ngiven by the time required by the faster process divided\nby the probability to activate the corresponding moves\nin the combined dynamics. Note that this relation holds\nin particular, when the second process by itself never\nreaches g (i.e. when tB(g) = \u221e) and that accord-\ning to Eq. (A4) the mixed process cannot reach MSD,\nwhich none of the pure processes can reach individually\n(tAB(g) = \u221e, if tA(g) = tB(g) = \u221e). Appendix B: Comparison of the null model to\nsimulation data for double-folded rings\nConveniently Eq. (A4) is almost trivial to implement\nfrom a practical point of view, since it is straightforward\n(i) to generate numerically evaluable functions t(g) and\n\n14\n101 103 105 107 109\nt [Ms]\n10\u22124\n10\u22121\n102\n105\ng1(t)\nNm =1000\n(a) null model\npm =0.0\npm =0.5\npm =0.75\npm =0.95\npm = 1.0\n101 103 105 107 109\nt", "metadata": {"type": "reasoning", "source": "2503.01446v1.pdf", "context": "1\n\u03c4AB\n= pA\n\u03c4A\n+ pB\n\u03c4B\n. (A3)\nNow consider\n1\ntAB(g) = pA\ntA(g) + pB\ntB(g) , (A4)\nwhere tX(g) = g\u22121\nX (t) denotes the inverse function of\ngX(t), i.e. the time t required with dynamics X to reach\nan MSD of g. tAB(g) defined by Eq. (A4) and its in-\nverse function gAB(t) are well-behaved in a number of\nlimiting cases. For simple diffusion, gX(t) \u223c DXt so\nthat tX(g) \u223c g/DX, Eq. (A4) reduces to Eq. (11). By\nconstruction, limpB\u21920 tAB(g) = tA(g)/pA: as accepting\nonly a fraction pA of the A moves has the ", "quality_score": 8}}
{"id": "test_72", "question": "How does the expression for CBDW(r) differ from that of CCDW(r)?", "golden_answers": ["The expression for CBDW(r) contains additional terms with X \u03b1 h \u27e8c\u2020 i,\u03b1cj+1,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020 j,\u03b1\u27e9 and other similar terms."], "prediction": "The expression for CBDW(r) includes additional terms involving the differences of operators (Di,A - Di,B) and (Dj,A - Dj,B), while CCDW(r) involves the sums of operators (Zi,A + Zi,B) and (Zj,A + Zj,B). Additionally, CBDW(r) has a more complex structure with multiple terms and factors, including sine functions and a factor of 4, whereas CCDW(r) is simpler and involves a single sine function.", "f1": 0.2285714285714286, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "good title\nSimilarly, we can obtain the connected correlation functions for the CDW and BDW channels as follows:\nCCDW(r) \u2261 (\u22121)i+j\u27e8(Zi,A + Zi,B)(Zj,A + Zj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Zi,A + Zi,B)\u27e9\u27e8(Zj,A + Zj,B)\u27e9\n= ( \u22121)i+j\n\u00ee\n\u27e8c\u2020\ni,Acj,A\u27e9\u27e8ci,Ac\u2020\nj,A\u27e9 + \u27e8c\u2020\ni,Bcj,B\u27e9\u27e8ci,Bc\u2020\nj,B\u27e9\n\u00f3\n= ( \u22121)r+1 2 sin2(\u03c0r/2)\n(\u03c0r)2 , (S10)\nand\nCBDW(r) \u2261 (\u22121)i+j\u27e8(Di,A \u2212 Di,B)(Dj,A \u2212 Dj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Di,A \u2212 Di,B)\u27e9\u27e8(Dj,A \u2212 Dj,B)\u27e9\n= ( \u22121)i+j X\n\u03b1\nh\n\u27e8c\u2020\ni,\u03b1cj+1,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni,\u03b1cj,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj+1,\u03b1\u27e9\n+ \u27e8c\u2020\ni+1,\u03b1cj+1,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni+1,\u03b1cj,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj+1,\u03b1\u27e9\ni\n= 4( \u22121)r+1\n\u00c7\nsin2(\u03c0r/2)\n(\u03c0r)2 + sin[\u03c0(r + 1)/2] sin[\u03c0(r \u2212 1)/2]\n\u03c02(r + 1)(r \u2212 1)\n\u00e5\n. (S11)\nB. Finite-bond and finite-size extrapolations of the correlation functions at the deconfined critical point\nAs shown in Fig. 2(b) in the main text, the bulk-bulk connected correlations are computed by infinite-size DMRG\nsimulations. Although the thermodynamic limit is approached in a natural way, the calculation of the correlation\nfunctions is still affected by the finite-bond effect (or the finite-entanglement effect). To this end, we first perform a\n\n12\n100 300 500\n\u00ac\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nCCDW(r, \u00ac)\nCCDW(r = 15)\nCCDW(r = 25)\nCBDW(r = 15)\nCBDW(r = 25)\n128 192 256\nL\n3\u00a310\u00b05\n4\u00a310\u00b05\n5\u00a310\u00b05\n6\u00a310\u00b05\nCCDW(r, L)\nCCDW(r = 56)\nCCDW(r = 64)\nCBDW(r = 56)\nCBDW(r = 64)0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\nCBDW(r, \u00ac)\n0.010\n0.011\n0.012\n0.013\nCBDW(r, L) <latexit sha1_base64=\"2RRwxLXlY8TROIoM98j2WcOjpro=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlhzI97xVLbsWdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE177Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNi4p3WaneV0u1myyOPJzAKZTBgyuowR3UoQEMBvAMr/DmCOfFeXc+Fq05J5s5hj9wPn8Ai5mNUw==</latexit>\n( a )\n<latexit sha1_base64=\"KWh0RLJ0bw8em/x3PU2+HIlN2FQ=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMQL2FXgnoMevEY0TwgWcLspDcZMju7zMwKIeQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGtzO/9YRK81g+mnGCfkQHkoecUWOlh3Jw3iuW3Io7B1klXkZKkKHeK351+zFLI5SGCap1x3MT40+oMpwJnBa6qcaEshEdYMdSSSPU/mR+6pScWaVPwljZkobM1d8TExppPY4C2xlRM9TL3kz8z+ukJrz2J1wmqUHJFovCVBATk9nfpM8VMiPGllCmuL2VsCFVlBmbTsGG4C2/vEqaFxXvslK9r5ZqN1kceTiBUyiDB1dQgzuoQwMYDOAZXuHNEc6L8+58LFpzTjZzDH/gfP4AjR6NVA==</latexit>\n( b )\nFIG. S4. (a) Each data point of the bulk-bulk correlation function for BDW and CDW orders shown in Fig.2(b) in the main\ntext is obtained by extrapolating to infinite MPS bond dimensions. The dotted and dashed lines are least-squares fittings with\nthe form C(r, \u03c7) = a(100/\u03c7)b + C(r, \u03c7\u2192 \u221e). (b) Each data of the boundary-bulk correlations displayed in Fig.2(c) in the\nmain text is obtained by extrapolating toL \u2192 \u221ewith the fitting form,C(r, L) = a/Lb + C(r, L\u2192 \u221e), whereL is the ladder\nlength. Numerical data are obtained from infinite-size DMRG calculations for (a) and finite-size DMRG calculations for (b)\nwith a sufficiently large bond dimension\u03c7 = 2048 to ensure the convergence of the results forL up to256.\n0 1 2 3 4 5\nk\n0\n1\n2\n3\n4\n5Ek(L)\u00b0E0(L) L= 16\nL= 32\nL= 48\nL= 64\nL= 80\nL= 96\n20 30 40 50 60 70 8090100\nL\n0.5\n0.6\n0.7\n0.8\n0.91.0\n2.0\ne(L) \u00a5E2(L)\u00b0E0(L)\ne(L)\u00aaaL\u00b0z\nz(L)= \u00b0d(lne(L))/d(lnL)\n0.00 0.01 0.02 0.03\n1/L\n0.950\n0.975\n1.000\nz(L)", "metadata": {"type": "reasoning", "source": "2503.01198v1.pdf", "context": "CCDW(r) \u2261 (\u22121)i+j\u27e8(Zi,A + Zi,B)(Zj,A + Zj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Zi,A + Zi,B)\u27e9\u27e8(Zj,A + Zj,B)\u27e9\n= ( \u22121)i+j\n\u00ee\n\u27e8c\u2020\ni,Acj,A\u27e9\u27e8ci,Ac\u2020\nj,A\u27e9 + \u27e8c\u2020\ni,Bcj,B\u27e9\u27e8ci,Bc\u2020\nj,B\u27e9\n\u00f3\n= ( \u22121)r+1 2 sin2(\u03c0r/2)\n(\u03c0r)2 , (S10)\nand\nCBDW(r) \u2261 (\u22121)i+j\u27e8(Di,A \u2212 Di,B)(Dj,A \u2212 Dj,B)\u27e9 \u2212(\u22121)i+j\u27e8(Di,A \u2212 Di,B)\u27e9\u27e8(Dj,A \u2212 Dj,B)\u27e9\n= ( \u22121)i+j X\n\u03b1\nh\n\u27e8c\u2020\ni,\u03b1cj+1,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni,\u03b1cj,\u03b1\u27e9\u27e8ci+1,\u03b1c\u2020\nj+1,\u03b1\u27e9\n+ \u27e8c\u2020\ni+1,\u03b1cj+1,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj,\u03b1\u27e9 + \u27e8c\u2020\ni+1,\u03b1cj,\u03b1\u27e9\u27e8ci,\u03b1c\u2020\nj+1,\u03b1\u27e9\ni\n= 4( \u22121)r+1\n\u00c7\nsin2(\u03c0r/2)\n(\u03c0r)2 + sin[\u03c0(r + 1)/2] sin[\u03c0(r \u2212 1)/2]\n\u03c02(r + 1)(r \u2212 1", "quality_score": 8}}
{"id": "test_73", "question": "What is a key advantage of using DiT and multi-layout-aware diffusion framework for retinal fundus image synthesis?", "golden_answers": ["It captures long-range dependencies, which enhances performance."], "prediction": "A key advantage is the ability to generate diverse yet anatomically accurate retinal images while preserving essential vascular layouts, which enhances segmentation model robustness across diverse imaging conditions and acquisition settings.", "f1": 0.1111111111111111, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01190v2\nCondition embeddings for A V , CD, and L are summed\ninto a single embedding, c. The DiT input consists of a beginning-of-conditioning (BOC) token, user input (UI), c, an end-of-conditioning\n(EOC) token, and the noised image latent. The DiT outputs the corresponding denoised image latent. The UI token specifies whether a\nlayout component is guided by user input or defaults to a neutral embedding when absent.\nartery/vein (A V) segmentation, which also differentiates ar-\nteries from veins. This distinction provides insights into\nvessel-specific pathologies[13, 57]. However, A V segmen-\ntation requires complex annotations, making it challenging\nto obtain sufficient labeled data for robust training. Generative models like GANs and V AEs have been ex-\nplored to address data scarcity in medical imaging [21,\n39]. When applied to retinal images, these models often\nencounter challenges, including difficulties in preserving\nanatomical fidelity and issues with training stability [20].\nDiffusion models have recently emerged as powerful tools\nfor generating diverse high-fidelity images, with superior\nstability and detail preservation, compared to GANs and\nV AEs [8, 27]. Despite their success in image synthesis tasks\nacross domains, e.g., natural image generation and text-to-\nimage modeling, their application in medical imaging has\nlargely focused on generating synthetic images rather than\ndirectly enhancing segmentation performance through data\naugmentation. To address these limitations, we propose Retinal Layout-\nAware Diffusion(RLAD), a diffusion-based framework for\nthe controllable generation of synthetic retinal images (Fig-\nure 1). By conditioning on multiple key retinal struc-\ntures\u2014such as artery/vein (A V), the optic cup/disc (CD),\nand lesions (L)\u2014RLAD preserves essential vascular lay-\nouts while introducing variability in other regions. This en-\nables the creation of paired image-segmentation maps that\nexpand training datasets without compromising structural\nintegrity. Synthetic data generated by RLAD improve seg-\nmentation model robustness across diverse imaging condi-\ntions and acquisition settings. We evaluated RLAD-generated data using state-of-the-\nart visual encoders such as Vision Transformers [9] and\nSwin Transformers [48], and demonstrate consistent im-\nprovements in generalization performance under distri-\nbution shifts (up to 8.1%). Additionally, we introduce\nREYIA, the largest multi-source collection of 586 retinal\nimages with human reference A V segmentation, which not\nonly complements our synthetic data but also demonstrates\nstrong baseline performance, further validating the effec-\ntiveness of our synthetic data. In summary, the main contributions of this work are:\n\u2022 A novel multi-layout-aware generative model ( RLAD)\nthat synthesizes diverse yet anatomically accurate retinal\nimages while preserving semantic structures. \u2022 Demonstrating consistent segmentation performance im-\nprovements across state-of-the-art architectures using\nRLAD-generated data.\n\u2022 Introducing REYIA, the largest multi-source collection\nof datasets for A V-segmented retinal fundus images. 2. Related Work\nRetinal A V segmentation plays a critical role in diagnos-\ning microvascular pathologies [22, 37, 65, 67, 75]. Early\nmethods [24, 31, 68, 81, 82], such as Little W-Net [19], fo-\ncused on compact convolutional neural networks to reduce\ncomputational complexity. More recently, LUNet achieved\nstate-of-the-art performance on optic disc-centered images\nbut struggled to generalize to macula-centered images [16]. This underscores the primary challenge of achieving robust\ngeneralization across diverse retinal imaging conditions.", "metadata": {"type": "factual", "source": "2503.01190v1.pdf", "context": "Transformer-based architectures such as DiT [58] further\nenhance performance by capturing long-range dependen-\ncies.\nBuilding on these developments, we propose a multi-\nlayout-aware diffusion framework specifically designed for\nretinal fundus image synthesis. Unlike prior approaches,\nour method conditions generation on multiple retinal lay-\nout components \u2014A V , CD, and L\u2014extracted from real,\nnon-annotated images using pretrained segmentation mod-\nels. This minimizes error propagation and enhanc", "quality_score": 8}}
{"id": "test_74", "question": "What is the primary motivation behind predicting gene expression from pathology slide images?", "golden_answers": ["To preserve spatial information and make spatial profiling of gene expression more clinically beneficial, cost-effective, and technically efficient."], "prediction": "The primary motivation behind predicting gene expression from pathology slide images is to enhance cancer diagnosis and prognosis by integrating histopathological features with genomic data, thereby improving the understanding of tumor biology and treatment outcomes.", "f1": 0.23529411764705885, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00374v2\n, \u201cA whole-slide foundation model for digital pathology\nfrom real-world data,\u201d Nature, 2024, :1-8. [38] E. V orontsov et al., \u201cVirchow: a million-slide digital pathology founda-\ntion model,\u201d arXiv preprint arXiv:2309.07778 , 2023. [39] N. A. Koohbanani, B. Unnikrishnan, S. A. Khurram, P. Krishnaswamy,\nand N. Rajpoot, \u201cSelf-path: Self-supervision for classification of pathol-\nogy images with limited annotations,\u201d IEEE Trans. Med. Imaging, 2021,\n40(10):2845-56. [40] K. Singhal et al., \u201cLarge language models encode clinical knowledge,\u201d\nNature, 2023, 620(7972):172-80. [41] M. Y . Lu et al., \u201cA visual-language foundation model for computational\npathology,\u201d Nature Medicine, 2024, 30(3):863-74. [42] C. Li et al., \u201cLlava-med: Training a large language-and-vision assistant\nfor biomedicine in one day,\u201d NeurIPS, 2024, 36. [43] G. Jaume et al., \u201cMultistain pretraining for slide representation learning\nin pathology,\u201d arXiv preprint arXiv:2408.02859 , 2024. [44] Z. Guo, J. Ma, Y . Xu, Y . Wang, L. Wang, and H. Chen, \u201cHistgen:\nHistopathology report generation via local-global feature encoding and\ncross-modal context interaction,\u201d in MICCAI, 2024, :189-99. [45] R. J. Chen et al. , \u201cPathomic fusion: an integrated framework for\nfusing histopathology and genomic features for cancer diagnosis and\nprognosis,\u201d IEEE Trans. Med. Imaging , 2020, 41(41):757-70. [46] Y . Zheng et al., \u201cGraph attention-based fusion of pathology images and\ngene expression for prediction of cancer survival,\u201d IEEE Trans. Med. Imaging, 2024, 43(9):3085-97. [47] Z. Wang, Y . Zhang, Y . Xu, S. Imoto, H. Chen, and J. Song, \u201cHisto-\ngenomic knowledge association for cancer prognosis from histopathol-\nogy whole slide images,\u201d IEEE Trans. Med. Imaging , 2025, :1. [48] Z. Shao et al., \u201cTransmil: Transformer based correlated multiple instance\nlearning for whole slide image classification,\u201d NeurIPS, 2021, 34:2136-\n47. [49] I. Guyon, J. Weston, S. Barnhill, and V . Vapnik, \u201cGene selection for\ncancer classification using support vector machines,\u201d Machine learning,\n2002, 46:389-422. [50] Z. Sondka et al. , \u201cCosmic: a curated database of somatic variants and\nclinical data for cancer,\u201d Nucleic Acids Research, 2024, 52(D1):D1210-\n7. [51] A. v. d. Oord, Y . Li, and O. Vinyals, \u201cRepresentation learning with\ncontrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748 , 2018. [52] S. Kullback and R. A. Leibler, \u201cOn information and sufficiency,\u201d The\nannals of mathematical statistics , 1951, 22(1):79-86. [53] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in CVPR, 2016, :770-8. [54] A. Filiot et al., \u201cScaling self-supervised learning for histopathology with\nmasked image modeling,\u201d medRxiv, 2023, :2023-07. [55] M. J. Goldman et al. , \u201cVisualizing and interpreting cancer genomics\ndata via the xena platform,\u201d Nat. Biotechnol., 2020, 38(6):675-8. [56] G. Campanella et al. , \u201cClinical-grade computational pathology using\nweakly supervised deep learning on whole slide images,\u201d Nat. Med. ,\n2019, 25(8):1301-09. [57] N. Otsu et al. , \u201cA threshold selection method from gray-level his-\ntograms,\u201d Automatica, 1975, 11(285-296):23-7. [58] D. Kingma, \u201cAdam: a method for stochastic optimization,\u201d arXiv\npreprint arXiv:1412.6980, 2014. [59] A. Paszke et al., \u201cPytorch: An imperative style, high-performance deep\nlearning library,\u201d NeurIPS, 2019, 32. [60] M. Ilse, J. Tomczak, and M. Welling, \u201cAttention-based deep multiple\ninstance learning,\u201d in ICML, 2018, :2127-36.", "metadata": {"type": "factual", "source": "2503.01347v1.pdf", "context": "gate values within spots of interest to predict the gene expression. Our\nPixNet outperforms state-of-the-art methods on 3 common ST datasets,\nwhile showing superior performance in predicting gene expression across\nmultiple spatial scales. The source code will be publicly available.\nKeywords: Spatial transcriptomics\u00b7 Computational pathology\u00b7 Gene\nexpression prediction\u00b7 Tissue slide image\u00b7 Pixel-level prediction.\n1 Introduction\nSpatially profiling gene expression with spatial transcriptomics (ST) ", "quality_score": 8}}
{"id": "test_75", "question": "Why is it necessary to consider memory usage, computation requirements, and mean latency in the evaluation?", "golden_answers": ["Because these factors are crucial for understanding the overall efficiency and feasibility of the proposed approach."], "prediction": "It is necessary to consider memory usage, computation requirements, and mean latency in the evaluation to ensure a balanced distribution of layers, optimize the use of device memory, reduce load imbalance, and achieve efficient performance in model segmentation, ultimately leading to improved speedups and resource utilization.", "f1": 0.07017543859649122, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01025v1\nIn this sense, Google\noffers a profiling tool that tests the latency of the fragments\nfor different distributions and tries to minimise the difference\nbetween the fastest and the slowest one. We will shortly\nanalyse a profiling-based split, but let us first test the default\nsegmentation scheme with a larger input batch. B. Performance on batched inputs with segmented models\nTo exploit the parallel potential of the pipeline, we repeated\nthe experiment with a 50-input batch. In this case, we divide\nthe execution time of the whole batch by its size to obtain the\ntime per inference. Figure ?? reports the speedup with respect\nto a single input and with respect to one TPU. In both layer types, the speedup with respect to a single\ninput is far from ideal (with n TPUs we could expect close\nto \u00d7n). The problem is that the workload distribution is\nunbalanced and there are stages much slower than others\nacting as bottlenecks. When the model fits in device memory\nthe speedup is moderately below ideal because the workload\ndistribution is uneven. When a TPU also needs host memory,\nit becomes a very slow stage in the pipeline that sequences\nthe executions. Therefore, the speedup with respect to a single\ninput drops sharply near \u00d71 when host memory is needed. These load imbalance issues combined with the commu-\nnication overhead make speedups compared to a single TPU\nvery poor for CONV models and for FC models that fit on-\nchip or still need host memory. Recall that in CONV layers the\ncommunication costs are very relevant on their own and it is\nnatural that, despite parallelisation, segmentation is inefficient\n(in many models it is still slower than 1 TPU). However, the\nresults are very positive for FC models that avoid the use\nof host memory completely (because its relative cost is very\nhigh). In these cases, we obtain speedups of several tens with\njust 2, 3 or 4 TPUs compared to using just one. We get up to\n\u00d736 in the largest models (with just 4 TPUs) where the size\nof the layers is already too big to be stored on the host. C. Optimizing model segmentation with profiling\nWe have observed two different issues to improve segmenta-\ntion through a better model distribution. First, it is convenient\nto make a better use of device memory with a more balanced\ndistribution of layers in terms of their size. In this way, we will\nachieve a further reduction in host memory usage for the same\nnumber of devices. Second, it is desirable for the workload to\nbe more evenly distributed so that the pipeline stages exhibit\nsimilar latencies and hence the load imbalance is reduced. In our synthetic models, both purposes match, since all\nlayers are of the same type (they are all either FC or CONV ). As they all exhibit the same arithmetic intensity, a higher\nworkload entails a higher memory usage.", "metadata": {"type": "reasoning", "source": "2503.01353v1.pdf", "context": "This section is organized as follows. Section IV-A details\nthe datasets used for the evaluation. Section IV-B evaluates\nthe classification capability. Section IV-C evaluates the per-\nformances of adding a new task during the operational life\nof HAR. Finally, Section IV-D discusses the memory usage,\ncomputation requirements, and mean latency.\nA. Datasets\n1) UCA-EHAR dataset: The UCA-EHAR dataset [12] in-\ncludes gyroscopic and accelerometer data collected from smart", "quality_score": 8}}
{"id": "test_76", "question": "Derive the expression for I(1)2m\u22121,i using calculus and explain the steps involved.", "golden_answers": ["I(1)2m\u22121,i = (m\u2212i\u2211 l=0 ((\u22121)m\u2212i\u2212lCl m\u2212i 2m\u2212l)) \u222b 1 0 y2m\u22121 1(y1 + 1 j)2m dy1 + O(1), where the identity m\u2212i\u2211 l=0 (\u22121)lCl m\u2212i m+ i+ l = 1/2mCm\u2212i 2m\u22121 is used."], "prediction": "I(1)2m\u22121,i = (m\u2212i\u2211 l=0 (\u22121)m\u2212i\u2212lCl m\u2212i 2m\u2212 l) \u222b 1 0 y2m\u22121 1 (y1 + 1 j )2m dy1 + O(1)", "f1": 0.5652173913043479, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01246v1\nWe proceed to analyze the integral I2m\u22121 by the explicit expression of \u2126 and P2m.\nIt follows that\nI2m\u22121 =\nm\u2211\ni=0\na2m, 2i\n\u222b\n\u2126\n1\n|y|\n(y1 \u2212 1\nj )2i(y2\n2 + y2\n3)m\u2212i\n|y\u2212 zj|4m+1 y2m\u22121\n1 dy\n= 2 \u03c0\nm\u2211\ni=0\na2m, 2i\n\u222b 0\n\u22121\n\u222b 1\n0\n1\u221a\ny2\n1 + r2\n(y1 \u2212 1\nj )2ir2(m\u2212i)\n\u221a\n(y1 \u2212 1\nj )2 + r2\n4m+1 y2m\u22121\n1 rdrdy1\n= \u2212\u03c0\nm\u2211\ni=0\na2m, 2i\n\u222b 1\n0\n\u222b 1\n0\n1\u221a\ny2\n1 + r\n(y1 + 1\nj )2irm\u2212i\n\u221a\n(y1 + 1\nj )2 + r\n4m+1 y2m\u22121\n1 drdy1. We spilt I2m\u22121 into two parts. For 0 \u2264 i\u2264 m, de\ufb01ne\nI(1)\n2m\u22121,i :=\n\u222b 1\n0\n\u222b 1\n0\n(y1 + 1\nj )2irm\u2212i\n((y1 + 1\nj )2 + r)2m+1 y2m\u22121\n1 drdy1,\nI(2)\n2m\u22121,i :=\n\u222b 1\n0\n\u222b 1\n0\n\uf8eb\n\uf8ed 1\n\u221a\ny2\n1 + r\n\u2212 1\u221a\n(y1 + 1\nj )2 + r\n\uf8f6\n\uf8f8 (y1 + 1\nj )2irm\u2212i\n\u221a\n(y1 + 1\nj )2 + r\n4m+1 y2m\u22121\n1 drdy1. Clearly, I2m\u22121 = \u2212\u03c0\u2211 m\ni=0 a2m, 2i(I(1)\n2m\u22121,i + I(2)\n2m\u22121,i ). From the basic knowledge of\ncalculus, it is seen that\nI(1)\n2m\u22121,i =\nm\u2212i\u2211\nl=0\n\u222b 1\n0\n(y1 + 1\nj)2(m\u2212l)y2m\u22121\n1\n(\u222b (y1+ 1\nj )2+1\n(y1+ 1\nj )2\n(\u22121)m\u2212i\u2212lCl\nm\u2212i\n1\ns2m+1\u2212l ds\n)\ndy1\n=\n(m\u2212i\u2211\nl=0\n(\u22121)m\u2212i\u2212lCl\nm\u2212i\n2m\u2212 l\n) \u222b 1\n0\ny2m\u22121\n1\n(y1 + 1\nj )2m dy1 + O(1)\n=\n(m\u2212i\u2211\nl=0\n(\u22121)lCl\nm\u2212i\nm+ i+ l\n)[ \u222b 1\nj +1\n1\nj\n2m\u22121\u2211\nl=0\nCl\n2m\u22121\n(\n\u2212 1\nj\n) 2m\u22121\u2212l 1\ns2m\u2212l ds\n]\n+ O(1)\n= 1\n2mCm\u2212i\n2m\u22121\nln j+ O(1),\nwhere we note that the identity\nm\u2212i\u2211\nl=0\n(\u22121)lCl\nm\u2212i\nm+ i+ l = 1\n2mCm\u2212i\n2m\u22121\n\n10 C. Wu and J. Yang\nholds. Further, direct calculation shows that\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n1\n\u221a\ny2\n1 + r\n\u2212 1\u221a\n(y1 + 1\nj )2 + r\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n=\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n1\nj (2y1 + 1\nj )\n\u221a\ny2\n1 + r\n\u221a\n(y1 + 1\nj )2 + r(\n\u221a\ny2\n1 + r+\n\u221a\n(y1 + 1\nj )2 + r)\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u23d0\n\u2264 1\nj\n1\u221a\ny2\n1 + r\n1\u221a\n(y1 + 1\nj )2 + r\nfor ( y1,r ) \u2208 (0, 1)2, which implies\n|I(2) 2m\u22121,i | \u2264 1\nj\n\u222b 1\n0\n\u222b 1\n0\n1\u221a\ny2\n1 + r\n(y1 + 1\nj )2irm\u2212i\n((y1 + 1\nj )2 + r)2m+1 y2m\u22121\n1 drdy1\n\u2264 1\nj\n\u222b 1\n0\n\u222b 1\n0\n(y1 + 1\nj )2irm\u2212i\n((y1 + 1\nj )2 + r)2m+1 y2m\u22122\n1 drdy1.", "metadata": {"type": "reasoning", "source": "2503.01246v1.pdf", "context": "Boundary determination for the Schr\u00a8 odinger equation 9\ndeduced that\nI2m\u22121 =\n\u222b\n\u2126\n1\n|y|\nP2m(y1 \u2212 1\nj ,\n\u221a\ny2\n2 + y2\n3)\n|y\u2212 zj|4m+1 y2m\u22121\n1 dy,\nI2m = 2 mI2m\u22121 +\n\u222b\n\u2126\n1\n|y|\nP2m+1(y1 \u2212 1\nj ,\n\u221a\ny2\n2 + y2\n3)\n|y\u2212 zj |4m+3 y2m\n1 dy.\nWe proceed to analyze the integral I2m\u22121 by the explicit expression of \u2126 and P2m.\nIt follows that\nI2m\u22121 =\nm\u2211\ni=0\na2m, 2i\n\u222b\n\u2126\n1\n|y|\n(y1 \u2212 1\nj )2i(y2\n2 + y2\n3)m\u2212i\n|y\u2212 zj|4m+1 y2m\u22121\n1 dy\n= 2 \u03c0\nm\u2211\ni=0\na2m, 2i\n\u222b 0\n\u22121\n\u222b 1\n0\n1\u221a\ny2\n1 + r2\n(y1 \u2212 1\nj )2ir2(m\u2212i)\n\u221a\n(y1 \u2212 1\nj )2 + r2\n4m+1 ", "quality_score": 8}}
{"id": "test_77", "question": "How do the transformation property in equation (8.3) and Lemmas 7.9 and 7.12 imply that there is a canonical injective map from H0(MY,Hr k,Y) to WN \u2264r k (\u0393Y(I))?", "golden_answers": ["The transformation property shows that any section s \u2208 H0 (\u2126, Sym r(HDR(\u03a8 Y)) \u2297 \u03c9(\u03a8 Y)\u2297(k\u2212r))\u0393 Y(I) satisfies a certain transformation equation under the action of \u0393Y(I), which is consistent with the definition of WN \u2264r k (\u0393Y(I)). Lemmas 7.9 and 7.12 provide the necessary conditions for this map to be injective."], "prediction": "The transformation property in equation (8.3) along with Lemmas 7.9 and 7.12 imply that there is a canonical injective map from H0(MY,Hr k,Y) to WN \u2264r k (\u0393Y(I)) by establishing the necessary isomorphisms and structures that allow for the mapping of sections of the sheaf H0(MY,Hr k,Y) into the space of modular forms WN \u2264r k (\u0393Y(I)). Specifically, Lemma 7.9 provides the isomorphism between \u03c0\u2217Y(\u03c9Y) and O\u2126, which is crucial for relating the sheaf cohomology to the modular forms. Furthermore, Lemma 7.12 shows how the symmetrization and tensoring with \u03c9(\u03a8 Y) respects the \u0393Y(I)-action, ensuring that the resulting sections can be identified with those in WN \u2264r k (\u0393Y(I)). Thus, the injective map arises from the compatibility of these structures under the action of the group \u0393Y(I).", "f1": 0.36477987421383645, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01357v1\nPart (ii) follows from (i) and the\nfunctional equation ( 4.8) of the false Eisenstein series E. \u25a1\nObserve that O\u2126 \u03b71 \u2282 HDR(\u03a8 Y) is a \u0393 Y(I)-subsheaf. Before we state our next lemma,\nrecall that \u03c9un = Lie( Eun\nI )\u2228 and \u03c9Y is its restriction to MY, after base change with C\u221e.\nLemma 7.9. We have \u03c0\u2217\nY(\u03c9Y) \u223c= O\u2126 \u03b71 as \u0393Y(I)-sheaves.\nProof. We aim to show that \u03c0\u2217\nY(\u03c9Y) is isomorphic to O\u2126 with a \u0393 Y(I)-structure given by\nmultiplication by j(\u03b3,\u2212)\u22121. Recall the notation in \u00a72.8 and let g \u2208 GL2( \u02c6A) be such that\n\u03c0g = \u03c0Y. By Proposition\n2.40, we see that \u03c0\u2217\nY(\u03c9Y) \u223c= O\u2126 . Using the commutative diagram\nin the proof of [ BBP21, Lem. 10.5], we obtain\n\u03c0\u2217\nY(\u03c9Y) O\u2126\n\u03b3\u2217\u03c0\u2217\nY(\u03c9Y) \u03b3\u2217O\u2126 = O\u2126 . \u223c\n= multiplication by j(\u03b3,\u2212)\n\u223c\nHence under the trivialization \u03c0\u2217\nY(\u03c9Y) \u223c= O\u2126 , we see that \u03b3\u2217O\u2126 = O\u2126 \u2192 O \u2126 is the map\ngiven by the multiplication by j(\u03b3,\u2212)\u22121. Equivalently, the map O\u2126 \u2192 \u03b3\u2217O\u2126 = O\u2126 obtained\nby adjunction, is also the multiplication by j(\u03b3,\u2212)\u22121, \ufb01nishing the proof of the lemma. \u25a1\nDe\ufb01nition 7.10. We de\ufb01ne HDR(\u03a8\nY\n) := HDR(\u03a8 Y)\u0393 Y(I). Furthermore, we set \u03c9(\u03a8 Y) :=\nO\u2126 \u03b71 and \u03c9(\u03a8\nY\n) := \u03c9(\u03a8 Y)\u0393 Y(I).\nRemark 7.11. Note that by Corollary 2.35, HDR(\u03a8\nY\n) = i\u2217\nY(HDR,Y) = Han\nDR,Y. Similarly\n\u03c9(\n\u03a8\nY\n) = \u03c9an\nY . We conclude this section with the following useful lemma. Lemma 7.12. Let r and k be non-negative integers so that k \u2265 r. Then there exists a\ncanonical isomorphism\nSymr(HDR(\u03a8\nY\n)) \u2297 \u03c9(\u03a8\nY\n)\u2297(k\u2212r) \u2243 (Symr(HDR(\u03a8 Y)) \u2297 \u03c9(\u03a8 Y)\u2297(k\u2212r))\u0393 Y(I). Proof. By de\ufb01nition, we have\nSymr(HDR(\u03a8\nY\n)) \u2297 \u03c9(\u03a8\nY\n)\u2297(k\u2212r) \u2243 Symr(HDR(\u03a8 Y)\u0393\nY(I)) \u2297 (\u03c9(\u03a8 Y)\u0393 Y(I))\u2297(k\u2212r). By Corollary\n2.35, applying () \u0393 Y(I) commutes with Sym r and \u2297, and hence we obtain the\nresult. \u25a1\n\n46 O \u02d8GUZ GEZM \u02d9IS \u00b8 AND SRIRAM CHINTHALAGIRI VENKATA\n8. Algebraic nearly holomorphic Drinfeld modular forms\nFor the convenience of the reader, we recall our notation from \u00a76. Let Y be the projective\nA-module of rank two embedded in K2 by Y = g(1,0) + h(0,1) \u2282 K2 for some fractional\nideals g and h of A, which are, without loss of generality, assumed to be integral ideals of\nA. Let I be an ideal of A such that |V(I)| \u2265 2. Let M2\nI,C\u221e = Spec( C\u221e) \u00d7Spec(A) M2\nI and\nset ( M2\nI,C\u221e )an to be the analyti\ufb01cation of M2\nI,C\u221e . Let MY \u2282 (M2\nI,C\u221e )an be the connected\ncomponent of M2\nI,C\u221e so that MY(C\u221e) = \u0393 Y(I) \\ \u2126 and set iY : MY \u2192 M2\nI,C\u221e to be the\ncorresponding inclusion. We also \ufb01x an embedding H \u2192 C\u221e so that MY = M2\nI \u00d7H C\u221e and\nMY := M2\nI \u00d7H C\u221e. We further denote by \u03c9Y the restriction of \u03c9un, after base change with\nC\u221e, to MY.", "metadata": {"type": "reasoning", "source": "2503.01357v1.pdf", "context": "describe the nearly holomorphic Drinfeld modular forms as the global sections of the sheaf\nHr\nk := Sym r(HDR,un) \u2297 \u03c9un\u2297(k\u2212r) pulled back to the appropriate component of M2\nI,C\u221e . To\nachieve our goal, in what follows, we \ufb01rst denote by HDR,Y (HDR,Y resp.) the pull back of\nHDR,un (HDR,un resp.), after base change with C\u221e, to MY (MY resp.) Moreover, we let\nHr\nk,Y := Sym r(HDR,Y) \u2297 \u03c9\u2297(k\u2212r)\nY and Hr\nk,Y := Sym r(HDR,Y) \u2297 \u03c9\u2297(k\u2212r)\nY .\nLet WN \u2264r\nk (\u0393Y(I)) be the C\u221e-vector space of weak nearly holomorphi", "quality_score": 8}}
{"id": "test_78", "question": "Do the speakers interrupt each other in the provided audio?", "golden_answers": ["No, they do not interrupt each other."], "prediction": "Yes, the speakers interrupt each other in the provided audio.", "f1": 0.39999999999999997, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01174v1\nThe second speaker\u2019s overlap is a mistimedattempt to start their turn, and the first speaker willcontinue, forcing the second speaker to wait for anatural break or pause before speakingQwen2-Audio-Instruct\nB\nInterruption Qwen-Audio-Chat Based solely on the provided audio, it is morelikely that the second speaker\u2019s overlap is a suc-cessful interruption, and they will take over theconversation. This is because the audio endsabruptly as the second speaker begins to speak,suggesting that the first speaker was interrupted. Additionally, the second speaker\u2019s tone soundsconfident and assertive, which is characteristic ofsomeone who is trying to take over a conversation. On the other hand, the first speaker\u2019s response isshort and does not indicate that they are about topause or continue speaking. Whisper+GPT-4o During spontaneous human conversation, inter-ruptions occur frequently and often correspond tobreaks in the information flow between conver-sation partners. Interruptions can happen due toa variety of reasons like urgency, disagreement,etc. Interruptions can be broadly classified intotwo categories: competitive and cooperative in-terruptions. Occurrences of competitive interrup-tions are usually disrupting to the flow of con-versation between speakers while cooperative in-terruptions are more supportive to the flow. Youare examining a short audio clip from a conver-sation between two people. The audio ends sud-denly RIGHT as the second speaker begins to talk,overlapping with the first speaker. It\u2019s impossibleto tell from this brief excerpt if the first speakerwas about to pause, was having trouble finding aword, or would have continued speaking. BasedONLY on the provided audio, is it more likelythat: (a) The second speaker\u2019s overlap is a suc-cessful interruption, and they will take over theconversation? (b) The second speaker\u2019s overlap isa mistimed attempt to start their turn, and the firstspeaker will continue, forcing the second speakerto wait for a natural break or pause before speak-ing? Justify your choice, considering the fac-tors that might lead to a successful interruptionor encourage someone to wait their turn, but fo-cus on how the SPECIFIC TIMING of this over-lap makes one outcome more likely than the other. Based on the provided audio excerpt, it is morelikely that: (b) The second speaker\u2019s overlap is amistimed attempt to start their turn, and the firstspeaker will continue, forcing the second speakerto wait for a natural break or pause before speak-ing. Justification: 1. **Context and Flow**: Thefirst speaker is in the middle of a detailed expla-nation about weather patterns and their impact onsnowfall. They have not yet reached a clear con-clusion or natural pause in their narrative. Thesentence\u00a8But I have played\u00a8suggests that the firstspeaker is about to continue with a related pointor anecdote. 2. **Timing of Overlap**: The over-lap occurs just as the first speaker is transitioningto a new but related point (\u00a8But I have played\u00a8).This timing suggests that the first speaker is notfinished and is likely to continue speaking. Thesecond speaker\u2019s attempt to interject at this mo-ment is mistimed because it interrupts the flow ofthe first speaker\u2019s ongoing thought. 3. **Con-versational Dynamics**: In spontaneous conver-sation, speakers often signal the end of their turnwith a clear pause or a concluding statement.", "metadata": {"type": "factual", "source": "2503.01174v1.pdf", "context": "with (a) Yes or (b) No.\n(b) No\nInterruption\nSALMONN Do the 2 speakers interrupt\neach other anywhere in the\nprovided audio? Briefly\nexplain your answer. You\nmust reply with (a) Yes or\n(b) No.\nNo, the 2 speakers\ndo not interrupt each\nother in the provided\naudio.\nQwen2-\nAudio-\nInstruct\nNo, the speakers do\nnot interrupt each\nother. They speak at\ndifferent times and\ntheir overlapping is\nminimal.\nQwen-Audio-\nChat\nYes, the two speakers\ninterrupt each other.\nWhisper+GPT-\n4o\nDo the 2 speakers interrupt\ne", "quality_score": 8}}
{"id": "test_79", "question": "What is preserved by any homomorphism of Kripke models?", "golden_answers": ["The tree order of a synchronization tree."], "prediction": "The tree order of a synchronization tree.", "f1": 1.0, "em": 1.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01247v1\n\u2a7d k with root a, and\n(M) for all x, y\u2208 A, x \u227a y if and only if RA(x, y) for some unique R \u2208 \u03c3. \u2022 morphisms: homomorphisms of Kripke models. 1Equivalently, it is a monotone map that preserves the height of elements. 10 SAMSON ABRAMSKY, THOMAS LAURE, AND LUCA REGGIO\nNote that the tree order of a synchronization tree is \u201cdefinable\u201d and hence preserved\nby any homomorphism of Kripke models. Thus the forgetful functor Lk : EM(Mk) \u2192\nStruct\u2022(\u03c3) is fully faithful and can be identified with the inclusion into Struct\u2022(\u03c3) of\nthe full subcategory defined by synchronization trees of height \u2a7d k. Its right adjoint\nFk : Struct\u2022(\u03c3) \u2192 EM(Mk) sends a Kripke model to its k-unravelling.\nRemark 3.5. In fact, the modal comonads Mk are idempotent, meaning that their\ncomultiplications \u03b4 are natural isomorphisms. In turn, idempotent comonads on a\ncategory C correspond precisely to coreflective subcategoriesof C, i.e. full subcategoriesD\nsuch that the inclusion functor D ,\u2192 C has a right adjoint. 3.3. Logical equivalences. In [2, 6] it was shown how game comonads can be used to\ncapture, in a categorical fashion, preservation of various fragments and extensions of the\nlogics FOk, Lk and MLk. We recall here the case of the existential positive fragments\nand the full fragments, and outline the way in which the equality symbol is handled in\nthe framework of game comonads. Existential positive fragments. Given a collection of formulas L \u2286 L\u221e,\u03c9, consider its\nexistential positive fragment\n\u2203+L\nconsisting of formulas with no universal quantifiers and no negations; for modal logic,\nthis corresponds to barring the use of modalities \u25a1R and negations. The next result\nshows that preservation of existential positive fragments is captured by the homomorph-\nism preorder in the Eilenberg\u2013Moore category for the corresponding comonad (for a\nproof, cf. [2, 6] and also [7]).2 Recall that, for a comonad Gk on a category C, we denote\nby Fk : C \u2192 EM(Gk) the right adjoint to the forgetful functor Lk : EM(Gk) \u2192 C.\nTheorem 3.6. The following are equivalent for all (pointed) structures A and B:\n(i) A \u21db\u2203+FOk B if, and only if, there exists an arrow FkA \u2192 FkB in EM(Ek). (ii) A \u21db\u2203+Lk B if, and only if, there exists an arrow FkA \u2192 FkB in EM(Pk).\n(iii) (A, a) \u21db\u2203+MLk (B, b) if, and only if, there exists an arrow Fk(A, a) \u2192 Fk(B, b)\nin EM(Mk). The equality symbol. In their basic form, game comonads capture fragments of logics\nwithout equality. This is sufficient for modal logic, since the image of the standard\ntranslation is contained in the equality-free fragment of FO, and was also sufficient for\nTheorem 3.6, since the fragments \u2203+FOk and \u2203+Lk admit equality elimination. To\nmodel logics with equality, such as the fragments FOk and Lk, we proceed as follows. Given a relational vocabulary \u03c3, consider a fresh binary relation symbol I and the\nexpanded vocabulary\n\u03c3I := \u03c3 \u222a {I}.", "metadata": {"type": "factual", "source": "2503.01247v2.pdf", "context": "10 SAMSON ABRAMSKY, THOMAS LAURE, AND LUCA REGGIO\nNote that the tree order of a synchronization tree is \u201cdefinable\u201d and hence preserved\nby any homomorphism of Kripke models. Thus the forgetful functor Lk : EM(Mk) \u2192\nStruct\u2022(\u03c3) is fully faithful and can be identified with the inclusion into Struct\u2022(\u03c3) of\nthe full subcategory defined by synchronization trees of height \u2a7d k. Its right adjoint\nFk : Struct\u2022(\u03c3) \u2192 EM(Mk) sends a Kripke model to its k-unravelling.\nRemark 3.5. In fact, the modal comonads M", "quality_score": 8}}
{"id": "test_80", "question": "What type of learning does the incorporation of control parameters achieve in this model?", "golden_answers": ["The incorporation of specific actions (control parameters) achieves tailored learning for a specific domain or machine setup"], "prediction": "The incorporation of control parameters achieves a form of probabilistic learning, specifically resembling Thompson sampling adapted to the framework discussed.", "f1": 0.37500000000000006, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "The Learning Approach to Games\nJust as a child starts with limited\ncapacities and gradually acquires a rich repertoire, such dynamic structural growth might\nindeed be essential for modeling higher-order intelligence. While this remains far beyond\ncurrent work, it represents a compelling direction for future research. As the player faces the optimization problem (2.6), it is not always feasible to solve for\nthe optimal control. When \u02c6T = 1, the problem might be relatively simple, allowing for\nstraightforward searches for \u03f5-optimal actions. However, for longer horizons, the space of\npotential controls becomes excessively large, complicating the search for optimal solutions. To formalize this, let us first define\n\u03b1 =t,x;i \u02dc\u03b1 if \u03b1(s, y) = \u02dc\u03b1(s, y) \u2200s \u2208 {t, . . . , t+ \u02c6Ti(t, x) \u2212 1}, y \u2208 Ss\nUnder this equivalency relation, we introduce the quotient space\nAt,x;i := Ai/ =t,x;i\nAnd then, to incorporate the potential difficulty and uncertainty in identifying the optimal\ncontrol, we introduce the next learning parameter;\n\u02c6\u03c0 : \u02c6\u2126 \u00d7 T \u00d7 \u2126 \u2192 P(Ai) where ,\n\u02c6\u03c0(\u02c6\u03c9, t, x)(d\u03b1) = \u02c6\u03c0(\u02c6\u03c9, t, x)(d\u02dc\u03b1) whenever \u03b1 =t,x;i \u02dc\u03b1\n(2.7)\nHere, at (\u02c6\u03c9, t, x), \u02c6\u03c0 approximates the potential optimal controls forJ(\u02c6\u03c9, t, x,\u00b7), which will be\ndictated by the equilibrium condition below. Note that, the equality should be understood\nin terms of two subsets of controls having equal probabilities, when they are equal once\nextended with the relation = t,x;i. Also, we didn\u2019t suppress \u02c6\u03c9 as it will be integrated. 9\n\nNow, the crucial observation is that, even when optimal control can be solved exactly,\nuncertainty over the value will naturally induce a probability distribution over controls. That is, given ( \u02c6T ,\u02c6p, \u02c6\u0393, \u02c6F ,\u02c6\u03d5, \u02c6\u03c0), we define \u03a5t,x;i \u2208 P(At,x;i) as\n\u03a5t,x;i(d\u03b1) :=\nZ\n\u02c6\u2126\n\u02c6\u03c0(\u02c6\u03c9, t, x)(d\u03b1)\u02c6P(d\u02c6\u03c9) (2.8)\nWe have introduced distributions over controls, which then further induces distribution over\nactions:\n\u03b3t,x;i(da) := \u03a5t,x;i\u0000\n\u03b1 : \u03b1(t, x) = da\n\u0001\n\u2208 P(At,x;i) (2.9)\nAlthough apriori \u03a5t,x;i appears to be solely induced by \u02c6\u03c0, \u02c6\u03c0 itself is a function of the value\nJi, and hence \u03a5t,x;i is a function from MT \u00d7Mp\u00d7M\u0393\u00d7MF \u00d7M\u03d5\u00d7M\u03c0. We introduced \u02c6\u03c0 to\ncarry out the discussions more explicitly.2 In the two-player example discussed in section 2.1,\n\u02c6\u03c0 solves the optimization by brute force, yielding a deterministic action for each scenario\nin \u02c6\u2126. In the control problem described in section 4, \u02c6 \u03c0 constructs the distribution over\nstrategies relying again on the associated values. Let us note that we induce a distribution\nover controls because it is a more familiar and convenient choice; however, by considering\nsequences of future states, one can easily induce a distribution over sequences of actions. This behavior, represented by \u02c6\u03c0, is well suited for the learning or playing phase and is,\nin essence, Thompson sampling adapted to our framework. During a competition phase,\none might modify \u02c6\u03c0 to be deterministic, selecting the control corresponding to the mode\nof the value. In particular situations, such as performing surgery, it is not only wrong but\nalso unethical to forgo the most likely action and instead select one at random.", "metadata": {"type": "factual", "source": "2503.01411v2.pdf", "context": "and reliability of the machine parameters and whether these\nparameters are truly indicative of the underlying process the\nmodel aims to capture. Unlike traditional SSL methods (e.g.,\ncontrastive learning or I-JEPA), which aim to learn general-\npurpose representations, this model is tailored to a specific\ndomain or machine setup by incorporating a predictive mod-\nule. The incorporation of specific actions (control parameters)", "quality_score": 8}}
{"id": "test_81", "question": "What is explicitly known about the number nI?", "golden_answers": ["nI is a finite number which can be described explicitly."], "prediction": "The document does not provide explicit information about the number nI.", "f1": 0.2222222222222222, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01026v1\n(3)\n3\n\nWe now obtain some explicit estimates for the quantities Ni+j \u2212 \u03b1jNi. These will be\nuseful later in the paper. For k \u2265 0 we have\nNi+k \u2212 \u03b1kNi = (Ni+k \u2212 c1\u03b1i+k) \u2212 \u03b1k(Ni \u2212 c1\u03b1i),\nand so by the triangle inequality and (3) we get\n|Ni+k \u2212 \u03b1kNi| \u2264 |Ni+k \u2212 c1\u03b1i+k| + \u03b1k|Ni \u2212 c1\u03b1i|\n\u2264 2|c2|(|\u03b2|k + \u03b1k)|\u03b2|i. (4)\nClosed forms for \u03b1, \u03b2, \u03b3, c1, c2, c3 were given by Lin [31], and from these we can easily\ncompute approximations to their values, as follows:\n\u03b1 .= 1.46557123187676802665673\n\u03b2 .= \u22120.2327856159383840133283656 + 0.7925519925154478483258983i\n\u03b3 .= \u22120.2327856159383840133283656 \u2212 0.7925519925154478483258983i\n|\u03b2| = |\u03b3| .= 0.826031357654186955968987\nc1 = \u03b15/(\u03b13 + 2) .= 1.313423059852349798783263\nc2 = \u03b25/(\u03b23 + 2) .= \u22120.15671152992617489939163 \u2212 0.001340333618411808095189i\nc3 = \u03b35/(\u03b23 + 2) .= \u22120.15671152992617489939163 + 0.001340333618411808095189i\n|c2| = |c3| .= 0.15671726167213060374568596. Now for k = 1, 2, 3, using (4) and the numerical approximations above, we get\n|Ni+1 \u2212 \u03b1Ni| < 0.71827(0.82604)i (5)\n|Ni+2 \u2212 \u03b12Ni| < 0.88710(0.82604)i (6)\n|Ni+3 \u2212 \u03b13Ni| < 1.16332(0.82604)i. (7)\nWe now prove a useful lemma. Lemma 5. For i \u2265 0 we have the bounds\n\u22120.79521 < [(i)N 0]N \u2212 \u03b1i <1.046298 (8)\n\u22121.09219 < [(i)N 00]N \u2212 \u03b12i <1.69794 (9)\n\u22121.09644 < [(i)N 000]N \u2212 \u03b13i <1.702181. (10)\nProof. (i) Our starting point is the inequalities (5)\u2013(7). Write i in its Narayana representa-\ntion: i = Ne1 + \u00b7\u00b7\u00b7 + Nes for some integers 0 \u2264 e1 < e2 < \u00b7\u00b7\u00b7 < es. Then\n[(i)N 0]N = Ne1+1 + \u00b7\u00b7\u00b7 + Nes+1,\nand hence\n[(i)N 0]N \u2212 \u03b1i =\nX\n1\u2264j\u2264s\n(Nej+1 \u2212 \u03b1Nej ). 4\n\nNow we split the sum on the right-hand side into two pieces: one where ej \u2264 30, and one\nwhere ej > 30:\n[(i)N 0]N \u2212 \u03b1i =\nX\n0\u2264ej\u226430\n(Nej+1 \u2212 \u03b1Nej ) +\nX\nk>30\n(Nk+1 \u2212 \u03b1Nk),\nand hence\n[(i)N 0]N \u2212 \u03b1i \u2212\nX\n0\u2264ej\u226430\n(Nej+1 \u2212 \u03b1Nej ) \u2264\nX\nk>30\n|Nk+1 \u2212 \u03b1Nk|\n<\nX\nk>30\n.71827(0.82604)k\n< .01104. (11)\nWe now compute bounds on P\n1\u2264ej\u226430(Nej+1 \u2212 \u03b1Nej ) by direct computation. If we examine\nall k in the range 0 \u2264 k < N31 = 125491, then we see the last 30 bits of all possible Narayana\nexpansions. By direct calculation we get\n\u22120.7841638 <\nX\n1\u2264ej\u226430\n(Nej+1 \u2212 \u03b1Nej ) < 1.035258. Putting this together with (11) we get\n\u22120.79521 < [(i)N 0]N \u2212 \u03b1i <1.046298, (12)\nas desired. (ii) and (iii) can be proved in exactly the same way. Remark 6. We can improve the upper and lower bounds by doing more computation, replac-\ning the 30 with a large number. Similar, but weaker numerical results were previously obtained by Dilcher [13] and\nLetouzey [29] in, however, more generality than what we obtained here. 4 Finite automata and Walnut\nFor more about the theory of finite automata, see [23], for example. We consider sequences (fi)i\u22650 defined by finite automata that take, as inputs, the Narayana\nrepresentation of one or more integers.", "metadata": {"type": "factual", "source": "2503.01357v1.pdf", "context": "(\nc1 c2\n0 1\n)\n\u2208\nGL2( \u02c6A) for c1,c2 \u2208 \u02c6A inside\nGL0\n2 and set nI := [\nGL0\n2 :\nG] and by [ Leh09, Ch.5,Prop.3.5],\nwe know that nI is a \ufb01nite number which can be described explicitly. For any \u03b1\u2208 G, from\nthe proof of [ Leh09, Prop.2.5], we see that \u03b1\u2217(\u03c6,\u03bb) = ( \u03c6,\u03bb \u25e6 \u03b1\u22121). Consequently, for any\ntwo representatives \u03c3,\u03c3\u2032 \u2208 GL0\n2 of the same coset in\nGL0\n2/\nG, we have that the underlying\nDrinfeld modules of \u03c3\u2217(\u03c6,\u03bb) and \u03c3\u2032\n\u2217(\u03c6,\u03bb) are the same and di\ufb00er only in the level I-structure\nby an element in G. We", "quality_score": 8}}
{"id": "test_82", "question": "How can one infer the content or purpose of this text based on its structure?", "golden_answers": ["Given the presence of multiple blocks with different codes and the use of forward slashes as separators, it is likely that this text represents a compressed or encoded form of data, possibly requiring decoding to reveal its original content."], "prediction": "The text appears to be a scholarly article discussing methods for analyzing differences in text documents based on certain characteristics (W=1 and W=0). It outlines a structured approach to testing for statistical differences between two distributions of text, emphasizing the challenges and limitations of traditional methods. The text is organized into sections that indicate a systematic exploration of the topic, including testing for effects, describing differences, and assessing the completeness of the analysis. This structure suggests that the purpose of the text is to present a methodological framework for researchers in the field of text analysis or data science.", "f1": 0.19672131147540986, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00725v1\nInstead,\nwe need to find ways of describing core aspects of the differences across Y |W=1 and Y |W=0 in a\nmanageable and statistically valid way, based on a potentially small sample {(Yi, Wi)}n\ni=1. In the\nfollowing three chapters, we tackle this challenge by providing an approach to addressing whether\n7\n\nthere is a difference between Y |W=1 and Y |W=0 (Section 4), what the difference is (Section 5),\nand how complete our description of said difference is (Section 6). 4 Testing for an Effect on Text\nOur first goal is to detect whether the two distributions Y |W=1 and Y |W=0 over text documents\nare the same. That is, before asking how the two distributions differ, we first want to checkwhether\nthere is any statistically significant difference at all. Even for this seemingly more effortless task,\nstandard approaches for testing for differences across two distributions are limited. First, we could\nchoose a few features of the text documents by hand that we think could be affected by treatment\nand then run a joint test for average differences in those features across the two groups. For example,\nwe could hypothesize that the texts are longer in one group than the other, that a specific word\n(or group of words) is more likely to appear in one group, and/or that the sentiment of the text\n(as measured by some pre-defined algorithm) differs in a systematic way. But the number of ways\nin which we could provide simple features of complex texts is so vast that this approach is likely to\nmiss important signals unless we have strong and correct priors about which aspect of text is most\naffected. Second, we could instead search through many such features automatically by trying them\none by one. However, this approach risks leading to spurious findings, requiring us to apply severe\ncorrections to avoid excess false positives. In the context of a very high number of potential features\nrelative to a small or moderate number of text documents, these corrections may make detecting\nsignificant effects very hard, if not impossible. Finally, we could apply general non-parametric\ntests that have been proposed for detecting the difference between two distributions, such as multi-\ndimensional generalizations of the Kolmogorov\u2013Smirnov test. Yet even such tests would rely on\nspecific representations of the text, as they typically focus on relatively low-dimensional outcome\nvectors and may quickly lose power otherwise. Instead of relying on tests that compare text features across the two groups, we adapt a trick\nfrom Ludwig, Mullainathan, and Spiess (2017) and ask: Given a document Yi in our sample, can\nwe predict its group assignment Wi better than some trivial benchmark? That is, rather than\nasking whether W has a causal effect on Y , we ask whether Y is predictive of W. This change\nin framing allows us to leverage non-parametric tools from machine learning directly, since these\ntools excel at predicting some simple outcome (here: Wi) from complex covariates (here: Yi) by\nsystematically searching over good predictors.", "metadata": {"type": "reasoning", "source": "2503.01200v1.pdf", "context": "/uni00000036/uni0000004c/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000047\n/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni00000051/uni00000050/uni0000000c\n/uni00000013/uni00000011/uni00000013\n/uni00000015/uni00000011/uni00000018\n/uni00000018/uni00000011/uni00000013\n/uni0000001a/", "quality_score": 8}}
{"id": "test_83", "question": "What is the characteristic age (\u03c4c) of pulsars in this study, and what does it indicate?", "golden_answers": ["The characteristic age \u03c4c is a reasonable estimate of the true age of pulsars. It is indicative of the time since the supernova explosion that formed the neutron star."], "prediction": "The characteristic age (\u03c4c) of pulsars in this study is a reliable age estimate that indicates the age of the pulsars, which is consistent with the assumption of LSR isotropy.", "f1": 0.4999999999999999, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01429v1\n2 we show the kinematic age\ndistributions\u2014-that follow from the sets of pulsar trajectories\u2014\nversus the characteristic ages. Indeed, the assumption of LSR\nisotropy results in the majority of the median kinematic age esti-\nmates to differ less than\u223c20% from the corresponding character-\nistic ages. There are, however, a few pulsars with \u03c4kin exceeding\n\u03c4c substantially, which could either mean that the assumption of\nLSR isotropy is invalid or that\u03c4c is not indicative of the true age\n(e.g. due to post-SN fallback on the NS, as discussed by Igoshev\net al. 2016a). In particular, we considered pulsars where median\n\u03c4kin exceeds \u03c4c with more than 5 Myr to have potentially un-\ncertain ages. Nevertheless, in general we deem the characteristic\nage a reliable age estimate and consistent with the assumption of\nLSR isotropy. In order to use the method of Disberg et al. (2024b) to kine-\nmatically constrain the natal kick of the pulsars, we first in-\nvestigated the shape (i.e. eccentricity) of their Galactic trajec-\ntories. After all, a larger kick disturbs the initial circular orbits\nmore and therefore results in a more eccentric path through the\nGalaxy. Since we have established that\u03c4c is a reasonable age es-\ntimate, we limited our analysis to the pulsar trajectories that do\nnot deviate from the Galactic disc when traced back for a pe-\nriod equal to the characteristic age. That is, we conservatively\nadopted the constraint that the traced-back trajectories should be\npositioned within R \u2264 20 kpc and |z| \u2264 1 kpc at t = \u03c4c. Al-\nthough for a few pulsars this decreases the amount of trajecto-\nries significantly (i.e. J1321 +8323, J1543+0929, J1840+5640,\nJ2046+1540, J2046\u20130421, J2248\u20130101, J2346\u20130609), the me-\ndian exclusion rate is only 3%. We evaluate the cylindrical radii\nArticle number, page 4 of 15\n\nP. Disberg et al.: A kinematically constrained kick distribution for isolated neutron stars\n(i.e. the Galactic distances projected on the Galactic plane) of the\nselected trajectories and take their minimum and maximum val-\nues (Rmin and Rmax, respectively). The eccentricity of the Galac-\ntic orbit is then defined as:\n\u02dce = Rmax \u2212Rmin\nRmax + Rmin\n, (6)\nanalogous to a Keplerian eccentricity (although the trajecto-\nries are not Keplerian due to the Galactic potential). If an ob-\nject that starts in a circular Galactic orbits experiences no kick,\nthen Rmin = Rmax and \u02dce = 0. In contrast, if the object re-\nceives an extremely large kick, it will escape the Galaxy meaning\nRmax \u226bRmin and \u02dce \u22481.\nIn Fig. 3 we show the resulting eccentricity distributions (cf.\nFig. 9 of Disberg et al. 2024b) for four subsets: the \u201cyoung\u201d pul-\nsars as defined by Igoshev (2020, i.e. \u03c4c \u22643 Myr) and Verbunt\net al. (2017, i.e. \u03c4c \u226410 Myr), as well as old pulsars (i.e. 40\nMyr \u2264\u03c4c \u22641 Gyr) that have obtained (galactocentric) velocities\n\u223c200 km/s (independent of kick distribution, see Appendix B of\nDisberg et al. 2024a) and pulsars in between these two categories\n(i.e. 10 Myr \u2264\u03c4c \u226440 Myr, see Appendix A of Disberg et al.\n2024a).", "metadata": {"type": "factual", "source": "2503.01429v2.pdf", "context": "istic ages. There are, however, a few pulsars with \u03c4kin exceeding\n\u03c4c substantially, which could either mean that the assumption of\nLSR isotropy is invalid or that\u03c4c is not indicative of the true age\n(e.g. due to post-SN fallback on the NS, as discussed by Igoshev\net al. 2016a). In particular, we considered pulsars where median\n\u03c4kin exceeds \u03c4c with more than 5 Myr to have potentially un-\ncertain ages. Nevertheless, in general we deem the characteristic\nage a reliable age estimate and consistent w", "quality_score": 8}}
{"id": "test_84", "question": "How do hierarchical exponential attention and reduced-bias modules contribute to the overall performance of the model, as shown by ablation studies in Table V?", "golden_answers": ["They expand data depth, excelling in conventional tasks (Figure 4) and domain tasks (Figures 5 and 6)."], "prediction": "Hierarchical exponential attention improves the model's ability to extract key information effectively, while the reduced-bias modules enhance feature expression by adaptively adjusting coefficients based on input text features. Both contribute to better performance metrics, as indicated by the results in Table V.", "f1": 0.035714285714285705, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01217v2\nHowever, compared with\nthe precision of DAE (69.68), proposed method still has a\ncertain gap in accuracy, which suggests that proposed model\nstill has room for improvement in accurate prediction when\ndealing with noisy social media data. 2) Ablation Studies: Ablation Study in Table V evaluates\nthe proposed structure through two experiments: (1) module\nremoval to assess synergy, and (2) BERT replacement/removal\nto analyze word embedding impacts. a) Comparison between the base model and different\nBERT models: The experimental results of comparing the base\nmodel (S.N.1) with the models that introduce different BERT\nvariants (S.N.2 and S.N.3) show that RoBERTa outperforms\nBERT in terms of vocabulary vectorization. b) Comparison between naive attention and hierarchical\nEMA: When comparing the naive attention model (S.N.4) with\nthe model based on hierarchical exponential moving average\n(Hierarchical EMA) (S.N.5), the experimental results show\nthat the hierarchical attention mechanism can more effectively\nextract the weight distribution of key information under the\nconstraint of exponential gain weights. c) Comparison between static and dynamic reduced-bias\nmodels: By comparing the immutable Reduced-bias model\n(S.N.4) with the self-learning Reduced-bias model (S.N.5), the\nresults verify that the dynamically changing coefficients can\nbe adaptively adjusted according to the input text features,\nthereby improving the model\u2019s ability to express features. d) Comparison of the proposed model with the naive\nBERT model: In the comparison of the final model (S.N.8)\nwith the ordinary BERT model (S.N.7), the experimental\nresults show that even with the introduction of a rich feature\nextraction module, RoBERTa can still efficiently capture the\ndeep features of the text and show a superior feature learning\nability. TABLE V\nABLATION STUDIES ON MSRA\nS.N. Module Metric\nA H R BERT P R F1\n1 84.65 80.68 82.62\n2 BERT-base 91.02 92.09 91.55\n3 RoBERTa 92.21 92.57 92.38\n4 \u221a 87.42 85.78 86.60\n5 \u221a 91.35 90.62 90.98\n6 \u221a \u221a 92.92 93.09 93.01\n7 \u221a \u221a BERT-base 93.55 93.88 93.71\n8 (Ours) \u221a \u221a RoBERTa 94.58 93.98 94.28\nE. Parameter Selection\nIn the design of the attention module based on the EMA\n(Exponential Moving Average) mechanism, the number of\nattention heads is considered to be one of the important factors\naffecting the model performance. In order to determine the\noptimal configuration of attention heads, different numbers\nof attention heads (16, 32, 64, 128) is set in the experiment\nand evaluated the performance of the model on the validation\nset. The experimental results show that when the number\nof attention heads is set to 64, the model achieves the best\nperformance in performance indicators such as precision,\nrecall, and F1 score. As seen from the Table VI, too few attention heads with\n16 or 32, may cause the model to be unable to fully capture\nfeature representations, thereby limiting the performance of\nthe model. When the number of attention heads increases to\n64, the performance of the model reaches its peak. However,\nwhen the number of attention heads is further increased to\n128), the performance of the model does not continue to\n\nFig. 5. Cases in the MSRA validation set are selected for analysis.", "metadata": {"type": "reasoning", "source": "2503.01217v2.pdf", "context": "across scene tasks, enhancing CNER robustness. As shown in\nTables II to IV, proposed method achieves top performance\nin regression and F1 scores. Ablation studies in Table V\nconfirm that hierarchical exponential attention and reduced-\nbias modules expand data depth, excelling in conventional\ntasks in Figure 4 and domain tasks in Figures 5 and 6.\n2) The Hierarchical Reduced-bias EMA architecture enables\nprecise CNER feature extraction. Inspired by DAE [27], the", "quality_score": 8}}
{"id": "test_85", "question": "Why would a fast CU partition strategy based on texture and neighboring partition information be beneficial in Versatile Video Coding Intra Coding?", "golden_answers": ["A fast CU partition strategy based on texture and neighboring partition information would be beneficial as it can improve the efficiency of the encoding process by reducing computational complexity, while also maintaining or improving coding performance."], "prediction": "A fast CU partition strategy based on texture and neighboring partition information would be beneficial in Versatile Video Coding Intra Coding because it can significantly reduce the computational complexity and encoding time while maintaining high video quality. By leveraging texture characteristics and the context of neighboring partitions, the algorithm can make more informed decisions about partitioning, leading to efficient encoding and improved performance in real-time applications. This approach enhances the overall efficiency of the encoding process, making it suitable for scenarios where speed is critical, such as live streaming or interactive video applications.", "f1": 0.48333333333333334, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.01415v1\n[13] X. Shang, G. Li, X. Zhao, and Y . Zuo, \u201cLow Complexity Inter Coding\nScheme for Versatile Video Coding (VVC),\u201d Journal of Visual Commu-\nnication and Image Representation , vol. 90, p. 103683, 2023. [14] Y . Li, Z. He, and Q. Zhang, \u201cFast Decision-Tree-Based Series Parti-\ntioning and Mode Prediction Termination Algorithm for H.266/VVC,\u201d\nElectronics, vol. 13, no. 7, p. 1250, 2024. [15] Y . Liu, M. Abdoli, T. Guionnet, C. Guillemot, and A. Roumy, \u201cLight-\nWeight CNN-Based VVC Inter Partitioning Acceleration,\u201d in 2022 IEEE\n14th Image, Video, and Multidimensional Signal Processing Workshop\n(IVMSP), pp. 1\u20135, IEEE, 2022. [16] Z. Peng and L. Shen, \u201cA Classification-Prediction Joint Framework to\nAccelerate QTMT-Based CU Partition of Inter-Mode VVC,\u201d Electronics\nLetters, vol. 59, no. 7, p. e12770, 2023. [17] X. Jiang, M. Xiang, J. Jin, and T. Song, \u201cExtreme Learning Machine-\nEnabled Coding Unit Partitioning Algorithm for Versatile Video Cod-\ning,\u201d Information, vol. 14, no. 9, p. 494, 2023. [18] J. Lin, H. Lin, Z. Zhang, and Y . Xu, \u201cEfficient Inter Partitioning of\nVersatile Video Coding Based on Supervised Contrastive Learning,\u201d\nKnowledge-Based Systems, vol. 296, p. 111902, 2024. [19] A. Wieckowski, J. Brandenburg, T. Hinz, C. Bartnik, V . George,\nG. Hege, C. Helmrich, A. Henkel, C. Lehmann, C. Stoffers, et al. ,\n\u201cVVenC: An Open and Optimized VVC Encoder Implementation,\u201d in\n2021 IEEE International Conference on Multimedia & Expo Workshops\n(ICMEW), pp. 1\u20132, IEEE, 2021. [20] A. Wieckowski, G. Hege, C. Lehmann, B. Bross, D. Marpe, C. Feld-\nmann, and M. Smole, \u201cVVC in the Cloud and Browser Playback: It\nWorks,\u201d in Proceedings of the 1st Mile-High Video Conference , pp. 19\u2013\n24, 2022. [21] E. C \u00b8 etinkaya, H. Amirpour, M. Ghanbari, and C. Timmerer, \u201cCTU Depth\nDecision Algorithms for HEVC: A Survey,\u201d Signal Processing: Image\nCommunication, vol. 99, p. 116442, 2021. [22] Y . Liu, H. Amirpour, M. Abdoli, C. Timmerer, and T. Guionnet,\n\u201cPreparing VVC for Streaming: A Fast Multi-Rate Encoding Approach,\u201d\nin 2023 IEEE International Conference on Visual Communications and\nImage Processing (VCIP) , (Jeju, Korea, Republic of), pp. 1\u20135, IEEE,\nDec. 2023. [23] J. Brandenburg, A. Wieckowski, T. Hinz, A. Henkel, V . George, I. Zu-\npancic, C. Stoffers, B. Bross, H. Schwarz, and D. Marpe, \u201cTowards\nFast and Efficient VVC Encoding,\u201d in 2020 IEEE 22nd International\nWorkshop on Multimedia Signal Processing (MMSP) , pp. 1\u20136, IEEE,\n2020. [24] V . V . Menon, H. Amirpour, C. Timmerer, and M. Ghanbari, \u201cINCEPT:\nIntra CU Depth Prediction for HEVC,\u201d in 2021 IEEE 23rd Interna-\ntional Workshop on Multimedia Signal Processing (MMSP) , (Tampere,\nFinland), pp. 1\u20136, IEEE, Oct. 2021. [25] H. Yang, H. Chen, J. Chen, S. Esenlik, S. Sethuraman, X. Xiu,\nE. Alshina, and J. Luo, \u201cSubblock-Based Motion Derivation and Inter\nPrediction Refinement in the Versatile Video Coding Standard,\u201d IEEE\ntransactions on circuits and systems for video technology, vol. 31, no. 10,\npp. 3862\u20133877, 2021.", "metadata": {"type": "reasoning", "source": "2503.01404v1.pdf", "context": "[12] V . V . Menon, H. Amirpour, M. Ghanbari, and C. Timmerer, \u201cEMES:\nEfficient Multi-Encoding Schemes for HEVC-Based Adaptive Bitrate\nStreaming,\u201d ACM Transactions on Multimedia Computing, Communica-\ntions and Applications, vol. 19, no. 3s, pp. 1\u201320, 2023.\n[13] Y . Liu, H. Amirpour, M. Abdoli, C. Timmerer, and T. Guionnet,\n\u201cPreparing VVC for Streaming: A Fast Multi-Rate Encoding Approach,\u201d\nin 2023 IEEE International Conference on Visual Communications and\nImage Processing (VCIP), pp. 1\u20135, IEEE, ", "quality_score": 8}}
{"id": "test_86", "question": "What is a potential benefit of combining equivariance with data augmentation?", "golden_answers": ["Competitive performance and approximate equivariance"], "prediction": "A potential benefit of combining equivariance with data augmentation is that it can enhance the robustness and generalization of machine learning models by allowing them to better handle variations in the input data while maintaining the underlying structure and relationships.", "f1": 0.09756097560975609, "em": 0.0, "topk_used": 1, "did_retrieve": true, "config": "k=1 (fixed)", "retrieved_text": "2503.00094v1\nFor this reason, uncertainty quantification has become a golden\nstandard in many industrial applications. It can be achieved in\nseveral ways. A first approach consists in combining standard ML algo-\nrithms with model-free UQ techniques. Standard ML algo-\nrithms already used in the literature to build surrogate models\ninclude gradient boosting methods, polynomial chaos expan-\nsion, random forest, response surface methods, and polynomial\nsplines. It can be plugged with model-free UQ techniques\nsuch as Monte Carlo, conformal predictions, and various en-\nsemble methods (Bootstrap prediction intervals, infinitesimal\nJackknife, and Out-Of-Bag errors). This first approach offers\ngreat flexibility in the surrogate model selection, and UQ\nmethods rely on mild assumptions that are often respected. The\nmain drawback of these model-free UQ techniques is that they\ntend to produce overly large confidence intervals with small\ndatasets. They also have a high computational cost, making\nthem poorly suited to building fast-to-evaluate uncertainty-\naware surrogate models. A second possible approach consists in adapting existing\nprediction models to enable them to quantify uncertainty. For\nexample, quantile regression (QR) methods modify the loss\nfunction so that the machine learning model predicts a specific\nquantile of the expected output. Predicting the median, the 5%,\nand the 95% quantiles allows obtaining a prediction and the\nassociated 90% confidence interval. Other works also propose\nto combine the predictive power of artificial neural networks\n(ANN) with deep-learning-specific UQ techniques, such as the\ndelta method, the mean-variance estimation, the lower upper\nbound estimation, and quality-driven ensemble methods. There\nare two drawbacks to this second approach. First, for both\nQR and ANN, a lot of data is required to train the models\neffectively. Second, the obtained surrogate model is black-box,\nwhich is undesirable for safety-critical applications such as\nensuring power grid security. A third increasingly adopted solution to build accuracy-\naware surrogate models is Gaussian processes. Instead of\na simple prediction, GPs provide a full posterior Gaussian\ndistribution N\n\u0000\n\u00b5\u2217, \u03c32\n\u2217\n\u0001\nof the outcome, which is much more\ninformative. The posterior mean \u00b5\u2217 constitutes the prediction,\nand the posterior standard deviation \u03c3\u2217 is used for UQ. Many\nreasons explain the growing popularity of GPs. \u2022 GPs are flexible non-parametric models . Many ma-\nchine learning models are parametric. They learn an\napproximation \u02dcf that belongs to a parametric set of\nfunctions { \u02dcf\u03b8, \u03b8 \u2208 \u0398} (polynomial regressors, neural\nnetworks...). The hyperparameters \u03b8 are optimized with\nthe train data that no longer contribute to the model\nafterward. All the exploited information is compressed\ninto \u03b8. Unlike these models, the GP does not assume a\nfixed number of parameters to approximate the function. Instead, it automatically adapts the model\u2019s complexity\nto the dataset size, which is very flexible. \u2022 GPs provide both prediction and UQ . Instead of\nlooking for one prediction that best fits the data, the\nGP assumes that many approximations could plausibly\nmodel the data. The GP then assigns to each approx-\nimation a probability of actually representing the true\nfunction. The most probable approximation constitutes\nthe GP prediction; the other likely ones contribute to\nUQ.", "metadata": {"type": "factual", "source": "2503.01431v2.pdf", "context": "equivariance combined with data augmentation results in competitive performance and approximate\nequivariance. Several theoretical works also suggest that unconstrained models can produce equivariant\noutputs under certain circumstances Gerken and Kessel[27], Nordenfors and Flinth[28], Puny et al.\n[45]. In contrast, the performance effects of removing energy conservation are less clear. Although\nbenchmarking results suggest non-conservative models can perform stable MD simulations [35]. Bigi et al", "quality_score": 8}}
