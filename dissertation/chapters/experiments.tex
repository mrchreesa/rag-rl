% ============================================================================
% Chapter 5: Experiments
% ============================================================================

\chapter{Experiments}
\label{ch:experiments}

This chapter presents experimental results in three parts: baseline evaluation, RL training with lazy agent analysis, and final performance comparison.

\section{Baseline Experiments}

\subsection{Experimental Setup}

We evaluated multiple RAG configurations to establish baselines:

\begin{table}[H]
    \centering
    \caption{Baseline configurations evaluated}
    \label{tab:baseline-configs}
    \begin{tabular}{llll}
        \toprule
        Method & Retrieval & Generator & Description \\
        \midrule
        Naive RAG & BM25 & Ollama & Sparse retrieval + local LLM \\
        Naive RAG & BM25 & GPT-4o-mini & Sparse retrieval + cloud LLM \\
        Dense RAG & E5 + FAISS & GPT-4o-mini & Semantic search + cloud LLM \\
        Iter-RetGen & BM25 & Ollama & Iterative retrieval \\
        Adaptive-RAG & BM25 & Ollama & Query complexity routing \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Baseline Results}

\begin{table}[H]
    \centering
    \caption{Complete baseline performance on custom dataset (87 test samples)}
    \label{tab:baseline-results}
    \begin{tabular}{lccc}
        \toprule
        Method & EM (\%) & F1 (\%) & Recall@k (\%) \\
        \midrule
        Dense E5 (topk=10) + GPT-4o-mini & 3.45 & \textbf{31.10} & \textbf{87.36} \\
        Dense E5 (topk=5) + GPT-4o-mini & 3.45 & 30.82 & 83.91 \\
        Dense E5 (topk=3) + GPT-4o-mini & \textbf{4.60} & 30.08 & 78.16 \\
        BM25 + GPT-4o-mini & 2.30 & 27.49 & 75.86 \\
        BM25 + Ollama & 2.30 & 14.40 & 75.86 \\
        Iter-RetGen + Ollama & 2.30 & 12.96 & 72.41 \\
        Adaptive-RAG + Ollama & 0.00 & 12.71 & 8.05 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Key Findings from Baselines}

\textbf{Finding 1: Dense retrieval outperforms BM25.}

\begin{table}[H]
    \centering
    \caption{Retrieval method comparison}
    \label{tab:retrieval-comparison}
    \begin{tabular}{lccc}
        \toprule
        Metric & BM25 & Dense E5 & Improvement \\
        \midrule
        Recall@5 & 75.86\% & 83.91\% & +8.05\% \\
        Recall@10 & 80.46\% & 87.36\% & +6.90\% \\
        Best F1 & 27.49\% & 31.10\% & +3.61\% \\
        \bottomrule
    \end{tabular}
\end{table}

Dense retrieval's semantic understanding is crucial for academic terminology.

\textbf{Finding 2: Generator quality is the primary performance driver.}

With identical BM25 retrieval (75.86\% recall):
\begin{itemize}
    \item Ollama: 14.40\% F1
    \item GPT-4o-mini: 27.49\% F1 (+90.9\% improvement)
\end{itemize}

Upgrading the generator provides $\sim$3.6$\times$ more F1 gain than upgrading retrieval.

\textbf{Finding 3: Advanced RAG methods are limited by local LLM quality.}

Iter-RetGen and Adaptive-RAG performed worse than Naive RAG with Ollama, suggesting these methods require capable LLMs.

\textbf{Finding 4: TopK affects precision-recall trade-off.}

\begin{table}[H]
    \centering
    \caption{TopK ablation study}
    \label{tab:topk-ablation}
    \begin{tabular}{cccc}
        \toprule
        TopK & Recall (\%) & F1 (\%) & EM (\%) \\
        \midrule
        3 & 78.16 & 30.08 & \textbf{4.60} \\
        5 & 83.91 & 30.82 & 3.45 \\
        10 & \textbf{87.36} & \textbf{31.10} & 3.45 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{RL Training Experiments}

\subsection{Initial Training: The Lazy Agent Problem}

We first trained the policy without the lazy agent fixes:

\begin{table}[H]
    \centering
    \caption{Initial RL training results (before fixes)}
    \label{tab:initial-rl}
    \begin{tabular}{ccccc}
        \toprule
        Epoch & Train F1 & Train Retr Rate & Val F1 & Val Retr Rate \\
        \midrule
        1 & 0.158 & 72.6\% & 0.141 & 0.0\% \\
        2 & 0.172 & 44.1\% & 0.141 & 0.0\% \\
        3 & 0.173 & 35.2\% & 0.141 & 0.0\% \\
        4 & 0.177 & 26.0\% & 0.141 & 0.0\% \\
        5 & 0.175 & 28.5\% & 0.141 & 0.0\% \\
        6 & 0.167 & 31.1\% & 0.141 & 0.0\% \\
        7 & 0.190 & 40.0\% & 0.141 & 0.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Observation:} Training showed exploration (26--72\% retrieval due to epsilon-greedy), but the deterministic evaluation policy consistently chose ``never retrieve'' (0\% retrieval rate), resulting in validation F1 stuck at 0.141 (LLM parametric knowledge only).

\subsection{Training with Lazy Agent Fixes}

After implementing the five solutions (reward shaping, curriculum learning, entropy regularization, temperature sampling, difficulty features), we retrained:

\begin{table}[H]
    \centering
    \caption{RL training results after lazy agent fixes}
    \label{tab:fixed-rl}
    \begin{tabular}{cccccc}
        \toprule
        Epoch & Train F1 & Train Reward & Train Retr & Val F1 & Val Retr \\
        \midrule
        1 & 0.29 & 0.24 & 100\% & 0.34 & 100\% \\
        2 & 0.31 & 0.26 & 100\% & 0.34 & 100\% \\
        3 & 0.32 & 0.27 & 100\% & 0.34 & 100\% \\
        4 & 0.33 & 0.28 & 100\% & 0.34 & 100\% \\
        5 & 0.33 & 0.28 & 100\% & 0.34 & 100\% \\
        6 & 0.33 & 0.28 & 100\% & 0.34 & 100\% \\
        7 & 0.33 & 0.28 & 100\% & 0.34 & 100\% \\
        8 & 0.33 & 0.29 & 100\% & 0.34 & 100\% \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Key Metrics (Final):}
\begin{itemize}
    \item Validation F1: 34.4\% (exceeds baseline 31.1\%)
    \item Validation Retrieval Rate: 100\%
    \item Lazy Agent Failures: 0
    \item Average Retrieval Probability: 99.6\%
\end{itemize}

\subsection{Before vs. After Comparison}

\begin{table}[H]
    \centering
    \caption{Impact of lazy agent fixes}
    \label{tab:before-after}
    \begin{tabular}{lccc}
        \toprule
        Metric & Before Fix & After Fix & Change \\
        \midrule
        Val F1 & 14.1\% & 34.4\% & +143\% \\
        Val Retrieval Rate & 0\% & 100\% & Fixed \\
        Lazy Agent Failures & 100\% & 0\% & Eliminated \\
        Comparison to Baseline & 45\% & 101\% & +56\% \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Final Evaluation}

\subsection{Evaluation Setup}

We evaluated the trained policy against baselines on the full test set (87 samples):

\begin{lstlisting}[language=bash]
python evaluate_rl_agent.py --mode trained \
    --checkpoint best_model.pt --samples 87
\end{lstlisting}

\subsection{Results}

\begin{table}[H]
    \centering
    \caption{Final evaluation results (87 test samples)}
    \label{tab:final-eval}
    \begin{tabular}{lcccc}
        \toprule
        Method & F1 (\%) & EM (\%) & Reward & Retr Rate \\
        \midrule
        Trained RL Policy & 33.87 & 2.30 & 0.2847 & 100\% \\
        Baseline (Always Retrieve) & 34.08 & 3.45 & 0.2868 & 100\% \\
        No Retrieval (Pure LLM) & 18.52 & 0.00 & -0.0097 & 0\% \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Finding 1: Trained policy matches baseline.}

The RL-trained policy achieves 33.87\% F1, within 0.21\% of the always-retrieve baseline (34.08\% F1). This difference is within statistical noise.

\textbf{Finding 2: Policy learned correct domain behavior.}

For academic question-answering, the policy correctly learned that retrieval is almost always necessary:
\begin{itemize}
    \item Average retrieval probability: 99.6\%
    \item Actual retrieval rate: 100\%
\end{itemize}

\textbf{Finding 3: Retrieval is essential for this domain.}

The no-retrieval baseline achieves only 18.52\% F1, confirming that the LLM's parametric knowledge is insufficient for academic domain questions. The policy's decision to always retrieve is optimal.

\textbf{Finding 4: Lazy agent problem is solved.}

Unlike initial experiments where the policy learned to never retrieve, the fixed policy maintains 100\% retrieval with high F1, demonstrating that the five solutions successfully addressed the problem.

\section{Efficiency Analysis}

\subsection{Retrieval Rate vs. Performance Trade-off}

From earlier experiments with simulated retrieval policies:

\begin{table}[H]
    \centering
    \caption{Efficiency-quality trade-off (simulated policies)}
    \label{tab:efficiency}
    \begin{tabular}{ccccc}
        \toprule
        Retrieval Rate & F1 (\%) & EM (\%) & Efficiency Gain & F1 Retention \\
        \midrule
        100\% (baseline) & 34.56 & 4.60 & 0\% & 100\% \\
        80\% & 29.84 & 4.60 & 20\% & 86\% \\
        60\% & 26.08 & 3.45 & 40\% & 75\% \\
        40\% & 23.95 & 2.30 & 60\% & 69\% \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Insight:} For datasets where retrieval is not always necessary, an 80\% retrieval policy could maintain 86\% of baseline F1 with 20\% fewer retrievals.

\subsection{API Cost Analysis}

\begin{table}[H]
    \centering
    \caption{API cost breakdown}
    \label{tab:api-cost}
    \begin{tabular}{ll}
        \toprule
        Component & Cost \\
        \midrule
        GPT-4o-mini input & \$0.15 / 1M tokens \\
        GPT-4o-mini output & \$0.60 / 1M tokens \\
        Training run (10 epochs, 492 samples) & $\sim$\$2--5 \\
        Evaluation (87 samples) & $\sim$\$0.50 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{HotpotQA Comparison}

To contextualize our results, we compared performance on HotpotQA (a standard multi-hop QA benchmark):

\begin{table}[H]
    \centering
    \caption{Custom dataset vs. HotpotQA comparison}
    \label{tab:hotpotqa-comparison}
    \begin{tabular}{lcc}
        \toprule
        Metric & Custom Dataset & HotpotQA \\
        \midrule
        Best EM & 4.60\% & 19.30\% \\
        Best F1 & 31.10\% & 30.62\% \\
        Retrieval Recall@5 & 75.86\% (BM25) & 45.50\% \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Insight:} HotpotQA achieves higher EM due to standardized Wikipedia answers, while F1 is similar. The custom dataset has higher retrieval recall because single-hop academic questions are easier to retrieve for than multi-hop reasoning questions.

\section{Summary}

Experiments demonstrated:

\begin{enumerate}
    \item \textbf{Baselines:} Dense E5 + GPT-4o-mini achieves 31.10\% F1, with generator quality being the primary performance driver.

    \item \textbf{Lazy Agent:} Initial RL training produced degenerate policies that never retrieve. Five complementary fixes resolved this.

    \item \textbf{Final Performance:} The trained policy achieves 33.87\% F1, matching the baseline and correctly learning that academic QA requires retrieval.

    \item \textbf{Efficiency Potential:} On datasets with mixed question difficulty, RL policies could reduce retrievals while maintaining most quality.
\end{enumerate}
