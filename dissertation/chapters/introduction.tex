% ============================================================================
% Chapter 1: Introduction
% ============================================================================

\chapter{Introduction}
\label{ch:introduction}

\section{Motivation}

Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, they suffer from fundamental limitations: knowledge cutoffs, hallucination of facts, and inability to access domain-specific or up-to-date information. Retrieval-Augmented Generation (RAG) addresses these limitations by augmenting LLM responses with relevant documents retrieved from external knowledge bases.

Traditional RAG systems employ fixed retrieval strategies---retrieving documents for every query regardless of whether external knowledge is actually needed. This approach is inefficient: simple factual questions within the LLM's training knowledge trigger unnecessary retrieval operations, increasing latency and computational costs. Conversely, complex multi-hop reasoning questions may require multiple retrieval iterations that fixed strategies cannot provide.

This dissertation investigates whether Reinforcement Learning (RL) can optimize RAG systems by learning \emph{when} to retrieve, creating a self-improving agent that adapts its retrieval strategy based on query characteristics and domain requirements.

\section{Research Questions}

This dissertation addresses the following research questions:

\begin{enumerate}
    \item \textbf{RQ1:} Can an RL-trained policy learn to make effective retrieval decisions in a RAG system?

    \item \textbf{RQ2:} What reward function design enables stable training without degenerate behaviors (e.g., never retrieving)?

    \item \textbf{RQ3:} How does an RL-optimized retrieval policy compare to fixed retrieval baselines on domain-specific question answering?
\end{enumerate}

\section{Contributions}

This dissertation makes the following contributions:

\begin{enumerate}
    \item \textbf{RL-RAG Framework:} A modular pipeline integrating neural policy networks with RAG components (dense retrieval, query rewriting, LLM generation) trained using policy gradient methods.

    \item \textbf{Lazy Agent Solutions:} Identification and resolution of the ``lazy agent'' problem---where policies learn to always skip retrieval---through five complementary techniques: reward shaping, curriculum learning, entropy regularization, temperature-based sampling, and question difficulty features.

    \item \textbf{Custom Academic Dataset:} A curated question-answering dataset of 492 high-quality QA pairs generated from 2,000 arXiv papers, with multi-stage quality filtering achieving $<5\%$ false positive rate.

    \item \textbf{Comprehensive Baseline Study:} Systematic evaluation of 8+ RAG configurations comparing sparse (BM25) vs. dense (E5) retrieval and local (Ollama) vs. cloud (GPT-4o-mini) generation.

    \item \textbf{Empirical Validation:} Demonstration that RL can discover optimal retrieval strategies, with the trained policy matching baseline performance (33.87\% vs.\ 34.08\% F1) while learning domain-appropriate behavior.
\end{enumerate}

\section{Dissertation Structure}

The remainder of this dissertation is organized as follows:

\begin{description}
    \item[Chapter 2: Background] Reviews foundational concepts in RAG systems, reinforcement learning, and related work on adaptive retrieval.

    \item[Chapter 3: Methodology] Presents the RL-RAG framework, including the policy network architecture, reward function design, and training algorithms.

    \item[Chapter 4: Implementation] Describes the technical implementation, including the FlashRAG integration, dataset preparation, and experimental infrastructure.

    \item[Chapter 5: Experiments] Reports experimental results on baseline comparisons, RL training, and the lazy agent problem resolution.

    \item[Chapter 6: Discussion] Analyzes findings, limitations, and implications for RAG system optimization.

    \item[Chapter 7: Conclusion] Summarizes contributions and outlines future research directions.
\end{description}
