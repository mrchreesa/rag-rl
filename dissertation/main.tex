% ============================================================================
% BSc Computer Science Dissertation
% Reinforcement Learning for Optimizing Retrieval-Augmented Generation
% ============================================================================

\documentclass[12pt,a4paper,oneside]{report}

% ============================================================================
% PACKAGES
% ============================================================================

% Core formatting
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{setspace}
\onehalfspacing

% Graphics and figures
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\graphicspath{{figures/}}

% Tables
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

% Code listings
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

% References and links
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}

% Bibliography
\usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\addbibresource{references.bib}

% Misc
\usepackage{enumitem}
\usepackage{parskip}
\usepackage{fancyhdr}

% Header/Footer style
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ============================================================================
% DOCUMENT METADATA
% ============================================================================

\title{Reinforcement Learning for Optimizing\\Retrieval-Augmented Generation:\\A Self-Improving RAG Agent for Academic Question Answering}
\author{Kristian Rahnev}
\date{2026}

% ============================================================================
% DOCUMENT
% ============================================================================

\begin{document}

% ----------------------------------------------------------------------------
% FRONT MATTER
% ----------------------------------------------------------------------------

% Title page
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Large\textbf{[University Name]}}\\[0.5cm]
    {\large Department of Computer Science}\\[2cm]

    {\LARGE\textbf{Reinforcement Learning for Optimizing\\Retrieval-Augmented Generation}}\\[0.5cm]
    {\Large A Self-Improving RAG Agent for Academic Question Answering}\\[2cm]

    {\large BSc Computer Science\\Final Year Dissertation}\\[2cm]

    {\Large\textbf{[Your Name]}}\\[0.5cm]
    {\large Student ID: [Your ID]}\\[2cm]

    {\large Supervisor: [Supervisor Name]}\\[2cm]

    {\large January 2026}

    \vfill
\end{titlepage}

% Abstract
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Retrieval-Augmented Generation (RAG) systems enhance Large Language Model (LLM) capabilities by grounding responses in external knowledge. However, traditional RAG systems use fixed retrieval strategies, retrieving documents for every query regardless of necessity. This dissertation presents an approach to optimizing RAG systems using Reinforcement Learning (RL), training a policy network to make intelligent retrieval decisions.

We developed an RL-enhanced RAG pipeline with a neural policy network trained using the REINFORCE algorithm. The system learns when retrieval is beneficial versus when the LLM's parametric knowledge suffices. We identified and solved the ``lazy agent'' problem---where the policy learns to always skip retrieval---through reward shaping, curriculum learning, entropy regularization, and temperature-based sampling.

Experiments on a custom academic question-answering dataset (492 training samples from 2,000 arXiv papers) demonstrate that the trained policy achieves 33.87\% F1 score, matching the always-retrieve baseline (34.08\% F1) while correctly learning that academic domain questions require retrieval. The policy outperforms the no-retrieval baseline (18.52\% F1) by 83\%.

This work contributes: (1) a modular RL-RAG training framework, (2) solutions to the lazy agent problem in retrieval policy learning, and (3) empirical evidence that RL can discover optimal retrieval strategies for domain-specific question answering.

\textbf{Keywords:} Retrieval-Augmented Generation, Reinforcement Learning, Large Language Models, Question Answering, Policy Gradient Methods

% Acknowledgements
\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

[ I would like to thank my supervisor, Dr. Sama Aleshaiker, for their guidance                                                                                                    
and feedback throughout this project. ]

% Table of contents
\tableofcontents
\listoffigures
\listoftables

% ----------------------------------------------------------------------------
% MAIN MATTER
% ----------------------------------------------------------------------------

\include{chapters/introduction}
\include{chapters/background}
\include{chapters/methodology}
\include{chapters/implementation}
\include{chapters/experiments}
\include{chapters/discussion}
\include{chapters/conclusion}

% ----------------------------------------------------------------------------
% BACK MATTER
% ----------------------------------------------------------------------------

% Bibliography
\printbibliography[heading=bibintoc,title={References}]

% Appendices
\appendix
\include{appendices/code}
\include{appendices/results}

\end{document}
